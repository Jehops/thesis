\documentclass[12pt,glossary]{dalthesis}

\usepackage{amsmath}
\usepackage{amsmath}
\usepackage{bm} % for bold math symbols
\usepackage{booktabs} % better tables
%\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} % margins
% for subfigures (requires caption, breaks knitr w/o subfloat defined below)
\usepackage{caption,subcaption}
%\captionsetup{compatibility=false}
\usepackage{float}
\usepackage{graphicx} % obviously for graphics
% \usepackage{latexsym} % MBE template for some fonts
\usepackage{lineno} % line numbers
\usepackage{mathtools} % an extension to amsmath to fix bugs
\usepackage{multirow} % column cells that span multiple rows
\usepackage{natbib} % nicer references
%\usepackage[natbib=true,style=apa]{biblatex}
%\addbibresource{/home/jrm/scm/references.git/refs.bib}
\usepackage{paralist} % inline lists
%\usepackage[section]{placeins} % keep figures and table inside section
\usepackage{rotating} % for landscape tables
\usepackage{setspace} % for line spacing
% need subfloat b/c knitr's fig.subcap was built with deprecated subfig package
%\newcommand{\subfloat}[2][need a sub-caption]{\subcaptionbox{#1}{#2}}
\usepackage[flushleft]{threeparttable} % description under table
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage[noindentafter,tiny]{titlesec}
\titleformat{\subsection}{\itshape\small\bfseries}{\thesubsection}{1em}{}
\titlespacing{\section}{0pt}{6pt}{6pt}
\titlespacing{\subsection}{0pt}{5pt}{5pt}
\usepackage{verbatim} % comments

\usetikzlibrary{arrows}

% specialcell is for line breaks in table cells
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

% partial derivative command from Dr. Susko
\newcommand{\pd}[2]{\frac{\partial#1}{\partial#2}}

\renewcommand{\figurename}{Fig.}

\begin{document}

<<setup,include=F>>=
library(ape)
library(ggplot2)
library(grid)
library(gtable)
library(knitr)
opts_chunk$set(fig.path='figures/',fig.align='center',fig.show='hold',cache=T,autodep=T) #$
options(formatR.arrow=TRUE,width=90)
setwd("/home/jrm/scm/thesis.git/")
@

\title{Assessing and Improving the Reliability of Models of Molecular Evolution}
\author{Joseph R. Mingrone}

\phd
\defencemonth{July}
\defenceyear{2021}

% \dedicate{Optionally, the thesis can be dedicated to someone, and the student can enter the dedication content here.}

\frontmatter

\begin{abstract}[s]
  % The study of life hinges on a unifying explanation for genetic diversity, evolution.
  Papers describing implementations of the most widely used models of molecular evolution have nearly $10,000$ citations from research in a variety of fields from vaccine design to mammalian physiology.  The aim of this thesis is to build upon these models to improve their reliability.

  Before the detection of positive selection at individual amino acid sites, models of molecular evolution commonly use a sweeping, protein-wide likelihood ratio test for positive selection.  Due to statistical irregularity, the distributions of the likelihood ratio statistic for these sweeping tests may not be strictly justified and thresholds determined from the distributions can give larger than expected type I error rates.  In Chapter 2, we present a modified likelihood approach that can restore statistical regularity to give tractable likelihood ratio statistic distributions for mixture models of codon evolution.

  To detect positive selection at individual amino acid sites, most methods use an empirical Bayes approach.  After parameters of a Markov process of codon evolution are estimated via maximum likelihood, they are passed to Bayes formula to compute the posterior probability that a site evolved under positive selection.  A difficulty with this approach is that parameter estimates with large errors can negatively impact Bayesian classification.  In Chapter 3, we present a technique we call smoothed bootstrap aggregation to accommodate the uncertainty in parameter estimates.

In Chapter...
\end{abstract}

\printglossary
\glossary{name={LR},description={Likelihood Ratio}}
\glossary{name={SBA},description={Smoothed Bootstrap Aggregation}}

%\begin{acknowledgements}
%Thanks to all the little people who make me look tall.
%\end{acknowledgements}

\mainmatter

\chapter{Introduction}
% The processes responsible for the evolution of morphology or behaviour are also relevant at the molecular level, however different approaches are required to study molecular evolution.
%In the twentieth century, new opportunities to model molecular evolution became possible with advancements in both computing power and genetic sequencing technologies.  Dozens of statistical models have been developed and thousands of scientific papers have been published on the topic.  The aim of this thesis is to study and build upon some of these models in order to improve the reliability of detection of positive selection at the level of proteins and amino acids.

% \begin{figure}[h!]
%   \centering
%   \includegraphics[scale=0.35]{images/me_growth.png}
%   \caption[Growth curves relevant to molecular evolution.]{The growth of genetic data and computing power over the past 40 years.  The graph shows the total number of nucleotides in the European Nucleotide Sequence Database\cite{EMBL} (circles) and the number of transistors in computer processors (squares\cite{INTEL2008} and diamonds\cite{WP2008}).  Note the log scale on the y-axis.}
%   \label{fig:me_growth}
% \end{figure}

%\section{Molecular Evolution}
Study in the field of molecular evolution involves answering questions about the evolutionary history of populations using statistical models and computational algorithms.  The field is termed \emph{molecular} evolution because the questions are related to organic molecules like proteins, amino acids, and the molecules that store genetic information for all life, nucleic acids.  Accordingly, we begin this chapter by reviewing relevant genetic and evolutionary topics.  In section ?, we present genetic distance as a statistical problem.  In section

\section{Nucleic Acids to Proteins, The Central Dogma of Molecular Biology}
Proteins are organic macromolecules that are a fundamental component of life.  They participate in nearly all cellular processes from catalyzing chemical reactions to transporting molecules.  They effect muscle contraction, form various support structures, and can act as toxins.  All proteins are composed of one or more long, linear chains containing different forms of a molecular unit or monomer called an amino acid.  The ordering of amino acids in a chain determines how a protein folds into a functioning three-dimensional structure \cite{anfinsen1972}.

The information about the precise order of a protein's amino acids is stored in another class of biological macromolecules called nucleic acids.  Both types of nucleic acids involved in protein synthesis, deoxyribonucleic acid (DNA) and ribonucleic acid (RNA), are composed of chains of monomers called nucleotides.  Each nucleotide containing a 5-carbon sugar, a phostphate group, and a nitrogen-containing base.  Linear chains of nucleotides are formed via bonds between the phostphate of one nucleotide and the sugar of another to form a sugar-phosphate backbone.  The information stored in nucleic acids is encoded by the ordering of four different forms of nucleotides in the chain with each form having a different nitrogen-containing base.  RNA molecules are composed of single chains of nucleotides containing four different bases: adenine (A), uracil (U), guanine (G), or cytosine (C).  DNA molecules are composed of two nucleotide chains that are bonded via specific nucleotide base pairings, C with G and A with thymine (T), the DNA analog of RNA's U.  The two linked nucleotides chains of DNA form the well known double-helix structure.

The central dogma of molecular biology originally conveyed the idea that once the protein-building information contained in nucleic acids was transferred to a protein, that information could no longer be recovered from the protein.  Nowadays, the central dogma often refers to a more detailed flow of the information, i.e., organisms replicate DNA, transcribe DNA to RNA, and translate RNA to protein.  During transcription, it is the protein-encoding unit of DNA, the gene, that is used as a template to synthesize single-stranded RNA called messenger (mRNA).  During translation three-nucleotide sequences within the mRNA called codons are bound to by the complementary anti-codon of transfer RNA (tRNA).  The tRNA molecules continue to bind to codons along the length of mRNA, each time carrying a particular amino acid to transfer to an elongating polypedtide chain that will become a protein.

The genetic code refers to the mapping of codons to amino acids during protein synthesis.  The code (table \ref{tab:code}) was believed to be universal in that the same codon to amino acid mappings were always employed by all organisms, however some exceptions have been discovered.  For example, there are some nonstandard codons in vertebrate mitochondrial DNA, bacteria, and the nuclear genes of portozoans.  Aside from the codon to amino acid mappings there are two types of special codons.  In the standard genetic code, the codon AUG is referred to as a start codon, because it signals cellular machinery to start reading a gene for translation and also to begin the polypedite chain with the amino acid Methionine.  No tRNA molecules have anticodons for three codons, UAA, UAG, and UGA.  These three codons are called stop codons, because they signal the end of the polypeptide chain.

\input{./genetic_code.tex}

With four possible nucleotides in each of the three positions in a codon, there are $4^3=64$ different codons.  Of these $64$ codons, the $61$ codons that code for amino acids are referred to as sense codons.  There are 20 different amino acids commonly found in proteins, thus the genetic code is redundant.  This means that most amino acids are encoded by multiple codons.  In the standard code, only tryptophan and methionine are encoded by single codons.  Because of this redundancy, a nucleotide substitution within a codon can either result in a change in the amino acid product (nonsynonymous substitution) or no change in the amino acid (synonymous substitution).

\section{Mutations are the Source of All Genetic Variation and the Starting Point for Evolution}
It is generally rare for errors to occur in genes, however when errors called mutations do occur, they provide the source of all genetic variation and the starting point for evolution.  When a new version of a gene called an allele is introduced into a population via mutation, it may, on average, affect the fitness of individuals to pass on their genetic information.  Purifying selection will act to reduce the frequency of the new allele if it reduces average fitness relative to other alleles.  Conversely, positive selection will act to increase the frequency of the new allele if it confers fitness advantages.  If the frequency of the new allele becomes $1$, we say the mutation has been fixed in the population.  If the new allele has no fitness consequences, its fate is left to random genetic drift.

\section{Models of Molecular evolution}
When two populations evolve from one, their genetic divergence can be measured using a quantity called the pairwise distance, the expected number of nucleotide substitutions per nucleotide site.  Of the different methods to calculate pairwise sequence distance, the proportion of different nucleotides is the simplest.  For example, two sequences that are 100 nucleotides long and have 10 nucleotides that are different are said to have a distance proportion of $p=0.1$.  Using $p$ to estimate the distance will often underestimate the true divergence unless the two sequences are very similar.  This is because common or different nucleotides between two sequences could be the result of multiple changes at nucleotide sites, which would not be accounted for with $p$.  Under a constant evolutionary rate, the pairwise distance will increase linearly, but $p$ will not.

A stochastic process is a collection of random variables that are indexed by a set $T$, which often represents time.   If $X(t)=i$, the process $X$ is said to be in state $i$ at time $t$.  For the values $i$ and $j$ from some finite set of states, all $t \ge 0$, and all $s \ge 0$, if
\begin{equation}
  P[X(t+s) = j | X(s)=i, X(u) = x(u), 0 \le u < s]  = P[X(t+s) = j | X(s)=i] = p_{ij}
  \label{eq:Markov}
\end{equation}
holds, then the stochastic process is referred to as a continuous-time Markov process and (\ref{eq:Markov}) is referred to as the Markov property, i.e., the conditional distribution of future states given present and past states depends only on the present state.  Markov processes are well suited for divergence estimation, because the transitions probabilities, $p_{ij}$ satisfy the Chapman-Kolmogorov theorem,
\begin{equation}
  p_{ij}(t_1 + t_2) = \sum_kp_{ik}(t_1)p_{kj}(t_2).
  \label{eq:Chapman-Kolmogorov}
\end{equation}
This means the probability of transitioning from state $i$ to state $j$ in time $t_1+t_2$ is equal to the probably of first transitioning to any intermediate state in time $t_1$ before transitioning to state $j$ in time $t_2$.  Thus, unlike using the proportion of different nucleotides, using a Markov process to estimate genetic divergenc accounts for unobserved nucleotide transitions.  Another property of Markov processes that is usually satisfied when modelling molecular evolution is time-homogeneity.  If (\ref{eq:Markov}) is independent of $s$, the Markov process is referred to as time-homogeneous.

The Jukes and Cantor (JC) model of evolution \cite{JukesCantor1969} uses a continuous time Markov process to estimate genetic distance between DNA sequences and model nucleotide substitution.  The model is useful for understanding properties that are shared with other models of molecular evolution, including more sophisticated models that will be presented later.    It assumes that any nucleotide, $i$, has the same instantaneous rate, $\lambda$, of transitioning to any other nucleotide state, $j$.  These rates can be arranged in a matrix as shown in figure \ref{fig:JC69_Qmat}.
\begin{figure}[h!]
  \centering
\[
Q = \{q_{ij}\} = \bordermatrix{~      & T          & C         & A          & G        \cr
                                    T & -3\lambda  & \lambda   & \lambda   & \lambda   \cr
                                    C & \lambda    & -3\lambda & \lambda   & \lambda   \cr
                                    A & \lambda    & \lambda   & -3\lambda & \lambda   \cr
                                    G & \lambda    & \lambda   & \lambda   & -3\lambda \cr}
\]
\caption{The rate matrix for the Jukes-Cantor model.}
\label{fig:JC69_Qmat}
\end{figure}
The probability of transitioning between states within some small time interval $h$ is $p_{ij}(h) = \lambda h + o(h)$ with $o(h)$ representing some function $g(h)$ such that $g(h)/h \rightarrow 0$ as $h \rightarrow 0$.  The probably of not transitioning is $p_{ii}(h) = 1 - \sum_{j \ne i} p_{ij}(h) = 1 - 3\lambda h + o(h)$, which gives transition probabilities within the small time interval $h$ shown in figure \ref{fig:JC69_Ph}.
\begin{figure}[h!]
  \centering
\[
P(h) = \bordermatrix{~                & T             & C            & A            & G           \cr
                                    T & 1-3\lambda h  & \lambda h    & \lambda h    & \lambda h   \cr
                                    C & \lambda h     & 1-3\lambda h & \lambda h    & \lambda h   \cr
                                    A & \lambda h     & \lambda h    & 1-3\lambda h & \lambda h   \cr
                                    G & \lambda h     & \lambda h    & \lambda h    & 1-3\lambda h \cr}
                                  + o(h)
\]
\caption{}
\label{fig:JC69_Ph}
\end{figure}
When $h$ is 0, the current nucleotide state can not change and the transition probability matrix is the identity matrix.  Factoring out the identity matrix, $I$, gives $P(h) = I + Qh + o(h)$.  The Champman-Kolmogorov equation (\ref{eq:ck}), can be used to find Kolmogrov backward equation (\ref{eq:kb}).
\begin{align}
  P(t+h) &= P(h)P(t) \label{eq:ck} \\
  &= [I + Qh + o(h)]P(t) \nonumber \\
  P(t+h)-P(t) &= QhP(t) + o(h)P(t) \nonumber \\
  [P(t+h)-P(t)]/h &= QP(t) + o(h)P(t)/h \nonumber \\
  P'(t) &= QP(t)  \label{eq:kb} \hskip 100pt \text{(as $h \rightarrow 0$)}
\end{align}
The transition probability matrix $P(t)$, which defines the probability of transitioning from nucleotide $i$ to $j$ in any time $t>0$ can be obtained by solving the backward Kolmorgorov equation. (see Appendix ...).  The solution for the JC model is shown in figure \ref{fig:JC69_Pmat}.
\begin{figure}[h!]
  \centering
\[
P(t) = \bordermatrix{~  & T         & C           & A           & G    \cr
               T & p_{0}(t)  & p_{1}(t)   & p_{1}(t)   & p_{1}(t)  \cr
  C & p_{1}(t)    & p_{0}(t) & p_{1}(t)   & p_{1}(t)  \cr
  A & p_{1}(t)    & p_{1}(t)   & p_{0}(t) & p_{1}(t)  \cr
  G & p_{1}(t)    & p_{1}(t)   & p_{1}(t)   & p_{0}(t) \cr}
\text{, with}\left\{
\begin{array}{l l}
  p_{0}(t) = \frac{1}{4}+\frac{3}{4}e^{-4\lambda t}\\
  p_{1}(t) = \frac{1}{4}-\frac{1}{4}e^{-4\lambda t}
\end{array} \right.
\]
\caption{The transition probability matrix for the Jukes-Cantor model.}

\label{fig:JC69_Pmat}
\end{figure}

Each row of the transition-probability matrix is a probability distribution, and thus sums to $1$.  When $t=0$ the transition-probability matrix is the identity matrix, i.e., in time $t=0$ the current nucleotide state can not change.  The limiting distribution when $\lim_{t \rightarrow \infty} p_{ij}(t) = \pi_j$ represents the probability that the process is in state $j$ after infinite time.  For the JC model, the limiting probabilities, $\bm{\pi} = (\pi_T,\pi_C,\pi_A,\pi_G)$ are $(1/4,1/4,1/4,1/4)$, i.e., when enough time has passed and so many substitutions have occurred, the probably of observing any nucleotide at a site is equal, regardless what the nucleotide state is at time $t=0$.  When the vector of states, $\bm{\pi}$, satisfies the $\bm{\pi} = \bm{\pi} P(t)$ for all $t \ge 0$, $\bm{\pi}$ is referred to as the stationary distribution.  If a stationary distribution of a Markov process exists, it is unique.  The stationary distribution for the JC model is the limiting distribution, $\bm{\pi} = (\pi_T,\pi_C,\pi_A,\pi_G) = (1/4,1/4,1/4,1/4)$.  Another notable property of the JC model is that time, $t$ and rate, $\lambda$ are present as a product, and thus only distance, $\lambda t$ can be estimated.

The data for models of evolution considered in this thesis, including the JC model, are an alignment of nucleotide sequences and a bifurcating phylogenetic tree.  The DNA or RNA data for the sequence alignment is obtained by first using genetic sequencing technology to determine the order of the nucleotides for the same gene or protein-coding region from a number of homologous taxonomic units (taxa).  Next, the data is formatted and arranged so that the sequence for each taxon is a row in a data matrix, $\bm{X}$.  A goal of the alignment is to optimize for homologous characters, either nucleotides or codons, in the columns of $\bm{X}$.  With short and highly conserved sequences this can be done by hand, but usually software-implemented algorithms \citep[e.g.,][]{altschul1990basic, buchfink2021sensitive} are necessary because different accumulated mutations in each of the sequences, such as deleted or inserted nucleotides, will make the alignment challenging.  For all models considered in this thesis, the aligned data at each site $x_h$, a column in $\bm{X}$, is assumed as an independent observational unit.  As an example figure ? shows portions on alignments from three genes, ...

Phylogenetic trees are structures that represent the inferred evolutionary relationships among taxa.  Their two components are nodes and branches.  Nodes represent taxa and branches represent the relationship between the nodes.  Branch lengths are interpreted as genetic distance and when the evolutionary rate is scaled to $1$, time.  Rooted trees have a unique internal node called the root node, which is interpreted as the common ancestor of all other nodes.  When the tree is bifurcating, all other internal nodes have two branches that each connect to a descendant node and a third branch that connects to an ancestral node.  Genetic sequence data is only observed for external nodes, which have just one branch connecting them to their ancestral node.  Unrooted trees do not have a root node and do not define the direction of evolution.

Introduce tree with a diagram to visual a JC example.  Show two versions.. one that is rooted and one that is not.  3 panels (original tree with divergence with 4 or 5 taxa), tree 2 change with rate*time, and third panel is an unrooted version of the tree.  B/C of time-reversibility can show different roots and unrooted tree.
Independence across sites... iid element is a site pattern
Things are independent across lineages.. Markov process
Take one column out and assume ancenstry... sequences that gave rise to ancestral nodes
Talk about how we assume no variation in population.. almost assume individuals
Event of interest along a lineage

- Eigen value decomposition is a more generally useful approach for solving P(t)
- Likelihood caclulation
- Simple JC Example

- Biological motivation for codon models
- Bayesian calculation of site posterors
- Simple Codon example

\subsection{Models of Codon Evolution}

A fundamental concept used in codon models of protein evolution is the neutral theory first proposed by Kimura \cite{K1968}.  Since the predictions of the theory have been widely validated by analysis of genetic data, it is viewed as a highly useful framework for understanding molecular evolution \cite{L1997} and in particular, for detecting molecular adaptation.  The theory asserts that the majority of genetic diversity results from the random fixation of selectively neutral mutations and advantageous mutations are rare.  This means that simply locating regions of genetic diversity is not effective for detecting molecular adaptation.  However, neutral theory provides predictions about the evolution of proteins that can be used as the basis of a hypothesis test \cite{YB2000,Nea2007}. One set of predictions makes use of the redundancy of the genetic code \cite{K1968}.  Because of this redundancy, a nucleotide substitution within a codon can either result in a change in the amino acid product (nonsynonymous substitution) or no change in the amino acid (synonymous substitution). FIGURE HERE.  Under a strictly neutral model (i.e., no selective consequences to any change in an amino acid) the nonsynonymous (\emph{dN}) and synonymous (\emph{dS}) substitution rates would be approximately the same. This leads to a computable index of selection: the ratio of the rates of nonsynonymous to synonymous substitutions, often expressed as the Greek letter \(\omega\) (\emph{dN/dS}=\(\omega\)).  As the two rates are equal under strict neutrality, the ratio is expected to be approximately $1$. However, neutral theory does not predict all amino acid changes are selectively neutral.  It predicts that the observable divergence between lineages was the result of a neutral process.  Genetic variation with negative fitness consequences also arises, but these deleterious mutations are removed by purifying selection \cite{K1968}.  Hence, the theory predicts that functional genes will have an \(\omega<1\) due to purifying selection pressure on deleterious, nonysynonymous changes \cite{K1968}.  The relevant null hypothesis then, is that the \(\omega\) ratio will be less than or equal to $1$ in a protein subject only to purifying selection pressure. The alternative hypothesis of molecular adaptation is characterized by an \(\omega\) ratio greater than $1$.  Nonsynonymous substitutions are fixed at a rate greater than synonymous substitutions (i.e., \(\omega>1\)) when they are advantageous and fixed due to positive selection.

%uncommentThe type of data modelled is an alignment of DNA sequences and a phylogenetic tree, which infers the evolutionary relationship among the lineages.  The sequence alignment is often described as an $s \times n$ matrix, $\mathbf{X}$, with $x_{jh}$ the $h$th nucleotide in the $j$th sequence and $\mathbf{x}_h$ the $h$th column of $\textbf{X}$. A sample tree with values at a particular site are shown in figure.
% \begin{figure}[h!]
%   \centering
%   \includegraphics[scale=0.5]{figures/sample_tree.jpg}
%   \caption[A phylogenetic tree with 6 extant lineages.]{A phylogenetic tree with 6 extant lineages.  The nucleotides for a single site are shown at the tips (nodes 1 to 6).  Ancestral nodes are labelled 0, 7, 8, 9, 10 with the root node 0.  Branch lengths, denoted by $t_{i}$, are a measure of the expected number of substitutions for a nucleotide site.}
%   \label{fig:sample_data}
% \end{figure}
% \subsection{Likelihood Calculation}
% Starting with the root node labelled 0, lineages are inferred to have diverged  according to the branching of the tree and the distances between divergent lineages is denoted by the branch lengths labelled $t_i$.  Extant lineages with known DNA sequences are at the tips of the tree while ancestral lineages with unknown DNA sequences are at the internal nodes.  In the tree shown in figure, the nucleotides, TCACCG, for a single site, column $\mathbf{x}_h$ in $\textbf{X}$, are shown.  Because the evolution of sites is assumed to occur independently, the probability of the entire data is the product of the probabilities of the data at each site.  The log likelihood of the data is
% \[ l = log[L(\lambda)] = log\left[\prod_{h=1}^np(\mathbf{x}_h|\lambda)\right] = \sum_{h=1}^nlog\left[p(\mathbf{x}_h|\lambda)\right]. \]
%In class, we discussed how to calculate the likelihood function for the linkage parameter for fully informative gametes in a three-generation pedigree.  When the phase of the parents or grandparents was unknown, we summed over those possible states to make the calculation.  The likelihood calculation here is similar because the states of the sites at the ancestral nodes are unknown.  For example, at site $h$ the probability of transitioning from the unknown state in ancestral node 7 ($x_{7,h}$) to T in node 1 ($x_{(1,h)}=T$) for a branch length $t_1$ is $ P_{x_{(7,h)},x_{(1,h)}=T}(t_1) = \sum_{k=ACTG}P_{(x_{7,h}),(x_{(1,h)}=T)|x_{(7,h)}=k}(t_1)$.  Omitting the subscript h for the states,
% \begin{eqnarray}
%   f(x_h|\lambda) &=& \sum_{x_{0}}\sum_{x_{6}}\sum_{x_{7}}\sum_{x_{8}}\sum_{x_{9}}\sum_{x_{10}} \pi_{x_{0}}P_{x_{0}x_{9}}(t_9) \nonumber \\
%     && \times P_{x_{9}x_{7}}(t_7)P_{x_{7}T}(t_1)P_{x_{7}C}(t_2)P_{x_{9}x_{8}}(t_8) \nonumber \\
%     && \times P_{x_{8}A}(t_3)P_{x_{8}C}(t_4)P_{x_{6}x_{10}}(t_{10})P_{x_{10}C}(t_5)P_{x_{10}G}(t_6),
% \end{eqnarray}
% which is the probability of the root node, $\pi_{x_{0}} = \frac{1}{4}$, multiplied by the transition probabilities along the branches of the tree.  Because there are $4^{s-1}$ possible combinations for $s-1$ interiors nodes, computation of the probability of the data over all $n$ sites can be computationally expensive.  For amino acid or codon sequences there are $20^{s-1}$ or $61^{s-1}$ combinations so computation is even more expensive.  Algorithms that identify common factors and calculate them a single time according to Horner's rule are employed.  A commonly used algorithm is Flesenstein's pruning algorithm \cite{Felsenstein1981}.

%- Early models like Jukes and Cantor and Muse and Gaut
%- Early Codon models
%- Review of the main statistical tasks (hypothesis testing, parameter estimation, and posterior inference).  It will be valuable to describe how they represent different levels of difficulty, and pose unique statistical challenges.
%- Development of Codon Models for Variable Selection Pressures

%\section{Basics of evolution: purifying selection, neutral evolution, positive selection and redundancy of the Genetic Code}
%\section{Briefly Discuss: Markov Processes, Maximum likelihood, Self and Liang, Bayes}
%- talk about basic stuff like sequences, lR, self and liang, say something about what I've contributed, It can help readers to have new accomplishments in the intro

%\section{Motivation and Overview of Thesis}
%I don’t think that you meed a separate section for "modl", SBA, and additional branch-site work plus a conclusion in the Into.  You will be going over all of them in great detail, and the final chapter will be a formal conclusion section.  I think you can combine all that into a single section in the Intro with a title like “Motivation and Overview of Thesis”. You could get away with as little as 1 paragraph to describe how the branch-site models remain widely used and yet there also remain some un-addressed statistical issues; this will be the motivation for the thesis, and the central theme will be improved inference under this popular family of codon models.  Then, you simply have one paragraph each for the motivation and design of the work in the next three chapters, each given as kind of conceptual “roadmap” but leaving out the details that will follow.


%Identifying positively selected amino acid sites is a challenging statistical task that is useful for investigating the functional consequences of molecular change \citep{yang2005power}.  Several approaches have been developed to detect positive selection within a protein \citep[reviewed in][]{pond2005not,anisimova2009investigating}, but their reliability varies according to the properties of the data in hand.  The most widely used methods employ a codon model to detect an excess in the rate of nonsynonymous substitutions relative to synonymous substitutions ($dN/dS = \omega > 1$), which is an indication of evolution by positive selection.  Proteins evolving under positive selection must retain the capacity to fold into complex structural and functional domains, so the majority of amino acid substitutions will be subject to purifying selection pressure, with $\omega < 1$ \citep{kimura1968evolutionary}.  From extensive surveys of positive selection in real genes, we expect that only a small fraction of amino acid sites will be subject to adaptive change and exhibit an $\omega > 1$ \citep[e.g., ][]{anisimova2007phylogenomic,ge2008protein}.  The sparseness of these sites makes them challenging to identify.

%Two general categories of methods for detecting positively selected amino acid sites include counting and fixed-effect methods.  Counting methods employ ancestral reconstruction of codon states for all internal nodes of a phylogenetic tree to obtain counts of the synonymous and nonsynonymous changes along each of its branches.  The counts inferred for a given site are used to test if $\omega \neq 1$.  Some counting methods use parsimony \citep{fitch1997long,bush1999positive,suzuki1999method}, and others likelihood \citep{suzuki2004new,nielsen2002mapping,nielsen2002detecting,suzuki2004false,pond2005not} to infer the ancestral codon states.  The reconstructions are often similar, but under the likelihood approach, uncertainty about the inference can be summarized via the posterior probabilities of the ancestral states.  Thus, the parsimony based methods must assume that these uncertainties are irrelevant to the statistical test.  While this makes the approach attractive for very large datasets where reliable reconstructions can be obtained relatively quickly \citep{lemey2012counting}, widespread use is hindered by a lack of power when the level of divergence is too low or by the negative impact of substitutional saturation when the level of divergence is too high \citep{pond2005not}.

%An alternative approach is to treat each site as independently relevant to the question of evolution by positive selection, and attempt to fit an $\omega$ parameter to the data at each site.  Thus, the effect of each site on the task of $\omega$ inference is fixed.  Model based testing for $\omega \neq 1$ can be carried out via a standard likelihood ratio test (LR), and no assumptions are required about the distribution of selection pressure, $\omega$.  Although $\omega$ is treated as a site-specific variable, other important variables in the codon model (e.g., branch lengths) are shared among sites, with their values estimated jointly from the complete set of sites.  Results obtained by using these modelling ideas \citep{pond2005not, massingham2005detecting} are encouraging \citep{scheffler2014validity}, however, $\chi^2$ approximations to the distribution of the test statistic assume relatively large numbers of taxa, which is often not the case.  The lack of independence of data across taxa due to homology creates further difficulties for $\chi^2$ approximations.

%A third approach for detecting positive selection at amino acid sites, which is the focus of this thesis, treats the value of \(\omega\) at a site as the realized value of a random variable.  From an alignment of protein coding DNA sequences, the \(\omega\) ratio can be estimated using maximum likelihood under an explicit model of codon evolution \cite{BY2005}.  However, because proteins subject to positive selection must still maintain the capacity to fold into complex structural and functional domains\cite{YNGP2000}, most amino acid sites will still be under purifying selection pressure. Only a small fraction will be subject to adaptive change and exhibit an \(\omega\) ratio \(>\) $1$. This means a single \(\omega\) ratio averaged over the entire sequence of such a protein would be less than $1$ and positive selection would go undetected. This problem is avoided by modeling several codon-site classes subject to different levels of selection pressure (e.g., \(\omega<1\), \(\omega=1\), \(\omega>1\)) \cite{NY1998}.  A Bayesian method can then be used to calculate a posterior probability (PP) that a given codon-site evolved under one of the site classes in the model.  Additionally, a posterior mean (PM) \(\omega\) can be computed as a measure of the selection pressure at each site.  Yang and colleagues introduced and implemented Markov models of codon evolution \cite{GY1994,NY1998} that have been widely used to accomplish these tasks.

%There are recommendations \citep[e.g.,][]{yang1998synonymous} to use a pre-screen that fits two models: one with a distribution that excludes values of \(\omega>1\), and another with the same distribution, except with weight on values of \(\omega>1\) permitted.  The nested-model pre-screening to test if the data conveys any evidence of positive selection is carried by LR test, usually applied with a threshold for declaring a protein under positive selection determined from a chi-square or mixture of chi-square distributions.  While it is known that such distributions are not strictly justified due to the statistical irregularity of the problem, the hope has been that the resulting tests are conservative and do not lose much power in comparison with the same test using the unknown, correct threshold.  In Chapter 2, it is shown that commonly used thresholds need not yield conservative tests, but instead give larger than expected type I error rates.  A modified LR test is described that restores statistical regularity.

%When the null hypothesis of no positive selection is rejected via the LR test, site-wise analysis is warranted.  Site-wise analysis is carried out using Bayes rule to calculate the posterior probability that a site \(h\) evolved under some estimated value of \(\omega\), given the data at site \(h\).  This approach is referred to as empirical Bayes (EB) because the marginal distribution of \(\omega\) is determined from the data.  Conclusions regarding the evolution at a site are made based on the estimated \(\omega\)-values along with their associated posterior probabilities conditioned on the data at the site.  For example, when the largest posterior probability for a site is associated with a value of \(\omega>1\), this is taken as evidence of positive selection at that site.

%A difficulty with this approach is that parameter estimates with large errors can negatively impact Bayesian classification.  By assigning priors to some parameters, Bayes Empirical Bayes (BEB) mitigates this problem.  However, as implemented, it imposes uniform priors, which causes it to be overly conservative in some cases.  When standard regularity conditions are not met and parameter estimates are unstable, inference, even under BEB, can be negatively impacted.  In Chapter 3, an alternative to BEB called smoothed bootstrap aggregation (SBA) is presented.  With SBA, boostrapping is used to respample site patterns from an alignment of protein coding DNA sequences to accommodate the uncertainty in the parameter estimates.  Deriving the correction for parameter uncertainty from the data in hand, in combination with kernel smoothing techniques, improves site specific inference of positive selection.  BEB to SBA are compared by simulation and real data analysis.  Simulation results show that SBA balances accuracy and power at least as well as BEB, and when parameter estimates are unstable, the performance gap between BEB and SBA can widen in favour of SBA.

\chapter{A modified likelihood approach to explore and restore regularity when testing for positive selection}
\section{Introduction}
Tests for detection of positive selection are important for understanding the processes of molecular evolution \citep{nielsen1998likelihood} and the likelihood methods for codon-based models developed in \cite{yang2000codon} are among the most widely used approaches.  An important component of the approach is the likelihood ratio (LR) test, which is used to test for evidence of positive selection within a gene before testing for positive selection at individual amino acid sites (sites).  Standard likelihood theory gives that, when certain regularity conditions are satisfied, the distribution of an LR statistic under the null hypothesis is that of a chi-square random variable with degrees of freedom equal to the difference in the number of parameters fit under the alternative and null hypotheses.  LR tests of positive selection usually employ two additional parameters under the alternative model, often an $\omega>1$ parameter to quantify the positive selection and another parameter for the proportion of sites evolving under $\omega>1$.  This suggests the LR statistics follows a $\chi_2^2$ null distribution.  It has long been recognized, however, that the regularity conditions required for standard likelihood theory are not satisfied for such LR tests of positive selection \citep{anisimova2001accuracy}.

Simulations suggest that a $\chi_2^2$ distribution will give 5\% thresholds for the LR test that are too large \citep{anisimova2001accuracy,wong2004accuracy}.  Drawing upon the non-standard likelihood theory of \cite{self1987asymptotic}, \cite{swanson2003pervasive} indicate that, for model comparison they describe as M8a vs M8, theory supports a 50:50 mixture of a point mass at 0 and a $\chi_1^2$ distribution or, more concisely, a $\chi_0^2/2+\chi_1^2/2$ distribution.  However, \cite{wong2004accuracy} and \cite{anisimova2001accuracy} raised concerns about whether this is the appropriate distribution for comparison.  Nevertheless, the $\chi_0^2/2+\chi_1^2/2$ distribution and, to be more conservative, the $\chi_1^2$ distribution are the most frequently used distributions.  While there have been some simulation studies indicating that the $\chi_1^2$ distribution is indeed conservative in the sense that LR statistics generated under the null tend to be smaller than predicted by a $\chi_1^2$ distribution \citep{anisimova2001accuracy,wong2004accuracy,berlin2005testing}, some of these same studies have found settings where the false positive rates are larger than 5\% \citep{wong2004accuracy,berlin2005testing}.

The lack of fit of chi-square and mixture of chi-square distributions to the null distribution of the likelihood ratio statistic is not entirely surprising since, due to the irregularity of the models, likelihood theory does not support any particular large sample null distribution.  Indeed, we expect the correct distribution generally depends on the particular parameters in the generating null distribution.  In any case, we will argue that the $\chi_0^2/2+\chi_1^2/2$ is often anti-conservative: under the null, LR statistics tend to be larger than is predicted from this distribution.  Borrowing from similar methods in mixture model tests of heterogeneity \citep{chen2001modified} we introduce a modified LR test.  The test statistic is obtained in the same way as for the LR test but with the likelihood replaced by one that penalizes small mass being placed on $\omega > 1$ relative to $\omega=1$.  The advantage with this approach is that it yields a tractable $\chi_0^2/2+\chi_1^2/2$ limiting null distribution.

\section{Theory and Methods}
The base model of \cite{yang2000codon} is a conventional stationary time-reversible Markov model of codon sequence evolution described in \cite{goldman1994codon} with instantaneous rate matrix for transitions from codon $i$ to $j$ given by
\[ Q_{ij}  = \left\{ \begin{array}{ll}
0 & \mbox{if $i$ and $j$ differ at two or three nucleotide positions} \\
\pi_j & \mbox{if $i$ and $j$ differ by one synonymous transversion} \\
\kappa\pi_j& \mbox{if $i$ and $j$ differ by one synonymous transition} \\
\omega\pi_j& \mbox{if $i$ and $j$ differ by one nonsynonymous transversion} \\
\omega\kappa\pi_j& \mbox{if $i$ and $j$ differ by one nonsynonymous transition} \\ \end{array} \right. \]
where $\kappa$ is the transition/transversion parameter, $\pi_j$ is the stationary frequency of codon $j$ and $\omega$ is the parameter quantifying selection pressure as purifying ($\omega<1$), neutral ($\omega=1$) or positive ($\omega>1$).  To model varying selection pressure at sites, the $\omega$ at a site is treated as coming from a probability distribution, which we refer to as the mixing distribution, with various distributional forms allowed \citep{yang2000codon}.  The null hypothesis of interest is that there is no positive selection, which corresponds to the distribution of $\omega$ having all of its mass between 0 and 1.  The alternative is that the distribution allows some positive probability of an $\omega >1$.  For example, following the naming conventions of \cite{yang2000codon} and \cite{berlin2005testing}, null model M1a uses a distribution with mass at an $\omega_0<1$ and at $\omega_1=1$.  The corresponding alternative model, M2a, adds an $\omega_2>1$ to the M1a distribution.

For any of the models considered, the probability of a site pattern $x$ can be expressed as a mixture over choices of $\omega$ of the following form
\begin{equation}
\label{eq:mixmod}
p(x;\beta,p_+) = p_0 p(x|\omega < 1; \zeta,\lambda) + (1-p_+)(1-p_0) p(x|1;\zeta) + p_+ (1-p_0) p(x|\omega_+;\zeta).
\end{equation}
Theoretical derivations are simpler with this unconventional parameterization.  Usually the weights on $\omega$ values are parameters.  For instance, model M2a replaces $(1-p_+)(1-p_0)$ and $p_+(1-p_0)$ with $p_1$ and $p_2$.  For both models M1a and M2a there is a single $\omega_0<1$,
so $p(x|\omega<1;\zeta,\lambda)=p(x|\omega_0;\zeta)$.  Here $\zeta$ denotes parameters common to each $\omega$ and includes edge-lengths and substitution model parameters.  The parameter $\omega_+$ is restricted to be at least 1 and the parameters in $\lambda$ are those involved in the mixture model under purifying selection.  For instance, for model M8a, $\lambda$ gives the parameters of the beta distribution.  We let $\psi=(\zeta^T,\lambda^T,p_0)^T$, the parameters that are common to both null and alternative models.  The LR statistic is
\begin{equation}
  \label{eq:lrs}
  2\{l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\hat\psi_H)\}
\end{equation}
where $l$ and $l_H$ denote the log likelihoods under the alternative and null models, and $\hat p_+$, $\hat\omega_+$, $\hat\psi$, and $\hat\psi_H$ denote the maximum likelihood (ML) estimates under the alternative and null hypotheses.

The likelihood theory of \cite{self1987asymptotic} gives appropriate null distributions in a number of cases where usual regularity conditions do not hold, but it does not generally apply to (\ref{eq:lrs}).  This is because there can be multiple parameter values under the alternative hypothesis that give the null model.  If the alternative model allows mass at $\omega=1$, any $\omega_+>1$ and $p_+=0$ gives the null model.  In addition, for null models that allow mass at $\omega=1$, $\omega_+=1$ and any $p_+$ gives the null model.  The M8a vs M8 comparison considered in \cite{swanson2003pervasive} finesses this difficulty by not allowing the alternative model to have mass at both an $\omega=1$ and an $\omega_+>1$.  Because of this restriction, whenever the true null generating model has mass on $\omega=1$, the only alternative model parameterization giving the generating distribution has $\omega_+=1$; $p_+=0$ and $\omega_+>1$ no longer gives the generating model.  The \cite{swanson2003pervasive} approach restores regularity, but may make it more difficult to model settings where the alternative is true but there is also appreciable mass near $\omega=1$.  In what follows, the model is allowed to have mass at $\omega=1$ under both the null and alternative
model, with additional mass at an $\omega_+>1$ under the alternative hypothesis.

The regularity problems for the Self and Liang theory does not arise if $\omega_+>1$ is fixed, in which case the LR statistic is
\begin{equation}
  \label{eq:lrs_sl}
  2\{l(\hat p_+(\omega_+),\omega_+,\hat\psi(\omega_+)) - l_H(\hat\psi_H)\}
\end{equation}
where $\hat p_+(\omega_+)$ and $\hat\psi(\omega_+)$ denote the ML estimates of $p_+$ and $\psi$ holding $\omega_+$ fixed.  With $\omega_+$ fixed, the only parameter giving a null model is $p_+=0$.  Because that value is on the boundary of the parameter space, standard chi-square results for the limiting distribution of the likelihood ratio statistic do not apply.  However, case 5 of \cite{self1987asymptotic} gives that the large sample distribution is $\chi_0^2/2 + \chi_1^2/2$.  This allows us to say something about the distribution of the usual LR statistic (\ref{eq:lrs}).  Because (\ref{eq:lrs}) can be obtained by maximizing (\ref{eq:lrs_sl}) over $\omega_+\ge 1$, it is sure to be larger than any test statistic (\ref{eq:lrs_sl}) that uses a fixed $\omega$.  Thus, since (\ref{eq:lrs_sl}) has a $\chi_0^2/2 + \chi_1^2/2$ distribution, usual LR statistic values (\ref{eq:lrs}) will tend to be larger than values predicted by the $\chi_0^2/2 + \chi_1^2/2$ distribution.  How much larger LR statistic values tend to be depends upon how much (\ref{eq:lrs_sl}) tends to vary over $\omega_+>1$ which in turn likely depends on how much of the mass of the generating distribution is near $\omega=1$.  Thus, using a $\chi_0^2/2+\chi_1^2/2$ distribution to calculate thresholds for the LR test can generally be expected to give an anti-conservative test: the null hypothesis is rejected too frequently when it is true.

The main reason that the null distribution of the LR statistic is intractable is that $p_+=0$ and any $\omega_+ > 1$ gives the null model.  A similar difficulty arises when testing for mixture structure or heterogeneity in mixture models.  The distribution for the data, $x$, is $\gamma p(x;\theta_1)+(1-\gamma) p(x;\theta_2)$ where $p(x;\theta)$ is a parametric distribution.  A hypothesis of particular interest is that the data corresponds to a single distribution $p(x;\theta)$.  If this is the case, the population might be considered homogeneous when it is otherwise heterogeneous with $\gamma \times 100\%$ of the individuals having parameter $\theta_1$ and the rest having parameter $\theta_2$.  As with tests for positive selection, the reason for a non-standard LR statistic distribution in mixture models is that multiple parameter settings correspond to the null hypothesis: (i) $\gamma=0$ and any $\theta_1$ or (ii) $\theta_1=\theta_2$ and any $\gamma$.  To restore simple limiting distributions while maintaining a test statistic similar to the LR statistic, \cite{chen2001modified} replace log likelihoods with modified log likelihoods that add a term, $C\log[\gamma(1-\gamma)]$ where $C>0$ is a tuning parameter.  Because this term gets very large in magnitude but negative when $\gamma$ is close to 0 or 1, the modified log likelihood is maximized by values with $\gamma$ away from these boundaries, implying that the only way modified ML estimates under the null can approach true values is if $\hat \theta_1 \approx \hat \theta_2$, which restores the sort of regularity needed for chi-square or mixture of chi-square distributions.  The strategy has been effective in a number of different settings \citep[cf.][and references therein]{chen2001modified,chen2004testing,fu2009modified} and we present a similar approach here.

The modified log likelihood we use under the alternative hypothesis is
\begin{equation}
  \label{eq:modlnL}
  \tilde l(p_+,\omega_+,\psi) = l(p_+,\omega_+,\psi) + C \log(p_+)
\end{equation}
The modified LR statistic is then
\begin{equation}
  \label{eq:modlrs}
  2\{\tilde l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\hat\psi_H)\}
\end{equation}
where now the estimates denote the maximizers of the modified log likelihood.  We show in Appendix I, that for $C>0$ the large sample distribution of (\ref{eq:modlrs}) under the null hypothesis is $\chi_0^2/2+\chi_1^2/2$.  Here $C>0$ is a tuning parameter.  While the theory holds for any $C>0$, choosing $C$ too small makes the modified LR statistic too similar to the LR statistic, leading to similar difficulties in behaviour.  We investigate the sensitivity to $C$ through simulations.

We used simulation to estimate LR and modified LR statistic cumulative distribution functions (CDFs) under the null hypothesis.  For each of six simulation scenarios, 10,000 sequence alignments $500$ codons long were generated using 5-, 10-, and 32-taxon trees with branch lengths summing to 3, 6, and 9.  The $5$-taxon tree (Fig. {\ref{sfig:5tree}}) was the same one used in the simulation studies of \cite{wong2004accuracy} and \cite{mingronesba} and the $10$-  and $32$-taxon trees have caterpillar (Fig. {\ref{sfig:10tree}}) and balanced (Fig. {\ref{sfig:32tree}}) topologies.  Sites were simulated to evolve under the M1a model \citep[described in][]{wong2004accuracy,yang2005bayes}, which places weight $p_0$ on a single $\omega_0<0$, with the remaining weight, $1-p_0$, placed on $\omega=1$; thus, the mixing distribution is determined by $(p_0,\omega_0)$.  Each simulation scenario used $\kappa=1$ and equal codon frequencies, but $(p_0,\omega_0)$ varied over scenarios.
\begin{figure}
  \centering
  \begin{subfigure}[t]{.49\textwidth}
    \centering
        <<5ttree,echo=F,warning=F>>=
    phylo.5.t <- read.tree(text="((1:0.333333,2:0.333333):0.333333,3:0.666667,4:0.666667,5:0.666667);")
    plot(phylo.5.t,cex=2)
    edgelabels(c('x','x','x','2x','2x','2x'),adj=c(0,1),bg='white',cex=1.5,col='black',frame="none")
        @
\caption{5-taxon tree}
\label{sfig:5tree}
\end{subfigure}
\begin{subfigure}[t]{.49\textwidth}
  \centering
      <<10ttree,echo=F,warning=F>>=
    phylo.10.t <- read.tree(text="(((((1:0.1,2:0.1):0.1,3:0.2):0.1,4:0.3):0.1,5:0.4):0.1,((((6:0.1,7:0.1):0.1,8:0.2):0.1,9:0.3):0.1,10:0.4):0.1);")
    plot(phylo.10.t,cex=2)
      edgelabels(c('x','x','x','x','x','x','2x','3x','4x','x','x','x','x','x','x','2x','3x','4x'),adj=c(0,1),bg='white',cex=1.5,col='black',frame="none")
        @
\caption{10-taxon tree}
\label{sfig:10tree}
\end{subfigure}
\begin{subfigure}[t]{.75\textwidth}
  \centering
      <<32ttree,echo=F,warning=F>>=
    phylo.32.t <- read.tree(text="(((((1:0.04839,2:0.04839):0.04839,(3:0.04839,4:0.04839):0.04839):0.04839,((5:0.04839,6:0.04839):0.04839,(7:0.04839,8:0.04839):0.04839):0.04839):0.04839,(((9:0.04839,10:0.04839):0.04839,(11:0.04839,12:0.04839):0.04839):0.04839,((13:0.04839,14:0.04839):0.04839,(15:0.04839,16:0.04839):0.04839):0.04839):0.04839):0.04839,((((17:0.04839,18:0.04839):0.04839,(19:0.04839,20:0.04839):0.04839):0.04839,((21:0.04839,22:0.04839):0.04839,(23:0.04839,24:0.04839):0.04839):0.04839):0.04839,(((25:0.04839,26:0.04839):0.04839,(27:0.04839,28:0.04839):0.04839):0.04839,((29:0.04839,30:0.04839):0.04839,(31:0.04839,32:0.04839):0.04839):0.04839):0.04839):0.04839);")
    par(mar=c(0,0,2,0))
    plot(phylo.32.t)
        @
\caption{32-taxon tree}
\label{sfig:32tree}
\end{subfigure}
\caption[]{Phylogenetic tree topologies used in simulation studies, with relative edge lengths shown for the 5- and 10-taxon trees.  All edge lengths are equal in the rooted 32-taxon tree.}
\label{fig:trees}
\end{figure}

To determine the effect of likelihood modification on power, sequence alignments $500$ codons long were simulated under the M2a alternative model \citep{wong2004accuracy,yang2005bayes}, which, by comparison with the M1a mixing distribution, has an additional component, $\omega_2>1$.  The mixing distributions used in simulations had $(p_0,\omega_0)=(0.45,0.5)$ with $(p_2,\omega_2)$ varying over simulation settings.  Codon frequencies were $1/61$ and $\kappa=1$, as was the case for simulations under the null hypothesis and the tree toplogies also matched those used in the simulations under the null.  For each $\omega$-distribution scenario, $10,000$ alignments were generated for the $5$- and $10$-taxon trees and $1000$ alignments for the $32$-taxon tree.  To ensure that comparisons of power with and without likelihood modification corresponded to the same false positive rate, we calibrated the thresholds for significant LR statistics.  For this, $10,000$ sequences were generated under the null with the weight on $\omega>1$ under the alternative settings added to $\omega=1$.  The $95$th percentiles of these LR statistic distributions under both M1a/M2a (C=$0$) and M1a/M2a (C=$2$) were used as the thresholds for calculating power.

\section{Results and Discussion}
\subsection{Modified LR distribution approximations are accurate for most settings}
Figure \ref{fig:CDF32taxaTL9} shows the estimated LR and modified LR statistic CDFs for the M1a/M2a nested model pair for the simulations under the null using the 32-taxon tree with branch lengths summing to $9$.  With likelihood modification, we used a tuning parameter of $C=2$.  Other tuning parameters were tested, but the LR statistic CDFs for values of $C$ between $2$ and $5$ were indistinguishable from those with $C=2$, and CDFs for values of $C<2$ were always between the one for $C=2$ and the one for the unmodified LR statistics.  CDFs for $\chi_0^2/2 + \chi_1^2/2$ are also included in each plot.  For all of the CDFs in Figure \ref{fig:CDF32taxaTL9}, the modified LR statistic distributions are better approximated by a $\chi_0^2/2 + \chi_1^2/2$ distribution than the corresponding distributions without likelihood modification.  Tree topology made little difference as both the LR statistic and modified LR statistic CDFs were similar when data were simulated with different topologies.  Figure \ref{fig:CDF5taxaTL3} and supplementary Figures \ref{fig:CDF5taxaTL6} - \ref{fig:CDF32taxaTL6} contain the LR statistic CDFs for the remaining simulation scenarios.
\begin{figure}
    \centering
<<CDF32taxaTL9,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.25_w0_0.25_32_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.25_w0_0.25_32_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.25_w0_0.25_32_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.25_w0_0.5_32_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.25_w0_0.5_32_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.25_w0_0.5_32_taxa_tl_9_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.5_w0_0.25_32_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.5_w0_0.25_32_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.5_w0_0.25_32_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.5_w0_0.5_32_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.5_w0_0.5_32_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.5_w0_0.5_32_taxa_tl_9_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.75_w0_0.25_32_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.75_w0_0.25_32_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.75_w0_0.25_32_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.75_w0_0.5_32_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.75_w0_0.5_32_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.75_w0_0.5_32_taxa_tl_9_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_32_taxa_m2a_lnl-p0_0.25_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_32_taxa_c2_lnl-p0_0.25_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_32_taxa_m2a_lnl-p0_0.25_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_32_taxa_c2_lnl-p0_0.25_w0_0.5_32_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_32_taxa_m2a_lnl-p0_0.5_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_32_taxa_c2_lnl-p0_0.5_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_32_taxa_m2a_lnl-p0_0.5_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_32_taxa_c2_lnl-p0_0.5_w0_0.5_32_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_32_taxa_m2a_lnl-p0_0.75_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_32_taxa_c2_lnl-p0_0.75_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_32_taxa_m2a_lnl-p0_0.75_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_32_taxa_c2_lnl-p0_0.75_w0_0.5_32_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_32_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_32_taxa_m2a,lrs_p0_0.25_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_32_taxa_m2a,lrs_p0_0.25_w0_0.5_32_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_32_taxa_m2a,lrs_p0_0.5_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_32_taxa_m2a,lrs_p0_0.5_w0_0.5_32_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_32_taxa_m2a,lrs_p0_0.75_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_32_taxa_m2a,lrs_p0_0.75_w0_0.5_32_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a balanced, 32-taxon tree topology with branch lengths summing to 9.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF32taxaTL9}
\end{figure}
\begin{figure}
    \centering
    <<CDF5taxaTL3,echo=F,warning=F>>=
    p0_0.25_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.25_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.25_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.25_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.25_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.25_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.25_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.5_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.25_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.25_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.25_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.5_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_5_taxa_m2a_lnl-p0_0.25_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_5_taxa_c2_lnl-p0_0.25_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_5_taxa_m2a_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_5_taxa_c2_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_5_taxa_m2a_lnl-p0_0.5_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_5_taxa_c2_lnl-p0_0.5_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_5_taxa_m2a_lnl-p0_0.5_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_5_taxa_c2_lnl-p0_0.5_w0_0.5_5_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_5_taxa_m2a_lnl-p0_0.75_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_5_taxa_c2_lnl-p0_0.75_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_5_taxa_m2a_lnl-p0_0.75_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_5_taxa_c2_lnl-p0_0.75_w0_0.5_5_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_5_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_5_taxa_m2a,lrs_p0_0.25_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_5_taxa_m2a,lrs_p0_0.25_w0_0.5_5_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_5_taxa_m2a,lrs_p0_0.5_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_5_taxa_m2a,lrs_p0_0.5_w0_0.5_5_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_5_taxa_m2a,lrs_p0_0.75_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_5_taxa_m2a,lrs_p0_0.75_w0_0.5_5_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.84),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 3.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF5taxaTL3}
\end{figure}

\subsection{False positive rates are too large without modified LR tests}
The false positive rates for each of the LR tests of positive selection under nested models M1a/M2a with and without likelihood modification are shown in Table \ref{tab:FPRates}.  The threshold used to reject each LR test was determined from the $95$th percentile of the $\chi_0^2/2 + \chi_1^2/2$ distribution.  Thus, when the $\chi_0^2/2 + \chi_1^2/2$ does well to approximate the LR statistic distribution, the expected false positive rate is $0.05$.  For each simulation setting under the null hypothesis, the rates were closer to the expected value using the modified likelihood than with the unmodified likelihood.  Excluding the simulation scenario with $5$ taxa and $(p_0,\omega_0)=(0.25,0.5)$ where parameters are almost unidentifiable (discussed below), the false positive rates were between $0.06$ and $0.1$ (average $0.09$) without likelihood modification and between $0.05$ and $0.07$ (average $0.06$) with likelihood modification.  While the false positive rate of the modified LR statistic was usually close to $0.05$, there is a small sample bias using sequences of length 500.  Analyzing datasets simulated under the same settings, but with sequences $1500$ codons long confirms this bias.  All but one of the false positive rates that were $0.06$ with sequences 500 codons long dropped to $0.05$ with sequences $1500$ codons long and the false positive rate for the simulation setting with $(p_0,\omega_0)=(0.25,0.5)$ dropped to $0.06$ with the longer sequences.
\begin{sidewaystable}
  \begin{threeparttable}
    \caption{False positive rates.}
    \centering
    \begin{tabular}[h!]{*{2}l*{18}c}
      \toprule
      &  & \multicolumn{18}{c}{Tree Length 3} \\
      \cmidrule(lr){3-20}
      &  & \multicolumn{6}{c}{5 taxa} & \multicolumn{6}{c}{10 taxa} & \multicolumn{6}{c}{32 taxa} \\
      \cmidrule(lr){3-8} \cmidrule(lr){9-14} \cmidrule(lr){15-20}
      &  & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} \\
      \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20}
      Model                & $p_0=$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ \\
      \cmidrule(lr){1-1} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12} \cmidrule(lr){13-13} \cmidrule(lr){14-14} \cmidrule(lr){15-15} \cmidrule(lr){16-16} \cmidrule(lr){17-17} \cmidrule(lr){18-18} \cmidrule(lr){19-19} \cmidrule(lr){20-20}
      M2a (C=0)          &        & .10   & .09  & .09   & .10   & .08  & .07   & .09   & .09  & .08   & .09   & .08  & .06   & .09   & .08  & .08   & .08   & .08  & .07 \\
      M2a (C=2)          &        & .06   & .06  & .05   & .08   & .06  & .06   & .06   & .06  & .05   & .07   & .06  & .05   & .06   & .06  & .05   & .07   & .06  & .06 \\
      \rule{0pt}{4ex}
      &  & \multicolumn{18}{c}{Tree Length 6} \\
      \cmidrule(lr){3-20}
      &  & \multicolumn{6}{c}{5 taxa} & \multicolumn{6}{c}{10 taxa} & \multicolumn{6}{c}{32 taxa} \\
      \cmidrule(lr){3-8} \cmidrule(lr){9-14} \cmidrule(lr){15-20}
      &  & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} \\
      \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20}
                         & $p_0=$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ \\
      \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12} \cmidrule(lr){13-13} \cmidrule(lr){14-14} \cmidrule(lr){15-15} \cmidrule(lr){16-16} \cmidrule(lr){17-17} \cmidrule(lr){18-18} \cmidrule(lr){19-19} \cmidrule(lr){20-20}
      M2a (C=0)          &        & .10   & .09  & .08   & .10   & .09  & .08   & .10   & .09  & .08   & .10   & .08  & .07   & .09   & .10  & .09   & .09   & .09  & .07 \\
      M2a (C=2)          &        & .06   & .06  & .05   & .08   & .06  & .07   & .06   & .05  & .05   & .07   & .06  & .05   & .05   & .06  & .06   & .07   & .07  & .05 \\
      \rule{0pt}{4ex}
      &  & \multicolumn{18}{c}{Tree Length 9} \\
      \cmidrule(lr){3-20}
      &  & \multicolumn{6}{c}{5 taxa} & \multicolumn{6}{c}{10 taxa} & \multicolumn{6}{c}{32 taxa} \\
      \cmidrule(lr){3-8} \cmidrule(lr){9-14} \cmidrule(lr){15-20}
      &  & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} \\
      \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20}
                         & $p_0=$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ \\
      \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12} \cmidrule(lr){13-13} \cmidrule(lr){14-14} \cmidrule(lr){15-15} \cmidrule(lr){16-16} \cmidrule(lr){17-17} \cmidrule(lr){18-18} \cmidrule(lr){19-19} \cmidrule(lr){20-20}
      M2a (C=0)          &        & .10   & .09  & .08   & .10   & .09  & .07   & .09   & .09  & .08   & .10   & .08  & .06   & .09   & .09  & .08   & .09   & .09  & .08 \\
      M2a (C=2)          &        & .07   & .05  & .06   & .08   & .07  & .06   & .06   & .06  & .05   & .06   & .06  & .05   & .05   & .05  & .05   & .06   & .06  & .06 \\
      \bottomrule
    \end{tabular}
    \label{tab:FPRates}
    \begin{tablenotes}
      \small
    \item  False positive rates for LR tests of positive selection under nested models M1a/M2a with and without likelihood modification.  For each of six simulations scenarios with varying weights and values for two site classes, $\omega<1$ and $\omega=1$, 10,000 sequence alignments $500$ codons long were generated using 5-, 10-, and 32-taxon tree topologies with branch lengths summing to 3, 6, or 9.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown in column and row labels.  Modified likelihood tuning parameters of $C=0$ (no likelihood modification) and $C=2$ were used.  The LR statistics were compared to the $95$th percentile of the $\chi_0^2/2 + \chi_1^2/2$ distribution.
    \end{tablenotes}
  \end{threeparttable}
\end{sidewaystable}

\subsection{Power of the modified LR tests is comparable to re-calibrated LR tests}
Likelihood ratio tests are generally expected to have power that is in some sense optimal \citep[cf Section 5.4.4 of ][]{bickel2015mathematical}.  By modifying the likelihood ratios, it is possible that some loss of power will accrue.  Figure \ref{fig:powerTl3} shows the power curves, using a threshold calibrated to have Type I error rate $0.05$, with and without likelihood modification.  The plots suggest that likelihood modification has minimal impact on power.
\begin{figure}
  \centering
<<powerPlots,echo=FALSE>>=
lnLs.5taxa.tl3 <- read.csv("~/scm/modl.git/sim/alt/5_taxa_balanced_tree/data/lnLs.csv")
lnLs.10taxa.tl3 <- read.csv("~/scm/modl.git/sim/alt/10_taxa_balanced_tree/data/lnLs.csv")
lnLs.32taxa.tl3 <- read.csv("~/scm/modl.git/sim/alt/32_taxa_balanced_tree/data/lnLs.csv")

power.df <- data.frame(power=0,tl=rep(c(3,6,9),each=90),
                       ntaxa=rep(c(5,10,32),each=30,times=3),
                       p2=rep(c(0.02,0.06,0.1),each=10,times=9),
                       w2=rep(c(1.05,1.2,1.5,3,5),each=2),
                       model=rep(c('M2a (C=0)','M2a (C=2)'),times=135))

i <- 1
## num taxa == 5
for (p2 in c('0.02','0.06','0.1')) {
    for (w2 in c('1.05','1.2','1.5','3','5')) {
        for (mod in c('m2a','c2')) {
            lrs <- 2*(lnLs.5taxa.tl3[,paste('p2_',p2,'_w2_',w2,'_',mod,sep='')] - lnLs.5taxa.tl3[,paste('p2_',p2,'_w2_',w2,'_m1a',sep='')])
            lrs.cal <- 2*(lnLs.5taxa.tl3[,paste('p2_0_',mod,sep='')] - lnLs.5taxa.tl3[,'p2_0_m1a'])
            power <- length(lrs[lrs >= sort(lrs.cal)[9500]])/10000
            power.df[i,1] <- power
            i <- i+1
        }
    }
}

## num taxa == 10
for (p2 in c('0.02','0.06','0.1')) {
    for (w2 in c('1.05','1.2','1.5','3','5')) {
        for (mod in c('m2a','c2')) {
            lrs <- 2*(lnLs.10taxa.tl3[,paste('p2_',p2,'_w2_',w2,'_',mod,sep='')] - lnLs.10taxa.tl3[,paste('p2_',p2,'_w2_',w2,'_m1a',sep='')])
            lrs <- lrs[!is.na(lrs)]
            lrs.null <- 2*(lnLs.10taxa.tl3[,paste('p2_0_',mod,sep='')] - lnLs.10taxa.tl3[,'p2_0_m1a'])
            th <- sort(lrs.null)[9500]
            power <- length(lrs[lrs >= th])/length(lrs)
            power.df[i,1] <- power
            i <- i+1
        }
    }
}

## num taxa == 32
for (p2 in c('0.02','0.06','0.1')) {
    for (w2 in c('1.05','1.2','1.5','3','5')) {
        for (mod in c('m2a','c2')) {
            lrs <- 2*(lnLs.32taxa.tl3[,paste('p2_',p2,'_w2_',w2,'_',mod,sep='')] - lnLs.32taxa.tl3[,paste('p2_',p2,'_w2_',w2,'_m1a',sep='')])
            lrs <- lrs[!is.na(lrs)]
            lrs.null <- 2*(lnLs.32taxa.tl3[,paste('p2_0_',mod,sep='')] - lnLs.32taxa.tl3[,'p2_0_m1a'])
            th <- sort(lrs.null)[950]
            power <- length(lrs[lrs >= th])/length(lrs)
            power.df[i,1] <- power
            i <- i+1
        }
    }
}

power.tl3.data <- subset(power.df, tl==3)

power.tl3.plot <- ggplot(power.tl3.data,aes(w2,power)) +
    #ggtitle("Tree Length 3") +
    coord_cartesian(xlim=c(1,5), ylim=c(0,1)) +
    labs(x=expression(omega[2]),y="Power") +
    geom_point(aes(group=model,shape=model),size=2) +
    geom_line(aes(linetype=model),size=.4) +
    scale_linetype_manual(values=c("dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)')) +
    ##scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
    scale_x_continuous(trans='log2',breaks=scales::pretty_breaks(n=5)) +
    ##scale_y_continuous(breaks=c(scales::pretty_breaks(n=5)),trans='log2') +
    ##scale_x_log10() +
    facet_grid(p2~ntaxa,labeller=label_bquote(rows=p[2]*'='*.(p2),cols=.(ntaxa)*' Taxa'))

power.tl3.plot +
    theme(panel.spacing=unit(0,"lines"),
          panel.background=element_blank(),
          strip.background=element_blank(),
          plot.title = element_text(hjust = 0.5),
          legend.title=element_blank(),
          legend.text.align=0,
          legend.key=element_rect(fill="transparent"),
          legend.position=c(0.9,.728),
          legend.key.width=unit(1.3,"line"),
          axis.line=element_line(colour="black"),
          text=element_text(size=14),
          panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{Comparison of power under model M2a without (C=0) and with (C=2) likelihood modification.  For each simulation setting, 10,000 (5 and 10 taxa) or 1,000 (32 taxa) alignments were generated with 500 codons and $45\%$ weight on $\omega=0.5$, $p_2$ weight on $\omega_2$ and the remaining weight on $\omega=1$.}
  \label{fig:powerTl3}
\end{figure}

\subsection{Modified likelihood improves estimation for difficult real data settings}
We analyzed the same $16$ genes described and analyzed in \cite{mingronesba} and the results are summarized in Table \ref{tab:realDataResultsOverview}.  For each of the genes \cite{mingronesba} described as \textit{regular} cases with ML estimation showing no evidence of instabilities and bootstrap parameter distributions having low variance (lysin, \textit{nuoL3}, \textit{pol}, \textit{RafL}, \textit{TrbL-VirB6\_3}, and \textit{vif}), the LR statistics, $\hat{p}_2$, and $\hat{\omega}_2$ are comparable with and without likelihood modification.  On the other hand, for $4$ of the $5$ genes for which the $\omega$ distribution had been poorly estimated in \cite{mingronesba} (\textit{CDH3}, \textit{mivN}, \textit{pgpA}, \textit{tax}, and \textit{TrbL-VirB6\_2}), the results are very different with and without likelihood modification.  Without likelihood modification, an estimated $\hat{p}_2=0.006$ of the sites in \textit{pgpA} were estimated to have evolved under $\hat{\omega}_2=34.7$ and the LR test was rejected.  With likelihood modification, $(p_2,\omega_2)$ was estimated to be $(0.09,1.00)$, the likelihoods under both the null and alternative models are the same, and the LR test was not rejected.  With the exception of \textit{tax}, the estimates of $p_2$ were always larger using modified likelihoods and the corresponding estimates of $w_2$ were always smaller with average decreases in the estimated $\omega_2$ equal to $16.85$, $2.22$, and $0.29$ for the genes described in \cite{mingronesba} as \textit{irregular} (excluding \textit{tax}), \textit{uncategorized}, and \textit{regular}, respectively.  Differences in the branch length and $\kappa$ estimates were minor in all cases.  The only \textit{irregular} gene with estimates that did not vary between the two likelihood approaches was the well-known \textit{tax} gene \citep{suzuki2004false,yang2005bayes}.  Its highly unusual site-pattern distribution gives extreme MLEs with $100\%$ weight ($\hat{p}_2=1$) placed on $\omega>1$.  Because the modified likelihood penalizes against small weight on $\omega>1$, it is not surprising that likelihood modification has no impact on likelihood estimation for the \textit{tax} gene.
\begin{threeparttable}
  \caption{Genes analyzed under models M1a and M2a without (C=0) and with (C=2) likelihood modification.}
  \centering
  \begin{tabular}[]{*{9}l}
    \toprule
    & & & \multicolumn{2}{c}{p-value} & \multicolumn{2}{c}{Tree Length} & \multicolumn{2}{c}{$\hat{p}_2$/$\hat{\omega}_2$} \\
    \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
    \multicolumn{1}{c}{Gene} & \multicolumn{1}{c}{$N_t$} & \multicolumn{1}{c}{$N_c$} & \multicolumn{1}{c}{C=0} & \multicolumn{1}{c}{C=2} & \multicolumn{1}{c}{C=0} & \multicolumn{1}{c}{C=2} & \multicolumn{1}{c}{C=0} & \multicolumn{1}{c}{C=2} \\
    \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9}
    \textit{CDH3}          & 11    & 176  & 1.40e-04 & 8.39e-03 & 0.56 & 0.54 & 0.00/24.57 & 0.08/2.01 \\
    \textit{mivN}          & 5     & 504  & 1.54e-01 & 5.00e-01 & 1.62 & 1.60 & 0.00/5.95  & 0.07/1.00 \\
    \textit{pgpA}          & 5     & 198  & 2.33e-02 & 5.00e-01 & 2.93 & 2.06 & 0.01/34.70 & 0.09/1.00 \\
    \textit{TrbL-VirB6\_2} & 5     & 657  & 4.03e-01 & 5.00e-01 & 2.12 & 2.11 & 0.00/6.17  & 0.11/1.00 \\
    \hline
    lysin                  & 25    & 134  & 0.00e+00 & 0.00e+00 & 8.81 & 8.92 & 0.26/3.25  & 0.27/3.24 \\
    \textit{nuoL3}         & 5     & 499  & 8.26e-14 & 9.63e-14 & 4.58 & 4.75 & 0.04/12.53 & 0.04/12.03\\
    \textit{pol}           & 23    & 947  & 4.33e-15 & 5.61e-15 & 1.31 & 1.32 & 0.02/5.59  & 0.02/5.14 \\
    \textit{RfaL}          & 5     & 403  & 6.20e-06 & 7.89e-06 & 3.46 & 3.50 & 0.07/4.34  & 0.08/3.94 \\
    \textit{TrbL-VirB6\_3} & 5     & 938  & 2.05e-09 & 2.36e-09 & 3.06 & 3.12 & 0.03/5.99  & 0.04/5.76 \\
    \textit{vif}           & 29    & 192  & 2.86e-13 & 3.47e-13 & 2.90 & 2.95 & 0.08/3.56  & 0.10/3.43 \\
    \hline
    $\beta$-globin         & 17    & 144  & 3.69e-03 & 5.84e-03 & 8.40 & 8.62 & 0.03/2.94  & 0.05/2.72 \\
    \textit{ccmF}          & 5     & 635  & 2.54e-05 & 4.40e-05 & 3.41 & 3.28 & 0.01/15.47 & 0.03/8.41 \\
    \textit{ENAM}          & 11    & 1142 & 7.66e-04 & 9.73e-04 & 0.46 & 0.46 & 0.02/5.69  & 0.08/3.41 \\
    \textit{env}           & 13    & 91   & 2.59e-05 & 1.33e-04 & 2.04 & 2.03 & 0.18/3.63  & 0.33/2.79 \\
    \textit{perM}          & 5     & 351  & 1.71e-01 & 2.16e-01 & 1.78 & 1.77 & 0.02/2.57  & 0.04/1.89 \\
    \textit{tax}           & 20    & 181  & 4.17e-03 & 4.17e-03 & 0.13 & 0.13 & 1.00/4.87  & 1.00/4.87 \\
    \bottomrule
  \end{tabular}
  \label{tab:realDataResultsOverview}
  \begin{tablenotes}
    \small
  \item $N_t$: number of taxa; $N_c$: sequence length in number of codons; p-value of the likelihood ratio test for the presence of positive selection using a $\chi^2_0/2 + \chi^2_1/2$ distribution; estimated total tree length; estimated proportion of sites evolving under $\omega>1$: $\hat{p}_2$/$\hat{\omega}_2$.   The top genes represent \textit{irregular} estimation, the middle \textit{regular}, and the bottom genes are uncategorized.
  \end{tablenotes}
\end{threeparttable}

\subsection{Real data results show that using modified likelihood improves estimation and detection of sites under positive selection}
Although site classification was not the focus in this study, we checked for evidence of positive selection at individual sites to assess differences using the two likelihood approaches and three site classifiers.  Spearman correlations for the site posteriors are summarized in Table \ref{tab:sitecors} for Naive empirical Bayes (NEB), Bayes empirical Bayes (BEB) \citep{yang2005bayes} and SBA, the smoothed bootstrap method from \cite{mingronesba}, each with and without likelihood modification.  Site classification was nearly identical using BEB with both likelihood approaches.  This is to be expected since BEB integrates over the uncertainties in the estimates of the $\omega$ distribution using discretized uniform and Dirichlet priors.  Thus, the only parameters under BEB that differ with or without modified ML estimation are the edge-lengths and some parameters in the rate matrix, which tended to change much less than the parameters of the mixing distribution.  By contrast, NEB directly uses the ML estimates of the mixing distribution, which differ considerably with and without likelihood modification.  Consequently, site classification differs substantially under NEB with and without the modified likelihood.  Given that previous studies have indicated that BEB and SBA do better than NEB at balancing accuracy and power for identifying sites under positive selection \citep[e.g., ][]{anisimova2002accuracy, mingronesba}, the stronger agreement between BEB and SBA with NEB using modified likelihood than NEB without modified likelihood suggests modified likelihood is beneficial for detecting sites under positive selection.
\begin{table}
  \centering
  \begin{threeparttable}
    \caption{Spearman rank correlations of site posterior probabilities for different methods of classification under model M2a.}
    \begin{tabular}[!ht]{*{10}l}
      \toprule
      Gene                       & N*/N & N*/B & N*/S & B*/N & B*/B & B*/S & N/B  & N/S  & B/S \\
      \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10}
      \textit{CDH3}              & 0.40 & 1.00 & 1.00 & 0.40 & 1.00 & 1.00 & 0.40 & 0.40 & 1.00 \\
      \textit{mivN}              & 0.76 & 0.99 & 0.97 & 0.77 & 1.00 & 0.96 & 0.77 & 0.78 & 0.96 \\
      \textit{pgpA}              & 0.71 & 0.99 & 0.99 & 0.72 & 1.00 & 0.98 & 0.72 & 0.73 & 0.98 \\
      \textit{TrbL-VirB6\_2}     & 0.72 & 1.00 & 0.98 & 0.72 & 1.00 & 0.98 & 0.72 & 0.72 & 0.98 \\
      \hline
      lysin                      & 1.00 & 1.00 & 0.99 & 0.99 & 1.00 & 0.99 & 1.00 & 0.99 & 0.99 \\
      \textit{nuoL3}             & 1.00 & 0.99 & 0.90 & 0.99 & 1.00 & 0.93 & 0.99 & 0.90 & 0.93 \\
      \textit{pol}               & 0.94 & 0.96 & 0.79 & 0.91 & 1.00 & 0.85 & 0.91 & 0.76 & 0.85 \\
      \textit{RfaL}              & 1.00 & 1.00 & 0.97 & 1.00 & 1.00 & 0.97 & 1.00 & 0.96 & 0.97 \\
      \textit{TrbL-VirB6\_3}     & 0.98 & 0.98 & 0.91 & 1.00 & 1.00 & 0.93 & 1.00 & 0.93 & 0.93 \\
      \textit{vif}               & 1.00 & 1.00 & 0.97 & 1.00 & 1.00 & 0.98 & 1.00 & 0.97 & 0.98 \\
      \hline
      $\beta$-globin             & 0.96 & 0.94 & 0.85 & 0.90 & 1.00 & 0.90 & 0.90 & 0.82 & 0.90 \\
      \textit{ccmF}              & 0.93 & 0.93 & 0.87 & 0.86 & 1.00 & 0.96 & 0.86 & 0.80 & 0.96 \\
      \textit{ENAM}              & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
      \textit{env}               & 0.99 & 1.00 & 0.99 & 1.00 & 1.00 & 0.99 & 1.00 & 0.98 & 0.99 \\
      \textit{perM}              & 0.99 & 1.00 & 0.92 & 0.99 & 1.00 & 0.93 & 0.99 & 0.91 & 0.93 \\
      \bottomrule
    \end{tabular}
    \label{tab:sitecors}
    \begin{tablenotes}
      \small
    \item  N: NEB; B: BEB; S: SBA; *: parameter estimation with modified likelihood. The top genes represent \textit{irregular} estimation, the middle \textit{regular}, and the bottom genes are uncategorized.
    \end{tablenotes}
  \end{threeparttable}
\end{table}

\subsection{Investigation of a problematic setting}
Estimation and inference becomes more challenging with smaller evolutionary distances or fewer taxa, but, perhaps surprisingly, we show the true mixing distribution is at least as important for determining whether a setting is challenging.  This is most evident in the CDFs for the null simulation settings using a 5-taxon tree of length $3$ (Fig. \ref{fig:CDF5taxaTL3}).  Note that the mixing distribution for all of these scenarios is determined by $(p_0,\omega_0)$.  Overall, except for the $(p_0,\omega_0)=(0.25,0.5)$ case, the modified LR statistic distribution is still well approximated by a $\chi_0^2/2 + \chi_1^2/2$ distribution, but for this one setting, neither the LR statistic nor the modified LR statistic distribution is well approximated by the $\chi_0^2/2 + \chi_1^2/2$ distribution.

Histograms of the $\omega_0$ estimates under models M1a and M2a with modified likelihood show the largest variation when $(p_0,\omega_0)=(0.25,0.5)$ (Fig. \ref{fig:w0MLEs}).  Of the $10,000$ sets of modified likelihood MLEs, under M2a $2315$ had $90\%$ or more weight on an $\hat{\omega}_0 \ge 0.65$.  Since the true mixing distribution had two well-separated $\omega$ values, $\omega_0=0.5$ and $\omega_1=1$, the expectation was that the estimated distribution would also have well-separated components with appreciable weight.  The theory leading to the $\chi_0^2/2 + \chi_1^2/2$ approximation relies on this being highly likely with sufficiently large sequence lengths.  It is clear from the simulations that sequence lengths of 500 are not long enough to guarantee well-separated components, which lead to the discrepancy for $(p_0,\omega_0)=(0.25,0.5)$ in Figure \ref{fig:CDF5taxaTL3}.  After removing the $2315$ sets of modified MLEs that had $90\%$ or more weight on an $\hat{\omega}_0 \ge 0.65$, the $\chi_0^2/2+\chi_1^2/2$ CDF provides a good approximation to the actual CDF of the modified LR statistic (Fig. \ref{fig:CDFFiltered}).  This indicates the estimates with $\hat p_0 \ge 0.9$ and $\hat{\omega}_0 \ge 0.65$ were the source of anomalously larger than expected LR statistics.
\begin{figure}
    \centering
    <<w0MLEs, echo=F,warning=F>>=
    p0_0.25_w0_0.25_m2a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.25_m2a_mles.csv",header=F)
    colnames(p0_0.25_w0_0.25_m2a_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.25_w0_0.5_m2a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m2a_mles.csv",header=F)
    colnames(p0_0.25_w0_0.5_m2a_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.5_w0_0.25_m2a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.25_m2a_mles.csv",header=F)
    colnames(p0_0.5_w0_0.25_m2a_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.5_w0_0.5_m2a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.5_m2a_mles.csv",header=F)
    colnames(p0_0.5_w0_0.5_m2a_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.75_w0_0.25_m2a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.25_m2a_mles.csv",header=F)
    colnames(p0_0.75_w0_0.25_m2a_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.75_w0_0.5_m2a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.5_m2a_mles.csv",header=F)
    colnames(p0_0.75_w0_0.5_m2a_mles) <- c('k','p0','p1','p2','w0','w1','w2')

    p0_0.25_w0_0.25_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.25_c2_mles.csv",header=F)
    colnames(p0_0.25_w0_0.25_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.25_w0_0.5_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_c2_mles.csv",header=F)
    colnames(p0_0.25_w0_0.5_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.5_w0_0.25_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.25_c2_mles.csv",header=F)
    colnames(p0_0.5_w0_0.25_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.5_w0_0.5_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.5_c2_mles.csv",header=F)
    colnames(p0_0.5_w0_0.5_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.75_w0_0.25_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.25_c2_mles.csv",header=F)
    colnames(p0_0.75_w0_0.25_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.75_w0_0.5_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.5_c2_mles.csv",header=F)
    colnames(p0_0.75_w0_0.5_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')

    w0 <- c(p0_0.25_w0_0.25_c2_mles$w0,p0_0.25_w0_0.5_c2_mles$w0,
                  p0_0.5_w0_0.25_c2_mles$w0,p0_0.5_w0_0.5_c2_mles$w0,
                  p0_0.75_w0_0.25_c2_mles$w0,p0_0.75_w0_0.5_c2_mles$w0)

    N <- nrow(p0_0.25_w0_0.25_m2a_mles)

    mle.data <- data.frame(w0,
                           weight=rep(c(0.25,0.5,0.75),each=2*N),
                           omega=rep(c(0.25,0.5),each=N,times=3))

    cdf.plot <- ggplot(mle.data,aes(w0,y=..ncount..)) +
        geom_histogram(binwidth=.005,fill=I('black')) +
        labs(x=expression(omega[0]),y='') +
facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(2,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              text=element_text(size=16),
              axis.text.y=element_blank(),
              axis.ticks.y=element_blank())
              #panel.border = element_rect(colour = "black", fill=NA, size=0))
@
\caption[]{MLEs of the $\omega_0$ parameter under model M2a using a modified likelihood parameter of $C=2$ for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 3.  The value of $\omega_0$ and its weight, $p_0$, used to simulate the data are shown as column and row labels.}
  \label{fig:w0MLEs}
\end{figure}
\begin{figure}
    \centering
    <<CDF5taxaFiltered, echo=F,warning=F>>=
    p0_0.25_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_c2_lnLs.csv",sep=',')

    p0_0.25_w0_0.5_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_c2_mles.csv",header=F)
    colnames(p0_0.25_w0_0.5_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')

    drop <- which(p0_0.25_w0_0.5_c2_mles$p0>0.9)

    p0_0.25_w0_0.5_c2_filtered_mles <- p0_0.25_w0_0.5_c2_mles[-drop,]
    colnames(p0_0.25_w0_0.5_c2_filtered_mles) <- c('k','p0','p1','p2','w0','w1','w2')


    p0_0.25_w0_0.5_5_taxa_m1a_filtered_lnl <- p0_0.25_w0_0.5_5_taxa_m1a_lnl[-drop]
    p0_0.25_w0_0.5_5_taxa_c2_filtered_lnl <- p0_0.25_w0_0.5_5_taxa_c2_lnl[-drop]
    lrs_p0_0.25_w0_0.5_5_taxa_c2_filtered <- sort(2*(p0_0.25_w0_0.5_5_taxa_c2_filtered_lnl-p0_0.25_w0_0.5_5_taxa_m1a_filtered_lnl))

    N <- length(p0_0.25_w0_0.5_5_taxa_m1a_filtered_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.5_5_taxa_c2_filtered)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- c(prob.t,1:N/N)

    cdf.data <- data.frame(lrs,cprob,model=rep(c('Theory','M2a (C=2) (Filtered)'),each=N))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="Likelihood Ratio Statistic",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dotted","solid"),labels=c('M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.80,.7),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{Cumulative distribution function (CDF) of filtered, modified likelihood ratio (LR) statistics (C=2).  The modified LR statistics were calculated under the nested model pair M1a/M2a for 10,000 simulated sequence alignments.  The alignments were simulated with 25\% of the sites evolving under $\omega=0.5$ and the remaining sites evolving under $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 3.  A modified likelihood tuning parameters of $C=2$ was used and 2315 LR statistics associated with ML estimates with greater than 90\% of the sites estimated in the $\omega<1$ site class were excluded from the plot.  A $\chi^2_0/2+\chi^2_1/2$ CDF is also included.}
  \label{fig:CDFFiltered}
\end{figure}

We tested whether a pre-screen would be useful for filtering out datasets with $\hat p_0\ge 0.9$ and $\hat \omega_0\ge 0.65$.  The pre-screen we considered was to ignore datasets failing to reject M1a in an M0 versus M1a test.  While this was effective in that it filtered all the $2315$ datasets described above, it also filtered many other datasets.  Consequently, the distribution of M1a/M2a LR statistics remaining after the pre-screen was not as well approximated by the $\chi_0^2/2+\chi_1^2/2$ CDF as the one with the $2315$ datasets manually filtered (Fig. \ref{fig:CDFPS}).  As a second check, we re-simulated the data under the same settings, but with codon frequencies derived from abolone sperm lysin \citep{yang2000maximum}, however the M1a/M2a LR statistic distribution was, again, not well approximated by a $\chi_0^2/2+\chi_1^2/2$ CDF.

\subsection{Parameters can be almost unidentifiable for codon models}
To further investigate why $(p_0,\omega_0)=(0.25,0.5)$ was a difficult setting, we approximated Kullback-Leibler (KL) divergences between pattern distributions coming from $(p_0,\omega_0)=(0.25,0.5)$ and pattern distributions from other mixing distributions (Fig. \ref{fig:KLp0.25w0.5}).  When $KL=0$, two mixing distributions give exactly the same pattern probabilities and the mixing distributions are said to be unidentifiable.  When $KL>0$ but small, distinguishing between the two mixing distributions will be difficult.  A number of the KL divergences in Figure \ref{fig:KLp0.25w0.5} are close to $0$, including those for $(p_0,\omega_0)=(0.5,0.75)$ and $(p_0,\omega_0)=(0.75,0.8)$.  To determine whether the $KL$ was indeed $0$, we restricted attention to all site patterns for pairs of taxa to give tractable calculations.  $KL_s$, calculated using site pattern distributions for a subset of the taxa, satisfies $KL_s\le KL$.  Thus, if any pair of taxa gives $KL_s>0$, then the KL for all taxa must be positive.  For $(p_0,\omega_0)=(0.5,0.75)$ this gives $KL>0.00018$, the maximium KL over pairs.
\begin{figure}
  \centering
<<klp0.25w0.5, echo=F>>=
site.lik.truth <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3_10000_codons/data/sim_p0_0.25_w0_0.5/site_lik_truth.csv",header=F)
colnames(site.lik.truth) <- c('cnt','lik')
n <- sum(site.lik.truth$cnt)
cnt <- site.lik.truth$cnt
y.truth <- site.lik.truth$lik
y.truth[y.truth==0] <- 1e-12
kl <- numeric(0)
vkl <- numeric(0)
for (p in seq(0,1,.1)) {
    for (w in seq(0,1,.1)) {
        site.lik <- read.csv(paste("~/scm/modl.git/sim/null/5_taxa_bl_3_10000_codons/data/sim_p0_0.25_w0_0.5/site_like_p_",p,"_w_",w,".csv",sep=''),header=F)
        y <- site.lik[,2]
        y[y==0] <- 1e-12
        y.bar <- sum(cnt*log(y.truth / y))/n
        kl <- c(kl,y.bar)
        vkl <- c(vkl,sum(cnt*(log(y.truth / y) - y.bar)^2)/n)
    }
}
p <- rep(seq(0,1,.1),each=11)
w <- rep(seq(0,1,.1),times=11)
lb <- kl - 2*sqrt(vkl/10000)
ub <- kl + 2*sqrt(vkl/10000)
kl.data <- data.frame(kl,lb,ub,w,p)
kl.plot <- ggplot(kl.data,aes(p,kl)) +
    labs(x=expression("p"[0]),y="Kullback-Leibler Divergence") +
    geom_point() +
    geom_hline(yintercept=0) + facet_wrap(~w,ncol=3,labeller=label_bquote(cols=omega[0]*'='*.(w))) +
    coord_cartesian(ylim = c(-0.001, 0.01)) +
    geom_errorbar(aes(ymin=lb,ymax=ub))

kl.plot + theme(panel.spacing=unit(0,"lines"),
                panel.background=element_blank(),
                strip.background=element_blank(),
                legend.title=element_blank(),
                legend.key=element_rect(fill="transparent"),
                text=element_text(size=12),
                panel.border = element_rect(colour = "black", fill=NA, size=1))

@
\caption[]{Approximations of the Kullback-Leibler divergences between the distributions of site likelihoods for the generating model and other mixing distributions.  The approximations were obtained as the mean lnL difference between 10,000 site patterns generated under model M1a using a 5-taxon tree with branch lengths summing to 3 and the mixing distribution $(p_0,\omega_0)=(0.25,0.5)$, and other mixing distributions with varying weights on values of $\omega$ ranging from $0$ to $1$.  Error bars for two standard errors $(s_{KL}/\sqrt{10000})$ above and below each Kullback-Leibler estimate are included.  Points missing from each plot are above the visible range.}
\label{fig:KLp0.25w0.5}
\end{figure}

Consideration of KL divergences for mixing distributions $(p_0,\omega_0)=(1,0.75)$ and $(p_0,\omega_0)=(0.5,0.5)$ allows us to show that there are ranges of distributions that are almost unidentifiable.  The maximum KL divergence over pairs of taxa from the $5$-taxon tree was small $(0.00085)$, thus \[p(x;\omega=0.75,\zeta) \approx p(x;\omega=0.5,\zeta)/2 + p(x;\omega=1,\zeta)/2.\]  Multiplying this equation by $p_0^{'}$ and rearranging, one can show \[p(x;\omega=0.75,\zeta)p_0^{'} + p(x;\omega=1,\zeta)(1-p_0^{'})  \approx p(x;\omega=0.5,\zeta)p_0^{'}/2 + p(x;\omega=1,\zeta)/(1-p_0^{'}/2).\]  Thus, mixing distributions $(p_0,\omega_0)=(p_0^{'},0.75)$ give pattern probabilities that difficult to distinguish from $(p_0,\omega_0)=(p_0^{'}/2,0.5)$.  This holds for the range of mixing distributions with $0 \le p_0^{'} \le 1$ (Fig. \ref{fig:KLp0.25w0.5}).

While we have shown that there are regions of mixing-distribution parameter space that can make estimation and inference difficult, our results also show that there are regions where distinguishing between mixing distributions is not difficult, even for the $(p_0^{'},0.75) \approx (p_0^{'}/2,0.5)$ comparison above.  This is because $p_0^{'}/2 \le 0.5$, so pattern probabilities generated from any $(p_0,\omega_0)=(p_0^{'},\omega=0.75)$ can not be consistent with e.g., $(p_0,\omega_0)=(0.75,\omega=0.5)$ (Fig. \ref{fig:KLp0.75w0.5}).  Finally, the good behaviour of the modified LR statistic when used with trees with more taxa and longer branch lengths (e.g., Fig. \ref{fig:CDF32taxaTL9}) suggests these problems are likely restricted to trees with fewer taxa and shorter branch lengths.

\subsection{Concluding Remarks}
We have described challenging issues with mixture models of codon evolution that result in null distributions of LR statistics that are not tractable when testing for positive selection.  A common violation of the regularity conditions under the widely employed M2a model is for small weight to be placed on $\omega>1$.  This results in LR statistics that, when compared to thresholds predicted by a $\chi^2_0/2 + \chi^2_1/2$ distribution, tend to give inflated false positive rates.  By including a penalty in likelihood calculations for small weight on $\omega>1$, in most cases, LR statistic distributions are well approximated by a $\chi^2_0/2 + \chi^2_1/2$ distribution and false positive rates are adequately controlled.  Simulations under the alternative hypothesis show that modifying the LR statistic has minimal impact on power.

The problematic behaviour of LR statistics discussed here arises more broadly with mixtures and is not usually amenable to solutions like those discussed in \cite{self1987asymptotic}.  For instance, \cite{chernoff1995asymptotic} outline a class of problems using a mixture of binomial distributions to evaluate genetic markers for genes representing heterogeneous traits.  Similar to how any $\omega_2>1$ with a weight of $p_2=0$ gives the null under model M2a, any mixing distribution gives the null when the parameters of the two binomials are estimated to be the same.
For many of the settings considered, no closed form expression for the distribution was available and, by contrast with usual chi-square distributions, depended on unknown parameters in the true model under the null hypothesis.  Generally, it appears that when dealing with mixtures in molecular evolution settings, simple limiting distributions like those of \cite{self1987asymptotic} are not likely without some modification of the likelihood.

The simulation results expose an additional difficulty.  For certain generating mixing distributions like $(p_0,\omega_0)=(0.25,0.5)$, there are other mixing distributions that can give very similar pattern probabilities when the number of taxa is small and edge lengths are short.  That some models were found to be almost unidentifiable suggests that there may be mixing distributions and trees that lead to a complete lack of identifiability.  There has been some work to explore identifiability of mixture models of molecular evolution \citep[e.g.,][]{allman2008identifiability,allman2009identifiability,chai2011rogers}, but none of these results apply directly to codon models.  Determining the extent to which these types of issues affect codon models of evolution will be a valuable topic for future research.

The utility of mixture models to solve a variety of problems in the field of molecular evolution has been well established \citep[e.g.,][]{gaston2011phylogenetic,lartillotbayesian,pagelpyhlogentic,wang2008class}.  The modified likelihood proposed here does not eliminate all potential pitfalls associated with the limiting distributions of LR statistics for mixture models of codon evolution, but it does do well to correct for one common issue.  Implementations for a variety of mixture models would be straightforward to implement, thus there is no compelling reason not add a penalty term for small mixing weights, especially, as in the case with model M2a, there is little impact on power.

\chapter{Smoothed Bootstrap Aggregation}
\section{Introduction}
Identifying positively selected amino acid sites is a challenging statistical task that is important for investigating the functional consequences of molecular change \citep{yang2005power}.  Several approaches have been developed to detect positive selection within a protein \citep[reviewed in][]{pond2005not,anisimova2009investigating}, but their reliability varies according to the properties of the data in hand.  The most widely used methods employ a codon model to detect an excess in the rate of nonsynonymous substitutions relative to synonymous substitutions ($dN/dS = \omega > 1$), which is an indication of evolution by positive selection.  Proteins evolving under positive selection must retain the capacity to fold into complex structural and functional domains, so the majority of amino acid substitutions will be subject to purifying selection pressure, with $\omega < 1$ \citep{kimura1968evolutionary}.  From extensive surveys of positive selection in real genes, we expect that only a small fraction of amino acid sites will be subject to adaptive change and exhibit an $\omega > 1$ \citep[e.g., ][]{anisimova2007phylogenomic,ge2008protein}.  The sparseness of these sites makes them challenging to identify.

%\subsection*{Counting and Fixed-Effect Methods}
Two general categories of methods for detecting positively selected amino acid sites include counting and fixed-effect methods.  Counting methods employ ancestral reconstruction of codon states for all internal nodes of a phylogenetic tree to obtain counts of the synonymous and nonsynonymous changes along each of its branches.  The counts inferred for a given site are used to test if $\omega \neq 1$.  Some counting methods use parsimony \citep{fitch1997long,bush1999positive,suzuki1999method}, and others likelihood \citep{suzuki2004new,nielsen2002mapping,nielsen2002detecting,suzuki2004false,pond2005not} to infer the ancestral codon states.  The reconstructions are often similar, but under the likelihood approach uncertainty about the inference can be summarized via the posterior probabilities of the ancestral states.  Thus, the parsimony based methods must assume that these uncertainties are irrelevant to the statistical test.  While this makes the approach attractive for very large datasets where reliable reconstructions can be obtained relatively quickly \citep{lemey2012counting}, widespread use is hindered by a lack of power when the level of divergence is too low or by the negative impact of substitutional saturation when the level of divergence is too high \citep{pond2005not}.

An alternative approach is to treat each site as independently relevant to the question of evolution by positive selection, and attempt to fit an $\omega$ parameter to the data at each site.  Thus, the effect of each site on the task of $\omega$ inference is fixed.  Model based testing for $\omega \neq 1$ can be carried out via a standard likelihood ratio test (LR), and no assumptions are required about the distribution of selection pressure, $\omega$.  Although $\omega$ is treated as a site-specific variable, other important variables in the codon model (e.g., branch lengths) are shared among sites, with their values estimated jointly from the complete set of sites.  Results obtained by using these modelling ideas \citep{pond2005not, massingham2005detecting} are encouraging, and we expect this family of methods will continue to have a role in real data analyses \citep{scheffler2014validity}.  However, $\chi^2$ approximations to the distribution of the test statistic assume relatively large numbers of taxa, which is often not the case.  The lack of independence of data across taxa that is due to phylogeny creates further difficulties for $\chi^2$ approximations.

% \subsection*{Empirical Bayes Methods}
A third approach for detecting positive selection at amino acid sites, which is the focus of this article, treats the value of \(\omega\) at a site as the realized value of a random variable.  A particular model for the distribution of \(\omega\) is chosen and maximum likelihood (ML) is used to fit the distribution to the data as part of an explicit model of codon evolution.  There are recommendations \citep[e.g.,][]{yang1998synonymous} to use a pre-screen that fits two models: one with a distribution that excludes values of \(\omega>1\), and another with the same distribution, except with weight on values of \(\omega>1\) permitted.  This nested-model pre-screening is used to test if the data conveys any evidence of positive selection.  When the null hypothesis of no positive selection is rejected using a LR test, site-wise analysis is warranted.  Site-wise analysis is carried out using Bayes rule to calculate the posterior probability that a site \(h\) evolved under some estimated value of \(\omega\), given the data at site \(h\).  This approach is referred to as empirical Bayes (EB) because the marginal distribution of \(\omega\) is determined from the data.  Conclusions regarding the evolution at a site are made based on the estimated \(\omega\)-values along with their associated posterior probabilities conditioned on the data at the site.  For example, when the largest posterior probability for a site is associated with a value of \(\omega>1\), this is taken as evidence of positive selection at that site.

Because the marginal distribution of $\omega$ is determined from the data %, and $Pr(\omega^{(h)}>1|x_h)$
and the site posterior probabilities always depend on the fitted values of the model parameters (shape parameters of the distribution, edge lengths, etc.), the reliability of EB inference depends on the accuracy of the fitted values.  If they have been accurately estimated, as is often the case with large, information-rich datasets, they can simply be treated as known without errors.  This approach is known as the na\"{i}ve empirical Bayes approach (NEB) \citep{nielsen1998likelihood}.  However, when the fitted values are subject to large errors, the detection of positive selection according to %%$Pr(\omega^{(h)}>1|x_h)$
the posterior probabilities can be negatively impacted and in some cases the false positive rate can be unacceptably high \citep{wong2004accuracy}.  Bayes empirical Bayes (BEB), has been used to adjust for uncertainty in the parameters of the $\omega$ distribution by assigning priors to those parameters and using numerical integration to average over the uncertainty represented by the priors \citep{yang2005bayes}.  Because this tactic can substantially reduce the false positive rate relative to NEB in problematic datasets, BEB has become a popular method for inferring the action of selection at individual sites.  A fully Bayesian approach that also assigns priors to edge-lengths and other parameters is available for the inference of positive selection at sites \citep{aris2003bayes, huelsenbeck2004bayesian}, but it is not as widely employed as EB because it is available for a limited set of models.

BEB does have limitations.  As currently implemented, the BEB approach only accommodates uncertainty in the parameters of the $\omega$ distribution, leaving all others fixed to their fitted values.  Furthermore, only uniform priors are used, which means the adjustment for uncertainty is independent of the signal in the data.  Although these will not be serious limitations for many analyses of real data, we show through simulation and real data analysis that deriving the adjustment for parameter uncertainty from the data can improve inference for some datasets.  To avoid the need for priors, we developed a new approach that uses bootstrapping \citep{efron1979bootstrap,efron1982jackknife} of site patterns to simulate dataset variability and adjust for the uncertainty in the data.  From bootstrap datasets, the distribution of the maximum likelihood estimates (MLEs) can be estimated.  The posterior probabilities %%$\omega^{(h)}>1|x_h$
for positive selection at a site is then obtained using an aggregate value coming from MLEs over bootstrapped data sets, rather than according to a single posterior probability obtained under NEB or BEB.  In principle, bootstrap-based methods should use as many replicates as possible to approximate the infinite-sample bootstrap distribution.  As this is computationally expensive, we use smoothing techniques borrowed from kernel density estimation \citep[Section 3.4]{silverman1987bootstrap,davison1997bootstrap} to obtain an approximation with less computational cost.  We refer to this new approach as smoothed bootstrap aggregation (SBA).  Our simulation results show that SBA balances accuracy and power at least as well as BEB.

We also investigated the behaviour of ML estimation when standard regularity conditions, such as the requirement for true parameter values to be in the interior of the parameter space, are not met.
Codon models fit $\omega$ distributions that, for some data-generating settings, violate regularity conditions, which leads to substantial instability in parameter estimation.  These instabilities have a negative impact on the inference of positive selection under EB, and we show that our new approach is an improvement over both NEB and BEB in such cases.  We also show that results previously reported for the \textit{tax} gene of HTLV \citep{suzuki2004false} are likely a consequence of such instabilities.  The \textit{tax} gene is a well known example where EB is widely considered unreliable, and it has been used to criticize the overall approach.  We provide an explanation for the previous results obtained under EB methods for the \textit{tax} gene, and show that SBA can help diagnose such dubious inferences.

\section{New Approaches}
We developed a new approach for classifying sites we call smoothed bootstrap aggregation (SBA).  SBA uses bootstrapping and kernel smoothing techniques to accommodate uncertainties in MLEs.  Site patterns from a sequence alignment are sampled with replacement to create a number of bootstrap sequence alignments.  For each of the bootstrap sequence alignments, MLEs are calculated.  The usual bootstrap distribution is the empirical distribution of the calculated MLEs.  To avoid difficulties due to 1) low information content in the data, 2) necessarily limited bootstrap sampling and 3) instabilities in the parameter estimates, we used, instead, a kernel density estimate of the bootstrap distribution coming from the MLEs.  The smoothness of the distribution is controlled by a bandwidth parameter, which we set larger than conventional values to give greater smoothing.

While typical applications of bootstrapping use MLEs to calculate confidence intervals and standard errors, we, instead, use the bootstrap to accommodate uncertainty in the posterior probabilities of positive selection at sites.  For any given site in the original sequence alignment, many parameter values are generated from the smoothed bootstrap distribution and substituted into posterior probability formulas to give a distribution of posterior probabilities which reflects parameter uncertainty.  The mean or median of these posteriors is a more stable estimate of the true posterior and is used for classification.  See figure \ref{fig:bootstrap} for an overview of SBA.
% \cmt{smoothed versions of the empirical distribution as a bootstrap distribution.  would involve working directly with the empirical distribution of the MLEs from the bootstrap samples.  SBA differs from the conventional application in two respects.  MLE distributions of $\omega$ weights associated with positive selection are smoothed via kernel density estimation techniques.  The techniques use a kernel function to locally average or smooth MLE distributions and the amount of smoothing is controlled by a bandwidth parameter of the kernel function.  Using bandwidth parameters larger than estimated from bootstrap samples, which gives smoother MLE distributions, compensates for 1) low information content in the data, 2) fewer bootstrap samples and 3) instabilities in the parameter estimates.  Conventional bootstrap applications use the histogram of the MLEs to approximate the sampling distribution of MLEs.}

\tikzstyle{line} = [draw,-stealth]
\tikzstyle{block} = [draw=none]
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node[block] (x) {\(\bm{x}\)};
    \node[block,below of=x] (x2) {\(\bm{x}^{*2}\)};
    \node[block,right of=x2] (dots1) {\(\dots\)};
    \node[block,left of=x2,xshift=-1cm] (x1) {\(\bm{x}^{*1}\)};
    \node[block,right of=dots1] (xB) {\(\bm{x}^{*B}\)};
    \node[block,below of=x1] (t1) {\(\hat{\theta}^{*1}\)};
    \node[block,below of=x2] (t2) {\(\hat{\theta}^{*2}\)};
    \node[block,below of=dots1] (dots2) {\(\dots\)};
    \node[block,below of=xB] (tB) {\(\hat{\theta}^{*B}\)};
    \node[block,below of=t1,xshift=-3cm] (p1) {\(Pr_h(\omega>1|x_h,\hat{\theta}^{*1})\)};
    \node[block,below of=t2] (p2) {\(Pr_h(\omega>1|x_h,\hat{\theta}^{*2})\)};
    \node[block,right of=p2,xshift=1.5cm] (dots3) {\(\dots\)};
    \node[block,below of=tB,xshift=3cm] (pB) {\(Pr_h(\omega>1|x_h,\hat{\theta}^{*B})\)};
%    \node[block,below of=p2,yshift=-.75cm] (agg) {\(avg_iPr_h^i(\omega>1|\bm{x},\theta^i)\) or \(med_ip_h^i(\omega>1|\bm{x},\theta^i)\)};
    \node[block,below of=p2,yshift=-1cm] (agg) {\(\sum_{b=1}^BPr_h(\omega>1|x_h,\hat{\theta}^{*b})/B\)};
    \path[line] (x) -- (x1);
    \path[line] (x) -- (x2);
    \path[line] (x) -- (xB);
    \path[line] (x1) -- (t1);
    \path[line] (x2) -- (t2);
    \path[line] (xB) -- (tB);
    \path[line] (t1) -- (p1);
    \path[line] (t2) -- (p2);
    \path[line] (tB) -- (pB);
    \path[line] (p1) -- (agg);
    \path[line] (p2) -- (agg);
    \path[line] (pB) -- (agg);
  \end{tikzpicture}
  \caption{Bootstrapping site patterns in a codon sequence alignment to classify selection pressure at codon sites.  From an alignment of protein coding DNA sequences, $\bm{x}$, with $n$ codon sites, site patterns are randomly sampled with replacement to obtain a bootstrap sample, $\bm{x}^{*b}$ with $n$ sites.  MLEs, $\hat{\theta}^{*b}$, are then estimated for bootstrap sample $\bm{x}^{*b}$.  Using $\hat{\theta}^{*b}$ and $\bm{x}$, the posterior probability $Pr_h(\omega>1|x_h,\hat{\theta}^{*b})$, that site $h$ is under positive selection is calculated.  These steps are repeated $B$ times to calculate $B$ sets of posterior probabilities.  An aggregate posterior probability that site $h$ is under positive selection is calculated by, for instance, averaging posterior probabilities over bootstrap replicates, $\sum_{b=1}^BPr_h(\omega>1|x_h,\hat{\theta}^{*b})/B$.}
  \label{fig:bootstrap}
\end{figure}

\section{Results}
\subsection{Non-standard ML estimation behaviour}
Parameter estimation by ML has attractive statistical properties, including consistency, efficiency and asymptotic normality, when certain regularity conditions hold \citep{kalbfleisch1985probability,bickel2015mathematical}.  For settings where regularity conditions hold, we verified that we could obtain well-behaved estimates of the parameters of the $\omega$ distribution under two commonly used codon models: M2a \citep{nielsen1998likelihood,yang2005bayes} and M8 \citep{yang2000codon}.  We simulated 100 datasets representing a \textit{regular} estimation problem with an $\omega$ distribution having at least $10\%$ weight on each site class ($45\%$ $\omega=0$, $45\%$ $\omega=0.5$, and $10\%$ $\omega=5$). As expected, MLEs obtained from these data under both M2a and M8 have unimodal and symmetric distributions (fig. \ref{fig:mle_hists}\subref{sfig:sim_M2a_easy},\subref{sfig:sim_M8_easy}).  For the estimates in this \textit{regular} case, there are no indications of departures from the limiting properties predicted by ML theory.

<<mles, echo=F>>=
cat1.scheme4.5.taxa.m2a.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme4_5_taxa_m2a_mles.csv',header=F)
cat1.scheme4.5.taxa.m8.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme4_5_taxa_m8_mles.csv',header=F)
cat1.scheme1.5.taxa.m2a.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme1_5_taxa_m2a_mles.csv',header=F)
## remove rows with crazy large w2
cat1.scheme1.5.taxa.m2a.mles <- cat1.scheme1.5.taxa.m2a.mles[-which(cat1.scheme1.5.taxa.m2a.mles[,7] > 15),]
cat1.scheme1.5.taxa.m8.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme1_5_taxa_m8_mles.csv',header=F)
## remove rows with crazy large w+
cat1.scheme1.5.taxa.m8.mles <- cat1.scheme1.5.taxa.m8.mles[-which(cat1.scheme1.5.taxa.m8.mles[,23] > 17),]
## over bootstraps for a single simulated dataset
cat1.scheme4.5.taxa.m2a.bs.4.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme4_5_taxa_bs_4_m2a_mles_easy.csv',header=F)
cat1.scheme4.5.taxa.m8.bs.73.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme4_5_taxa_bs_73_m8_mles_easy.csv',header=F)
cat1.scheme1.5.taxa.m2a.bs.86.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme1_bs_86_m2a_mles_difficult.csv',header=F)
cat1.scheme1.5.taxa.m8.bs.70.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme1_bs_70_m8_mles_difficult.csv',header=F)
@

\begin{figure}[H]
  \centering
   \begin{subfigure}[t]{0.49\textwidth}\centering Simulated\end{subfigure}
   \begin{subfigure}[t]{0.49\textwidth}\centering Bootstrap\end{subfigure}
   \begin{subfigure}[t]{.01\textwidth}
     \vspace{15 mm}
     \rotatebox{90}{\textit{Regular}}
   \end{subfigure}
   \begin{subfigure}[t]{0.22\textwidth}
     \centering
     <<echo=F>>=
     par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
     hist(cat1.scheme4.5.taxa.m2a.mles[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
     axis(1,cex.axis=3,padj=1)
     par(mar=c(4.5,1.5,2.8,4))
     hist(cat1.scheme4.5.taxa.m2a.mles[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
     axis(1,cex.axis=3,padj=1)
     @
     \caption{M2a}
     \label{sfig:sim_M2a_easy}
   \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(cat1.scheme4.5.taxa.m8.mles[,12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme4.5.taxa.m8.mles[,23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:sim_M8_easy}
  \end{subfigure}
   \begin{subfigure}[t]{0.22\textwidth}
     \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(1-cat1.scheme4.5.taxa.m2a.bs.4.mles[,8]-cat1.scheme4.5.taxa.m2a.bs.4.mles[,9],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme4.5.taxa.m2a.bs.4.mles[,11],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M2a}
    \label{sfig:bs_M2a_easy}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(1-cat1.scheme4.5.taxa.m8.bs.73.mles[,8],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme4.5.taxa.m8.bs.73.mles[,11],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:bs_M8_easy}
  \end{subfigure}
  \par\bigskip
  \begin{subfigure}[t]{.01\textwidth}
    \vspace{12 mm}
    \rotatebox{90}{\textit{Irregular}}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(cat1.scheme1.5.taxa.m2a.mles[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme1.5.taxa.m2a.mles[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M2a}
    \label{sfig:sim_M2a_diff}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(cat1.scheme1.5.taxa.m8.mles[,12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme1.5.taxa.m8.mles[,23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:sim_M8_diff}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(1-cat1.scheme1.5.taxa.m2a.bs.86.mles[,8]-cat1.scheme1.5.taxa.m2a.bs.86.mles[,9],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme1.5.taxa.m2a.bs.86.mles[,11],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M2a}
    \label{sfig:bs_M2a_diff}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(1-cat1.scheme1.5.taxa.m8.bs.70.mles[,8],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme1.5.taxa.m8.bs.70.mles[,11],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:bs_M8_diff}
  \end{subfigure}
  \caption{MLE distributions of the $p_{\omega>1}$ and $\omega_{>1}$ parameters under M2a and M8.  Histograms are over 100 simulated (\subref{sfig:sim_M2a_easy},\subref{sfig:sim_M8_easy},\subref{sfig:sim_M2a_diff},\subref{sfig:sim_M8_diff}) and bootstrap (\subref{sfig:bs_M2a_easy},\subref{sfig:bs_M8_easy},\subref{sfig:bs_M2a_diff},\subref{sfig:bs_M8_diff}) datasets with the bootstrap datasets generated by sampling from one simulated dataset.  Data were simulated under \textit{regular} (\subref{sfig:sim_M2a_easy} - \subref{sfig:bs_M8_easy}) and \textit{irregular} (\subref{sfig:sim_M2a_diff} - \subref{sfig:bs_M8_diff}) conditions.\\ \\\textit{regular} simulation conditions: 5 taxa, $45\% \omega=0$, $45\% \omega=0.5$, and $10\% \omega=5$\\ \textit{irregular} simulation conditions: 5 taxa, $100\% \omega=1$}
  \label{fig:mle_hists}
\end{figure}

% Because the $\omega$ and $p$ parameters have boundaries on the values they can take on, all these distributions can be impacted by boundary conditions leading to a truncation in the distribution.
The regularity condition requiring true parameter values to be in the interior of the parameter space is sometimes violated when using codon models.  For such parameter settings, instabilities or departures from the expected limiting properties of ML estimation can arise including non-Gaussian and over-dispersed distributions of estimates.  To investigate instabilities under models M2a and M8, we simulated 100 datasets representing an \emph{irregular} estimation problem with sparse information, i.e., $100\%$ of the sites at the threshold for positive selection, $\omega=1$.  In figure \ref{fig:mle_hists}\subref{sfig:sim_M2a_diff},\subref{sfig:sim_M8_diff}, in contrast to the results presented in figure \ref{fig:mle_hists}\subref{sfig:sim_M2a_easy},\subref{sfig:sim_M8_easy}, there are instabilities in the MLEs for the parameters representing the proportion of sites under positive selection, $p_{\omega>1}$.  The $p_{\omega>1}$ parameter distributions under both models have mass concentrated on both the lower and upper boundaries of the parameter space, and the distributions of the corresponding $\omega_{>1}$ parameters are concentrated on the lower boundary.  Application of the LR test to filter datasets that convey no evidence of positive selection did not prevent instabilities.  The null hypothesis of no positive selection was rejected for 10 datasets under M2a and 9 under M8, however, the MLE distributions after applying this pre-screening step remained unstable (supplementary fig. S1).

Some of the model M2a MLE instabilities shown in figure \ref{fig:mle_hists}\subref{sfig:sim_M2a_diff},\subref{sfig:sim_M8_diff} are due to the discrete $\omega$ distribution.  True discrete distributions of interest can lie on the boundary of the parameter space, which is a regularity condition violation that gives rise to MLE instabilities.  For instance, consider data generated from an $\omega$ distribution with no mass on $\omega>1$.  Estimates of the $\omega$ distribution will tend to approximate the true distribution and one way this can occur under M2a is when $\hat\omega_{>1} \approx 1$.  When this happens, the likelihood will remain approximately constant over all choices of $p_{\omega=1}$ and $p_{\omega>1}$, giving a sum, $p_{\omega>1}+p_{\omega=1}$, that is approximately the same as that of the MLE.  Consequently, estimates of $p_{\omega=1} + p_{\omega>1}$ are stable, but estimates of $p_{\omega=1}$ and $p_{\omega>1}$ are not, because many different choices give the same sum.  Likewise, when a $p_{\omega>1}$ parameter is estimated near 0, the corresponding $\omega_{>1}$ can take on almost any value without changing the likelihood.  For example, two M2a and six M8 biologically unrealistic estimates of the $\omega_{>1}$ parameter (e.g. $\omega_{>1}=999$) occurred when the corresponding $p_{\omega>1}$ parameters were estimated to be 0.  These estimates were excluded from the $\omega_{>1}$ histograms.  For the data representing an \textit{irregular} estimation problem with all sites simulated with $\omega=1$, two other problematic M2a parameterizations that fit the data equally well occurred often.  First, all the weight was put on the $w_1$ category and second, all the weight was put on the $w_{>1}$ category when it was estimated very close to 1. Although there is virtually no difference in the likelihood scores between the two parameterizations, the NEB posterior probabilities for positive selection were 0 and 1 respectively.  These different MLE instabilities arose with two general types of simulation settings: 1) when fewer site classes were simulated than exist in the fitted model, and 2) when different site classes were simulated with similar levels of selection pressure.
%\cmt{The second simulation setting was especially problematic when the similar site classes were near a value where the $\omega$ distribution was discretized in the fitted model (really just w=1)}.

When working with real data, often only a single sample is available and alternative techniques must be used to approximate distributions of parameter estimates.  One such technique is the bootstrap.  We used our bootstrap-based approach with sequence alignments to investigate properties of the MLE distributions and to detect settings where inference tends to be problematic (see Methods).  While sampling with replacement from a single sample leads to a bootstrap parameter distribution that is a jagged estimate of a smooth distribution, we found the bootstrap, in many cases, can effectively estimate the distributions of MLEs.  Figure \ref{fig:mle_hists}\subref{sfig:bs_M2a_easy},\subref{sfig:bs_M8_easy} shows the distribution of the $\omega$ MLEs associated with positive selection generated over 100 bootstrap samples of a \textit{regular} dataset.  Note the resemblance of the bootstrap distributions in figure \ref{fig:mle_hists}\subref{sfig:bs_M2a_easy},\subref{sfig:bs_M8_easy} to the analogous distributions over simulated datasets in figure \ref{fig:mle_hists}\subref{sfig:sim_M2a_easy},\subref{sfig:sim_M8_easy}.  A comparison of figures \ref{fig:mle_hists}\subref{sfig:sim_M2a_diff},\subref{sfig:sim_M8_diff} and \ref{fig:mle_hists}\subref{sfig:bs_M2a_diff},\subref{sfig:bs_M8_diff} illustrates that when the distribution over multiple samples is problematic, so too is the distribution over bootstrap samples.  Among the 100 bootstrap MLE distributions obtained from the datasets simulated under \textit{irregular} model conditions, we identified 91 of the M2a and 95 of the M8 $p_{\omega>1}$ parameter distributions as unstable using the criterion that at least 5\% mass lies both below 0.2 and above 0.8.  These distributions indicate that the mixture distribution for $\omega$ ``flip-flops'' between few and many sites in a positive selection class.  Recall that under the generating model for these data, no sites are under positive selection.  Plots of the other parameters of the $\omega$ distributions can be found in supplementary figure S2.
Scenarios when the bootstrap distribution is not a good estimate of the true distribution of parameter estimates has been described in other settings, e.g., \citet[p. 81]{efron1994introduction}.
%Bootstrap amplification of unstable parameter estimates has been described in other settings (e.g., \citet[p. 81]{efron1994introduction}.
So, while the bootstrap alone can be helpful for identifying problems, it is not always a robust solution for deriving a correction for parameter uncertainty.
%Nevertheless, the information contained in the unstable bootstrap distributions can have value.  Plots of the empirical CDFs of $\omega$ over simulated datasets do well at estimating the true $\omega$ distribution.
%When mle estimation is pathalogic, the bootstrap can flag, but alone cannot fix it.
%fig. C shows the distribution of X parameters before and after smoothing with the labelled bandwidth parameters.

\subsection{Kernel smoothing improves the bootstrap-based method for approximating MLE distributions}
To avoid results that are a consequence of randomness due to bootstrapping, it is beneficial to choose the number of bootstrap samples, $B$, large enough so that the finite-sample bootstrap distribution approximates the infinite-sample bootstrap distribution well.  However, when regularity conditions are violated there is no guarantee that even the infinite-bootstrap distribution provides an adequate assessment of the variability of an MLE.  We tested this assertion under codon models where the distributions of the $p_{\omega>1}$ parameters were unstable over simulated and bootstrap datasets.  For the data representing \textit{irregular} model conditions described above, we generated 10,000 bootstrap datasets for each of the first 10 simulated datasets. The instabilities that characterize these 10 bootstrap distributions were largely unchanged by increasing $B$ (supplementary fig. S3).  Similar difficulties arise in a variety of bootstrap applications.  As a simple example of the phenomenon, suppose interest is in $\theta$ from a binomial distribution with small $n$ and small $\theta$.  It is possible to sample almost all zeros, in which case the variance of the bootstrap distribution of $\theta$ estimates will be too small.  Such boundary issues related to small samples can similarly be problematic for $\omega$ distributions when estimated weights are close to 0.
%the estimate and the variance of the estimate will both be 0.  Bootstrapping such data will not change these estimates.  Moreover, boundary issues related to small samples can be more problematic for mixture models because multiple distributions can be fit to the same sample of data via alternative mixture weights on the boundaries (zero mixing probabilities).  %Likewise, increasing B does not mitigate this problem in mixture models for codons.

%Even with the larger number of bootstrap samples, the instabilities remain and the true variability in the estimates is not reflected due to the boundary parameter issues described above and small sample biases. The situation is comparable to the bias in estimating $\theta$ from a binomial distribution with small $n$ and small $\theta$, Binom($n$, $\theta$).  It's possible to sample all zeros, and in such an extreme case the estimate and the variance of the estimate will both be 0.

%Even when finite sample bootstrap distributions do well to approximate the infinite-sample distribution, the approximation can be crude, not unlike the rough approximation histograms provide for data generated from smooth, continuous distributions.  Kernel smoothing \citep{akaike1954approximation,parzen1962estimation,rosenblatt1956remarks,wand1994kernel} is a class of nonparametric techniques that can improve the estimation of a distribution.  The techniques use a kernel function to locally average or smooth the observations and the amount of smoothing is controlled by a bandwidth parameter of the kernel function.  (Refs or background for choosing) discuss techniques for choosing an optimal bandwidth parameter when smoothing.  However, these techniques are not effective when regularity conditions in the underlying estimates are not met.  For such cases, a different approach is necessary to make conservative estimates of the MLE distributions.  To do this, we used larger bandwidth parameters with a uniform kernel to compensate for 1) low information content in the data, 2) fewer bootstrap samples and 3) instabilities in the parameter estimates.  fig. X shows plots of the parameter values with different bandwidth parameters.

We used kernel smoothing along with bootstrapping to characterize the uncertainty in MLEs under \emph{difficult} estimation conditions.
%Kernel smoothing \citep{akaike1954approximation,parzen1962estimation,rosenblatt1956remarks,wand1994kernel} is a class of nonparametric techniques that use a kernel function to locally average or smooth the observations and the amount of smoothing is controlled by a bandwidth parameter of the kernel function.
Kernel smoothing is typically used to approximate the infinite-sample distribution more effectively when using a smaller number of bootstrap samples.  However, the standard application of this technique \citep[p. 79]{davison1997bootstrap} was not sufficient when the MLEs were unstable.  For such cases, \textit{over smoothing} (i.e., using a larger than typically considered optimal bandwidth) was necessary to obtain conservative estimates of the MLE distributions, with larger variance, that suppressed the influence of the instabilities (supplementary fig. S4).  By over-smoothing the $p$ parameters of codon models M2a and M8 with a uniform kernel we compensated for 1) low information content in the data, 2) fewer bootstrap samples, and 3) instabilities in the parameter estimates.  For this reason we included over-smoothing of the $p$ parameters in all applications of SBA.
%We use ideas from \citet{silverman1987bootstrap} to create smoothed bootstrap distributions for the $p$ parameters of the $\omega$ distributions for the M2a and M8 models to better approximate the true variation in the distribution of the $p$ parameters.  fig. \ref{fig:simplex_plots} shows the estimates of the $p0$ and $p_1$ parameters of the $\omega$ distribution under model M2a before and after smoothing.  The data were estimated under the same \textit{difficult} conditions described above ($100\%$ sites $\omega=1$).  For the distributions of the $p$ parameters that are unstable over simulated and bootstrap datasets, we tested if a larger number of bootstrap samples stabilized the parameter estimations.  We sampled 10,000 bootstrap datasets for each of the first 10 simulated datasets described above.  fig. X is used to compare the distributions of the parameters with 100 and 10,000 bootstrap samples.  Even with the larger number of bootstrap samples, the instabilities remain and the true variability in the estimates is not reflected due to the boundary parameter issues described above and small sample biases.  The situation is analogous to the problem of estimating $\theta$ from a binomial distribution with small $n$ and small $\theta$, Binom($n$, $\theta$).  It's not unlikely to sample all zeros and in such a case the estimate and the variance of the estimate will both be 0.  For such cases the optimal bandwidth parameter for kernel smoothing is not effective and over smoothing is warranted.  Even with over smoothing where we may over state the variance, conditioning on the data when calculating the posterior probabilities means the the posteriors will be small for unrealistic values for the parameters.  Refs discuss techniques for choosing an optimal bandwidth parameters when smoothing.  These techniques apply when the conditions necessary for bootstrapping to provide useful results are met.  For cases where parameter estimation is \textit{difficult}, as described above, a different approach is necessary to compensate for the lack of consistency.  To do this,

\subsection{Simulation Results}
We used simulation to compare the performance of SBA with BEB and NEB.  The design of our studies was motivated by the more challenging schemes of \citet{wong2004accuracy} and \citet{yang2005bayes}, however our design extends theirs to investigate performance under progressively more model misspecification.  The design is divided into three scenarios covering three levels of model misspecification.  The \textit{Correct Model Scenario} is comprised of four simulation studies (studies $1$-$4$) where the nuisance parameters of the generating model ($\kappa=1$, $\pi_i=1/61$) were freely estimated by the fitted model.  The $\omega$ distributions used to generate the datasets are listed in the third column of table \ref{tab:sim}.

\begin{table}[H]
  \begin{threeparttable}
    \caption{Simulation design and false positive rates under NEB, BEB, and SBA each with models M2a and M8.}
    \centering
    \begin{tabular}[h!]{*{3}l*{6}c}
      \toprule
      Study              & Misspecification   & $\omega$ distribution  & \multicolumn{2}{c}{NEB}       & \multicolumn{2}{c}{BEB}         & \multicolumn{2}{c}{SBA}       \\
      \cmidrule(lr){1-1}   \cmidrule(lr){2-2}   \cmidrule(lr){3-3}       \cmidrule(lr){4-5}              \cmidrule(lr){6-7}                \cmidrule(lr){8-9}
                         &                    &                        & M2a           & M8            & M2a           & M8              & M2a           & M8            \\
      \cmidrule(lr){4-4} \cmidrule(lr){5-5}    \cmidrule(lr){6-6} \cmidrule(lr){7-7}  \cmidrule(lr){8-8} \cmidrule(lr){9-9}
      1                  & None               & 100\% 1                & \textbf{0.34} & \textbf{0.35} & 0.00          & 0.00            & 0.00          & 0.00          \\
      2                  & None               & 50\% 0.5, 50\% 1       & 0.00          & 0.00          & 0.00          & 0.00            & 0.00          & 0.00          \\
      3                  & None               & 50\% 1 50\% 1.5        & \textbf{0.35} & \textbf{0.37} & 0.00          & \textbf{0.05}   & 0.00          & 0.02          \\
      4                  & None               & 45\% 0, 45\% 1, 10\% 5 & 0.00          & 0.00          & 0.00          & 0.01            & 0.00          & 0.00          \\
      \\
      5                  & Mild               & 100\% 1                & \textbf{0.20} & \textbf{0.37} & 0.00          & \textbf{0.24}   & 0.00          & \textbf{0.13} \\
      6                  & Mild               & 50\% 0.5, 50\% 1       & 0.00          & \textbf{0.13} & 0.00          & \textbf{0.11}   & 0.00          & 0.02          \\
      7                  & Mild               & 50\% 1, 50\% 1.5       & \textbf{0.30} & \textbf{0.30} & 0.00          & \textbf{0.39}   & 0.00          & \textbf{0.12} \\
      8                  & Mild               & 45\% 0, 45\% 1, 10\% 5 & 0.00          & 0.04          & 0.00          & \textbf{0.12}   & 0.00          & 0.00          \\
      \\
      9                  & Heavy              & 100\% 1                & \textbf{0.71} & \textbf{0.71} & \textbf{0.55} & \textbf{0.62}   & \textbf{0.13} & \textbf{0.52} \\
      10                 & Heavy              & 50\% 0.5, 50\% 1       & \textbf{0.53} & \textbf{0.50} & 0.00          & 0.00            & 0.00          & 0.01          \\
      \bottomrule
    \end{tabular}
    \label{tab:sim}
    \begin{tablenotes}
      \small
    \item This was for 10,000 simulated datasets with 5-taxa and a total tree length of 3.
    \end{tablenotes}
  \end{threeparttable}
\end{table}

% The $\omega$ distributions used to generate the datasets are as follows.
% \begin{inparaenum}[(1)]
% \textit{Positive Selection Absent}:
% \item $100\%$ $\omega = 1$
% \item $50\%$ $\omega = 0.5$, $50\%$ $\omega = 1.0$.
% \textit{Positive Selection Present}:
% \item $50\%$ $\omega = 1$, $50\%$ $\omega = 1.5$
% \item $45\%$ $\omega = 0$, $45\%$ $\omega = 1$, $10\%$ $\omega = 5$.
% \end{inparaenum}
This scenario design matches selected schemes in \citet{yang2005bayes}.  The \textit{Mild Misspecification Scenario} uses the same $\omega$ distribution as the first scenario as the basis of four additional studies (studies $5$-$8$), but includes mild misspecification of the nuisance parameters (see Methods).  Lastly, the \textit{Heavy Misspecification Scenario}, includes two studies (studies $9$-$10$) with heavy misspecification for the fitted model, which represents a more plausible scenario for the analysis of real sequences.  In one study (study 9) the data were simulated using the highly biased codon frequencies from the Drosophila \textit{GstD1} gene \citep{bielawski2005maximum}.  In the second study (study 10), the generating model is based on a 50/50 mixture of two heterogeneous classes of sites.  One class was generated using equal codon frequencies, $\kappa=1$, and $\omega=0.5$, while the other used the Drosophila \textit{GstD1} gene codon frequencies, $\kappa=8$, and $\omega=1$.  For all 10 simulation studies we simulated 100 alignments, each having 500 codons, using the same 5-taxon tree from \citet{wong2004accuracy}.  The studies in the \textit{Correct Model Scenario} were repeated under model M2a with the 30-taxon tree from the same paper.

%\subsubsection*{Controlling False Positive Classification of Sites - heading to be removed}
%When MLE estimates are unstable, NEB is prone to a higher than acceptable error rate when identifying sites to be under positive selection \cite{yang2005bayes}.  To address the question whether SBA suffers from the same problem, we focused on simulations where positive selection is absent.  We use the frequentist measure of the false positive rate, the proportion of sites under purifying or neutral evolution incorrectly inferred to be under positive selection.  table X summarizes the false positive rates for these simulations for each of the models.  While NEB had a false positive detection rate as high as 0.34, both BEB and SBA had rates of 0 in all cases.  With mild and/or heavy model misspecification....

Table \ref{tab:sim} lists the false positive rates (proportion of sites inferred positively selected among those that are not) using a posterior probability cutoff of $0.95$ for NEB, BEB, and SBA under models M2A and M8.  Study 1 (no misspecification of the nuisance parameters and all sites simulated using $\omega=1$) is an interesting case as NEB exhibits false positives, while BEB and SBA do not.  This is expected; NEB is known to yield unreliable posterior probability calculations in small datasets \citep[e.g., ][]{anisimova2002accuracy,yang2005bayes}.  Because the conditions of study 1 yield unstable parameter estimates (fig. \ref{fig:mle_hists}\subref{sfig:sim_M2a_diff}-\subref{sfig:bs_M8_diff}), the false positives under NEB reflect more than mere sampling errors.  MLE instabilities cause large $p_{\omega>1}$ to occur too often and these values lead to high posterior probabilities for positive selection under M2a and M8. The posterior probability calculations under SBA and BEB are reliable because those approaches do not assume the MLEs have been estimated without error.  \citet{yang2005bayes} suggests that with more data, the problems with NEB controlling false positives can be mitigated.  However, the MLE instabilities persisted in study 1 using a tree topology with 30 taxa (supplementary fig. S5), indicating that large sample sizes do not always ensure accurate predictions.

Relative to simulations with a single $\omega=1$ (study 1), when the $\omega$ distribution was 50\% $\omega=0.5$ and 50\% $\omega=1$ (study 2), the overall signal for positive selection was diminished and all false positive rates were $0$. Conversely, when the $\omega$ distribution was 50\% $\omega=1$ and 50\% $\omega=1.5$ (study 3) there was a slight increase in the NEB false positive rates relative to study 1. Under M2a the false positive rates were $0$ using BEB and SBA, but under M8 they increased to $0.05$ using BEB and to $0.02$ using SBA.  For study 4, because the simulated $\omega$ values for the three sites classes were far enough apart, the false positive rates were well controlled.

%\subsubsection*{The Impact of Model Misspecification}
The introduction of mild model misspecification of the nuisance parameters did not result in higher false positive rates under M2a, but did under M8.  For studies $5$-$8$, the BEB false positive rates (using a $0.95$ posterior probability threshold) under M8 increased in all four cases relative to the corresponding studies ($1$-$4$) in the \textit{Correct Model Scenario}.  The same SBA false positive rates only increased in two cases and by smaller amounts than with BEB.  When heavy model misspecification was introduced in the third scenario, NEB failed to adequately control false positives with rates between 50 and 71\% under both M2a and M8.  BEB and SBA also did not control the false positive rates in study 9, but did in study 10.

The results in table \ref{tab:sim} are over all sites in all simulated datasets.  After applying LR tests at the 0.05 level to filter datasets that convey no evidence of positive selection, none of the false positive rates under BEB or SBA changed.  Supplementary table S1 gives the false positive rates under NEB after the adjustment.  With the exception of two cases, the effect is minimal.  Interestingly, under the null hypothesis, the false positive rates of the LR tests were larger than expected, particularly with model misspecification.



%Accuracy: proportion of sites inferred to be under positive selection truly under positive selection.
%Power:    proportion of truly positively selected sites that were detected by the model.
% To access the site-classification performance of the models we used the same measures as \citet{anisimova2002accuracy}, accuracy and power.
%True positive rate: the number of correctly identified sites over the total number of sites under positive selection

%\subsubsection*{Balancing Accuracy and Power when Classifying Sites - heading to be removed}
When testing for positive selection, we aim for large true positive rates, the proportion of sites truly under positive selection that are correctly identified, sometimes referred to as power.  A difficulty in comparing methods for detecting positive selection is the choice of threshold.  Lower thresholds tend to increase the true positive rate, but tend to also increase the false positive rate.  To ensure that comparisons of power for different methods correspond to the same false positive rate we used Receiver Operator Characteristic (ROC) curves, a convenient way to visualize the balance between accuracy and power for classification problems.  Each point on a curve represents a threshold for the posterior probability of positive selection.  Figure \ref{fig:roc} shows ROC curves for each of the simulations that included positive selection in the generating model (studies 3, 4, 7, and 8).  Curves are also included for the classification of sites using the generating parameters, i.e., the MLEs are fixed to the simulated values.  These curves represent an expected upper limit in performance of site classification (supplementary file S1).  The lower limit for classification, when each site is randomly identified to be under positive selection, is represented by a $y=x$ line.

\begin{figure}[H]
    \centering
  \begin{subfigure}[t]{.48\textwidth}
    \centering
    \hspace{10 mm} \small{$50\%$ $\omega=1$, $50\%$ $\omega=1.5$}
  \end{subfigure}
  \begin{subfigure}[t]{.49\textwidth}
    \centering
    \hspace{10 mm} \small{$45\%$ $\omega=0$, $45\%$ $\omega=1$, $10\%$ $\omega=5$}
  \end{subfigure}
  \\[-2.6ex]
  \begin{subfigure}[t]{.03\textwidth}
    \vspace{8 mm}
    \rotatebox{90}{\textit{\small{Mild Misspec.}} \hspace{4 mm} \textit{\small{Correct Model}}}
  \end{subfigure}
  \begin{subfigure}[t]{0.465\textwidth}
    \centering
    <<echo=F,warning=F>>=

    c1s3.ft.fp     <- scan("../sba.git/doc/paper/data/c1s3_opt_fp.csv",sep=',')
    c1s3.ft.tp     <- scan("../sba.git/doc/paper/data/c1s3_opt_tp.csv",sep=',')

    c1s3m2a.beb.fp <- scan("../sba.git/doc/paper/data/c1s3_m2a_beb_fp.csv",sep=',')
    c1s3m2a.neb.fp <- scan("../sba.git/doc/paper/data/c1s3_m2a_neb_fp.csv",sep=',')
    c1s3m2a.sba.fp <- scan("../sba.git/doc/paper/data/c1s3_m2a_sba_fp.csv",sep=',')
    c1s3m2a.beb.tp <- scan("../sba.git/doc/paper/data/c1s3_m2a_beb_tp.csv",sep=',')
    c1s3m2a.neb.tp <- scan("../sba.git/doc/paper/data/c1s3_m2a_neb_tp.csv",sep=',')
    c1s3m2a.sba.tp <- scan("../sba.git/doc/paper/data/c1s3_m2a_sba_tp.csv",sep=',')

    c1s3m8.beb.fp  <- scan("../sba.git/doc/paper/data/c1s3_m8_beb_fp.csv",sep=',')
    c1s3m8.neb.fp  <- scan("../sba.git/doc/paper/data/c1s3_m8_neb_fp.csv",sep=',')
    c1s3m8.sba.fp  <- scan("../sba.git/doc/paper/data/c1s3_m8_sba_fp.csv",sep=',')
    c1s3m8.beb.tp  <- scan("../sba.git/doc/paper/data/c1s3_m8_beb_tp.csv",sep=',')
    c1s3m8.neb.tp  <- scan("../sba.git/doc/paper/data/c1s3_m8_neb_tp.csv",sep=',')
    c1s3m8.sba.tp  <- scan("../sba.git/doc/paper/data/c1s3_m8_sba_tp.csv",sep=',')

    c2s7.ft.fp     <- scan("../sba.git/doc/paper/data/c2s7_opt_fp.csv",sep=',')
    c2s7.ft.tp     <- scan("../sba.git/doc/paper/data/c2s7_opt_tp.csv",sep=',')

    c2s7m2a.beb.fp <- scan("../sba.git/doc/paper/data/c2s7_m2a_beb_fp.csv",sep=',')
    c2s7m2a.neb.fp <- scan("../sba.git/doc/paper/data/c2s7_m2a_neb_fp.csv",sep=',')
    c2s7m2a.sba.fp <- scan("../sba.git/doc/paper/data/c2s7_m2a_sba_fp.csv",sep=',')
    c2s7m2a.beb.tp <- scan("../sba.git/doc/paper/data/c2s7_m2a_beb_tp.csv",sep=',')
    c2s7m2a.neb.tp <- scan("../sba.git/doc/paper/data/c2s7_m2a_neb_tp.csv",sep=',')
    c2s7m2a.sba.tp <- scan("../sba.git/doc/paper/data/c2s7_m2a_sba_tp.csv",sep=',')

    c2s7m8.beb.fp  <- scan("../sba.git/doc/paper/data/c2s7_m8_beb_fp.csv",sep=',')
    c2s7m8.neb.fp  <- scan("../sba.git/doc/paper/data/c2s7_m8_neb_fp.csv",sep=',')
    c2s7m8.sba.fp  <- scan("../sba.git/doc/paper/data/c2s7_m8_sba_fp.csv",sep=',')
    c2s7m8.beb.tp  <- scan("../sba.git/doc/paper/data/c2s7_m8_beb_tp.csv",sep=',')
    c2s7m8.neb.tp  <- scan("../sba.git/doc/paper/data/c2s7_m8_neb_tp.csv",sep=',')
    c2s7m8.sba.tp  <- scan("../sba.git/doc/paper/data/c2s7_m8_sba_tp.csv",sep=',')

    thin <- floor(seq(1,length(c1s3.ft.fp),length=(length(c1s3.ft.fp)-1)/100))

    fp <- c(c1s3.ft.fp[thin],c1s3m2a.neb.fp[thin],c1s3m2a.sba.fp[thin],c1s3m2a.beb.fp[thin])
    fp <- c(fp,c1s3.ft.fp[thin],c1s3m8.neb.fp[thin],c1s3m8.sba.fp[thin],c1s3m8.beb.fp[thin])
    fp <- c(fp,c2s7.ft.fp[thin],c2s7m2a.neb.fp[thin],c2s7m2a.sba.fp[thin],c2s7m2a.beb.fp[thin])
    fp <- c(fp,c2s7.ft.fp[thin],c2s7m8.neb.fp[thin],c2s7m8.sba.fp[thin],c2s7m8.beb.fp[thin])

    tp <- c(c1s3.ft.tp[thin],c1s3m2a.neb.tp[thin],c1s3m2a.sba.tp[thin],c1s3m2a.beb.tp[thin])
    tp <- c(tp,c1s3.ft.tp[thin],c1s3m8.neb.tp[thin],c1s3m8.sba.tp[thin],c1s3m8.beb.tp[thin])
    tp <- c(tp,c2s7.ft.tp[thin],c2s7m2a.neb.tp[thin],c2s7m2a.sba.tp[thin],c2s7m2a.beb.tp[thin])
    tp <- c(tp,c2s7.ft.tp[thin],c2s7m8.neb.tp[thin],c2s7m8.sba.tp[thin],c2s7m8.beb.tp[thin])

    roc.data <- data.frame(fp,tp,
                               Method=rep(c("OPT","NEB","SBA","BEB"),
                                              each=length(thin),times=4),
                               Study=rep(c('Study 3 - M2a',
                                               'Study 3 - M8',
                                               'Study 7 - M2a',
                                               'Study 7 - M8'),
                                             each=length(thin)*4),
                               Scenario=rep(c('CM','MM'),
                                                each=length(thin)*8))

    roc.plot <- ggplot(roc.data,aes(fp,tp)) +
        coord_cartesian(xlim=c(-0.005,.405), ylim=c(-0.005,.405)) +
        labs(x="False Positive Rate",y="True Positive Rate") +
        geom_line(aes(color=Method,linetype=Method),size=1.2) +
        geom_abline(slope=1,intercept=0,color='gray60') +
        scale_color_manual(values=c("gray30","grey40","grey50","black")) +
        scale_linetype_manual(values=c("dotted","dotdash","dashed","solid")) +
        scale_x_continuous(breaks=c(0.1,0.2,0.3)) +
        facet_wrap(~Study,ncol=2)

    roc.plot +
            theme(panel.margin=unit(0,"lines"),
                  panel.background=element_blank(),
                  strip.background=element_blank(),
                  panel.grid.major=element_line(color="gray95"),
                  legend.position="none",
                  axis.line=element_line(colour="black"),
                  text=element_text(size=20),
                  panel.border = element_rect(colour = "black", fill=NA, size=1))
@
    %\caption{Difficult with no Model Misspecification}
    \label{sfig:roc_hard}
  \end{subfigure}
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    <<echo=F,warning=F>>=

    c1s4.ft.fp     <- scan("../sba.git/doc/paper/data/c1s4_opt_fp.csv",sep=',')
    c1s4.ft.tp     <- scan("../sba.git/doc/paper/data/c1s4_opt_tp.csv",sep=',')

    c1s4m2a.beb.fp <- scan("../sba.git/doc/paper/data/c1s4_m2a_beb_fp.csv",sep=',')
    c1s4m2a.neb.fp <- scan("../sba.git/doc/paper/data/c1s4_m2a_neb_fp.csv",sep=',')
    c1s4m2a.sba.fp <- scan("../sba.git/doc/paper/data/c1s4_m2a_sba_fp.csv",sep=',')
    c1s4m2a.beb.tp <- scan("../sba.git/doc/paper/data/c1s4_m2a_beb_tp.csv",sep=',')
    c1s4m2a.neb.tp <- scan("../sba.git/doc/paper/data/c1s4_m2a_neb_tp.csv",sep=',')
    c1s4m2a.sba.tp <- scan("../sba.git/doc/paper/data/c1s4_m2a_sba_tp.csv",sep=',')

    c1s4m8.beb.fp <-  scan("../sba.git/doc/paper/data/c1s4_m8_beb_fp.csv",sep=',')
    c1s4m8.neb.fp <-  scan("../sba.git/doc/paper/data/c1s4_m8_neb_fp.csv",sep=',')
    c1s4m8.sba.fp <-  scan("../sba.git/doc/paper/data/c1s4_m8_sba_fp.csv",sep=',')
    c1s4m8.beb.tp <-  scan("../sba.git/doc/paper/data/c1s4_m8_beb_tp.csv",sep=',')
    c1s4m8.neb.tp <-  scan("../sba.git/doc/paper/data/c1s4_m8_neb_tp.csv",sep=',')
    c1s4m8.sba.tp <-  scan("../sba.git/doc/paper/data/c1s4_m8_sba_tp.csv",sep=',')

    c2s8.ft.fp  <-    scan("../sba.git/doc/paper/data/c2s8_opt_fp.csv",sep=',')
    c2s8.ft.tp  <-    scan("../sba.git/doc/paper/data/c2s8_opt_tp.csv",sep=',')

    c2s8m2a.beb.fp <- scan("../sba.git/doc/paper/data/c2s8_m2a_beb_fp.csv",sep=',')
    c2s8m2a.neb.fp <- scan("../sba.git/doc/paper/data/c2s8_m2a_neb_fp.csv",sep=',')
    c2s8m2a.sba.fp <- scan("../sba.git/doc/paper/data/c2s8_m2a_sba_fp.csv",sep=',')
    c2s8m2a.beb.tp <- scan("../sba.git/doc/paper/data/c2s8_m2a_beb_tp.csv",sep=',')
    c2s8m2a.neb.tp <- scan("../sba.git/doc/paper/data/c2s8_m2a_neb_tp.csv",sep=',')
    c2s8m2a.sba.tp <- scan("../sba.git/doc/paper/data/c2s8_m2a_sba_tp.csv",sep=',')

    c2s8m8.beb.fp <-  scan("../sba.git/doc/paper/data/c2s8_m8_beb_fp.csv",sep=',')
    c2s8m8.neb.fp <-  scan("../sba.git/doc/paper/data/c2s8_m8_neb_fp.csv",sep=',')
    c2s8m8.sba.fp <-  scan("../sba.git/doc/paper/data/c2s8_m8_sba_fp.csv",sep=',')
    c2s8m8.beb.tp <-  scan("../sba.git/doc/paper/data/c2s8_m8_beb_tp.csv",sep=',')
    c2s8m8.neb.tp <-  scan("../sba.git/doc/paper/data/c2s8_m8_neb_tp.csv",sep=',')
    c2s8m8.sba.tp <-  scan("../sba.git/doc/paper/data/c2s8_m8_sba_tp.csv",sep=',')

    thin <- floor(seq(1,length(c1s4.ft.fp),length=(length(c1s4.ft.fp)-1)/100))

    fp <- c(c1s4.ft.fp[thin],c1s4m2a.neb.fp[thin],c1s4m2a.sba.fp[thin],c1s4m2a.beb.fp[thin])
    fp <- c(fp,c1s4.ft.fp[thin],c1s4m8.neb.fp[thin],c1s4m8.sba.fp[thin],c1s4m8.beb.fp[thin])
    fp <- c(fp,c2s8.ft.fp[thin],c2s8m2a.neb.fp[thin],c2s8m2a.sba.fp[thin],c2s8m2a.beb.fp[thin])
    fp <- c(fp,c2s8.ft.fp[thin],c2s8m8.neb.fp[thin],c2s8m8.sba.fp[thin],c2s8m8.beb.fp[thin])

    tp <- c(c1s4.ft.tp[thin],c1s4m2a.neb.tp[thin],c1s4m2a.sba.tp[thin],c1s4m2a.beb.tp[thin])
    tp <- c(tp,c1s4.ft.tp[thin],c1s4m8.neb.tp[thin],c1s4m8.sba.tp[thin],c1s4m8.beb.tp[thin])
    tp <- c(tp,c2s8.ft.tp[thin],c2s8m2a.neb.tp[thin],c2s8m2a.sba.tp[thin],c2s8m2a.beb.tp[thin])
    tp <- c(tp,c2s8.ft.tp[thin],c2s8m8.neb.tp[thin],c2s8m8.sba.tp[thin],c2s8m8.beb.tp[thin])


    roc.data <- data.frame(fp,tp,
                               Method=rep(c("OPT","NEB","SBA","BEB"),
                                              each=length(thin),times=4),
                               Study=rep(c('Study 4 - M2a',
                                               'Study 4 - M8',
                                               'Study 8 - M2a',
                                               'Study 8 - M8'),
                                             each=length(thin)*4),
                               Scenario=rep(c('CM','MM'),
                                                each=length(thin)*8))

    roc.plot <- ggplot(roc.data,aes(fp,tp)) +
        coord_cartesian(xlim=c(-0.005,.1),ylim=c(-0.005,.805)) +
        labs(x="False Positive Rate",y="True Positive Rate") +
        geom_line(aes(color=Method,linetype=Method),size=1.2) +
        geom_abline(slope=1,intercept=0,color='gray60') +
        scale_shape_manual(values=c(15,18,17,16)) +
        scale_color_manual(values=c("gray30","grey40","grey50","black")) +
        scale_linetype_manual(values=c("dotted","dotdash","dashed","solid")) +
        scale_x_continuous(breaks=c(0.02,0.04,0.06,0.08)) +
        guides(colour = guide_legend(override.aes = list(size=1.4))) +
        facet_wrap(~Study,ncol=2)

    roc.plot +
        theme(panel.background=element_blank(),
              strip.background=element_blank(),
              panel.grid.major=element_line(colour="gray95"),
              panel.margin=unit(0,"lines"),
              axis.line=element_line(colour="black"),
              legend.title=element_blank(),
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.83,.80),
              legend.key.width=unit(6,"line"),
              text=element_text(size=20),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
    %\caption{Difficult with Model Misspecification}
    \label{sfig:roc_easy}
  \end{subfigure}
  \caption[ROC Curves.]{ROC curves for the detection of sites under positive selection for BEB, NEB, and SBA analyses of data generated under two different simulation scenarios: without model misspecification (\textit{Correct Model}, studies 3 and 4) and with mild model misspecification (\textit{Mild Misspecification}, studies 7 and 8).  The data were simulated using a 5-taxon tree topology.  In studies 3 and 7, $50\%$ of the sites were simulated under neutral evolution ($\omega=1$) and $50\%$ of the sites under positive selection ($\omega=1.5$).  In studies 4 and 8, $45\%$ of the sites were simulated under purifying selection ($\omega=0$), $45\%$ under neutral evolution ($\omega=1$) and $10\%$ under positive selection ($\omega=5$).  Each plot includes a line for the lower bound (y=x) and an expected upper bound (OPT) when classification is made using the generating model parameters.  Curves for NEB do not always cover the whole range of false positive rates, because NEB sometimes estimates the $\omega$ distribution with all mass on $\omega > 1$.  In these cases, even with a posterior probability cut-off of 1, NEB still incorrectly classifies sites to be under positive selection.}
  \label{fig:roc}
\end{figure}

The introduction of mild misspecification made the task of detecting sites under positive selection more difficult in study 8.  This is evident from the shifting of the ROC curves down and to the right (lower rates of true positives for a given false positive rate) in study 8 relative to the corresponding simulations without the misspecification of the nuisance parameters in study 4.  The same effect was not observed between the ROC curves of studies 3 and 7.

In all cases, the SBA curves were at least as close as the BEB curves to the expected upper limit.  In studies 3 and 7 (50\% $\omega=1$, 50\% $\omega=1.5$), under M2a, where the estimates of the $p_{\omega>1}$ and $\omega_{>1}$ parameters were unstable (supplementary fig. S6), the gaps between the curves for BEB and SBA were the largest, even when the number of taxa was increased from 5 to 30 (fig. \ref{fig:roc_30_taxa}).  This indicates that SBA, for a given false positive rate, had more power to detect sites under positive selection than BEB.
<<ROC_30_data,echo=F>>=
c1s3.30.ft.fp  <-    scan("../sba.git/doc/paper/data/c1s3_30t_opt_fp.csv",sep=',')
c1s3.30.ft.tp  <-    scan("../sba.git/doc/paper/data/c1s3_30t_opt_tp.csv",sep=',')

c1s3m2a.30.beb.fp <- scan("../sba.git/doc/paper/data/c1s3_30t_m2a_beb_fp.csv",sep=',')
c1s3m2a.30.neb.fp <- scan("../sba.git/doc/paper/data/c1s3_30t_m2a_neb_fp.csv",sep=',')
c1s3m2a.30.sba.fp <- scan("../sba.git/doc/paper/data/c1s3_30t_m2a_sba_fp.csv",sep=',')

c1s3m2a.30.beb.tp <- scan("../sba.git/doc/paper/data/c1s3_30t_m2a_beb_tp.csv",sep=',')
c1s3m2a.30.neb.tp <- scan("../sba.git/doc/paper/data/c1s3_30t_m2a_neb_tp.csv",sep=',')
c1s3m2a.30.sba.tp <- scan("../sba.git/doc/paper/data/c1s3_30t_m2a_sba_tp.csv",sep=',')

thin <- floor(seq(1,length(c1s3.30.ft.fp),length=(length(c1s3.30.ft.fp)-1)/100))

fp <- c(c1s3.30.ft.fp[thin],c1s3m2a.30.neb.fp[thin],c1s3m2a.30.sba.fp[thin],c1s3m2a.30.beb.fp[thin])
tp <- c(c1s3.30.ft.tp[thin],c1s3m2a.30.neb.tp[thin],c1s3m2a.30.sba.tp[thin],c1s3m2a.30.beb.tp[thin])

roc.data <- data.frame(fp,tp,
                       Method=rep(c("OPT","NEB","SBA","BEB"),each=length(thin)))

title <- expression(paste("Study 3 (50% ",omega,"=1.0 50% ",omega,"=15) - 30 taxa - M2a"))

roc.plot <- ggplot(roc.data,aes(fp,tp)) +
    ggtitle(title) +
    geom_line(aes(color=Method,linetype=Method),size=1.2) +
    coord_cartesian(xlim=c(-0.005,.301), ylim=c(-0.005,.301)) +
    labs(x="False Positive Rate",y="True Positive Rate") +
    geom_abline(slope=1,intercept=0,color='gray60') +
    scale_shape_manual(values=c(15,18,17,16)) +
    ##scale_color_manual(values=c("#F8766D","#C77CFF","#00BFC4","#7CAE00")) +
    scale_color_manual(values=c("gray30","grey40","grey50","black")) +
    scale_linetype_manual(values=c("dotted","dotdash","dashed","solid"))
@
\begin{figure}[H]
    \centering
    <<echo=F>>=
    roc.plot +
        theme(plot.title=element_text(size=18),
              panel.background=element_blank(),
              panel.spacing=unit(2,"lines"),
              panel.grid.major=element_line(color="gray95"),
              legend.title=element_blank(),
              legend.position=c(0.88,0.65),
              legend.key=element_rect(fill = "transparent"),
              legend.key.width = unit(4,"line"),
              axis.line=element_line(colour="black"),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
    @
    \caption[ROC 30 taxa]{ROC curves for the detection of sites under positive selection for BEB, NEB, and SBA analyses of data generated under \textit{Correct Model}, study 3 ($50\%$ $\omega=1$, $50\%$ $\omega=1.5$).  The data were simulated using a 30-taxon tree topology. The plot includes a curve for the lower bound (y=x) and an expected upper bound (OPT) when classification is made using the generating model parameters.  The curves for NEB do not always cover the whole range of false positive rates, because NEB sometimes estimates the $\omega$ distribution with all mass on $\omega > 1$.  In these cases, even with a posterior probability cut-off of 1, NEB still incorrectly classifies sites to be under positive selection.}
    \label{fig:roc_30_taxa}
\end{figure}

%The larger gaps in the ROC curves between the two approaches when the parameter estimates are unstable suggests SBA does a better job than BEB at accommodating the parameter instabilities.  \cmt{Under M8...I need to figure out why introducing mild misspecification made the p+ and w+ estimates more stable under M8.  I verified the data is correct.  Maybe we can talk about this tomorrow?}.
In studies 4 and 8 (45\% $\omega=0$, 45\% $\omega=1$, 10\% $\omega=5$), where the parameters of the $\omega$ distribution were well estimated, all approaches (NEB, BEB, and SBA) performed well and the ROC curves were all close to the expected upper limit.  Taken together, the results suggest that SBA balances accuracy and power at least as well as BEB and may be preferable to BEB when parameter estimates are unstable.

%In classification problems, we aim for higher rates of detection along with lower rates of error.  Lowering the threshold for classification will result in a higher detection rate, but more errors will tend be made.  Conversely, when that threshold is increased, we will tend to make fewer errors, but the power of detection will also decrease.  Thus, the error rate and power are a trade off and neither should be interpreted in isolation.  We use the true positive rate, the proportion of sites truly under positive selection correctly identified, as a measure of power.  Receiver Operator Characteristic (ROC) curves are a convenient way to visualize the balance between accuracy and power for classification problem.  fig. X shows ROC curves for each of the simulations that included positive selection in the generating model.  Curves for the theoretical upper limit in performance when the MLEs are fixed to the simulated values are included when possible.  Each point on the plot represents a threshold in the posterior probability that a site is under positive selection.  In all cases SBA performs at lease as close or closer than BEB to this theoretical upper bound.

%% \subsubsection*{Other notes}
%% ...when considering When the equilibrium codon frequencies are heavily misspecified as in the third scenario with the fitted model using equal frequencies, both NEB and BEB do not do as well as SBA to control the false positive rate.  BEB did better with the misspecification in scheme B.  All did fine with scheme C.
%% - Mild model misspecification has little impact on either the false positive rate or the overall performance based on ROC curves.
%% - In almost all cases (except cat1 schemes 4/8 under M8 where BEB does a slightly worse) , regardless of the model misspecification BEB and SBA always do better than NEB in terms of controlling the false positive rate.
%% - Cat3b A, BEB has the biggest problems with misspecification in therms of FPs.  It improves a lot with f61 relative to equal pis.
%% - Cat3b B, BEB does better with equal pis.  NEB is horrible with equal pis, all do well with f61.
%% - Cat3b C, no problems with any model
%% Misspecification of the fitted model relative to the generating model resulted in the performance gap between BEB and SBA to widen in favour of bootstrapping (compare the figure for cat1 scheme 3 and cat2 scheme 7).
%% Comparing simulations schemes 3 and 4 with 7 and 8 allow us to investigate the effect of model misspecification on the false positive rate.  False positive rates were largely unaffected by mild model misspecification (Studies 2 and 6 in table X).  However, this was not the case when the fitted model was heavily misspecified. [Describe results here, and finish this paragraph.]

\subsection{Real Data Analysis}
We began our analysis of the 16 real datasets (described in Methods and summarized in table \ref{tab:realDataResultsOverview}) by using the bootstrap distributions of the MLEs to investigate their properties.  We examined the unsmoothed distributions of the parameters of the $\omega$ distribution.  These distributions indicate that the MLEs for a given model can have very different properties in different real datasets (supplementary figs. S8, S9, S10, S11).  Although the real data represent different degrees of \textit{regular} and \textit{irregular} model properties, we were able to identify groups of genes that represent both extremes.  The \textit{regular} cases had no clear evidence of MLE instabilities and low bootstrap variance (e.g., lysin; fig. \ref{fig:hists_lysin_CDH3_mles}\subref{sfig:lysin_m2a_mles},\subref{sfig:lysin_m8_mles}).  We determined that the $\omega$ distributions had been well estimated for 6 genes (\textit{pol}, \textit{vif}, lysin, \textit{nuoL3}, \textit{RafL}, and \textit{TrbL-VirB6\_3}).  In contrast, we uncovered evidence of MLE instabilities in other genes (e.g., \textit{CDH3}; fig. \ref{fig:hists_lysin_CDH3_mles}\subref{sfig:CDH3_m2a_mles},\subref{sfig:CDH3_m8_mles}).  We determined that the $\omega$ distributions had been poorly estimated for 5 genes (\textit{CDH3}, \textit{mivN}, \textit{pgpA}, \textit{tax}, and \textit{TrbL-VirB6\_2}) under at least one model.  Because no single summary statistic (number of taxa, sequence length, tree length) was generally predictive of \textit{irregular} model properties, we recommend visual inspection of the bootstrap distributions for all real data analyses (supplementary figs. S10, S11).

\begin{sidewaystable}
  \begin{threeparttable}
    \caption{Genes analyzed under models M2a and M8 using NEB, BEB, and SBA approaches for site classification.}
    \centering
    \begin{tabular}[]{*{10}l}
      \toprule
      &       &       & \multicolumn{2}{c}{-lnL}          &                   &           &                \\
      \cmidrule(lr){4-5}
      \multicolumn{1}{c}{Gene} & \multicolumn{1}{c}{$N_t$} & \multicolumn{1}{c}{$N_c$} & \multicolumn{1}{c}{M1a/M2a} & \multicolumn{1}{c}{M7/M8} & \multicolumn{1}{c}{p-value} & \multicolumn{1}{c}{TTL} & \multicolumn{1}{c}{$N_s$} \\
      \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8}
      $\beta$-globin         & 17    & 144   & 3716.14/3712.55 & 3697.22/3686.13 & 0.0275/1.53e-5    & 8.40/8.57 & 0(0)/3(4) \\
      \textit{ccmF}          & 5     & 635   & 6121.78/6113.57 & 6127.62/6116.48 & 2.72e-4/1.46e-5   & 5.60/3.03 & 3(2)/3(5) \\
      \textit{CDH3}          & 11    & 176   & 5629.97/5623.37 & 5630.66/5623.88 & 1.35e-3 /1.14e-3  & 0.56/0.56 & 1(1)/1(1) \\
      \textit{ENAM}          & 11    & 1142  & 7514.30/7509.28 & 7609.16/7605.74 & 6.61e-3/0.0327    & 0.46/0.56 & 1(1)/2(1) \\
      \textit{env}           & 13    & 91    & 1114.64/1106.45 & 1115.40/1106.39 & 2.76e-4/1.23e-4   & 2.04/2.04 & 2(2)/2(4) \\
      lysin                  & 25    & 134   & 4472.65/4410.28 & 4472.16/4410.57 & 2.86e-14/0.00     & 8.81/8.82 & 22(22)/23(23) \\
      \textit{mivN}          & 5     & 504   & 3383.45/3832.93 & 3834.69/3831.44 & 0.595/0.0388      & 1.62/1.60 & 0(0)/1(1) \\
      \textit{nuoL3}         & 5     & 499   & 5006.16/4978.97 & 5011.37/4977.19 & 1.56e-12/1.44e-15 & 4.58/4.49 & 9(8)/10(10) \\
      \textit{perM}          & 5     & 351   & 2619.88/2619.43 & 2621.64/2617.94 & 0.638/0.0247      & 1.78/1.80 & 0(0)/2(0) \\
      \textit{pgpA}          & 5     & 198   & 1541.27/1539.29 & 1542.65/1538.91 & 0.138/0.0238      & 2.93/2.23 & 1(0)/1(1) \\
      \textit{pol}           & 23    & 947   & 9394.05/9363.96 & 9405.74/9365.88 & 8.52e-14/0.00     & 1.31/1.30 & 6(6)/10(13) \\
      \textit{RfaL}          & 5     & 403   & 3964.89/3955.34 & 3970.38/3955.44 & 7.16e-05/3.23e-7  & 3.46/3.46 & 2(1)/4(3) \\
      \textit{tax}           & 20    & 181   & 895.50/892.02   & 895.50/892.02   & 0.0309/0.0309     & 0.13/0.13 & 181(0)/181(21) \\
      \textit{TrbL-VirB6\_2} & 5     & 657   & 5492.55/5492.52 & 5301.23/5286.43 & 0.976/3.74e-7     & 2.12/2.10 & 0(0)/1(0) \\
      \textit{TrbL-VirB6\_3} & 5     & 938   & 8305.65/8288.36 & 8307.06/8269.09 & 3.09e-8/0.00      & 3.06/3.02 & 3(2)/18(11) \\
      \textit{vif}           & 29    & 192   & 3393.83/3367.86 & 3400.45/3370.66 & 2.29e-06/1.16e-13 & 2.90/2.91 & 10(8)/10(10) \\
      \bottomrule
    \end{tabular}
    \label{tab:realDataResultsOverview2}
    \begin{tablenotes}
      \small
    \item $N_t$: number of taxa, $N_c$: sequence length in number of codons, -lnL: -log likelihood for each nested model pair, p-value of the likelihood ratio test for the presence of positive selection, TTL: total tree length estimated under M2a/M8, $N_s$: number of sites classified to under positive selection using a posterior probability threshold of 0.95 under M2a/M8 for NEB(BEB).
    \end{tablenotes}
  \end{threeparttable}
\end{sidewaystable}

<<lysin,echo=F>>=
lysin.m2a.params <- read.csv("../sba.git/doc/real_data_results/mles/lysin_m2a_mles.csv",header=F)
lysin.m2a.omegas <- c(as.matrix(lysin.m2a.params[,5:7]))
lysin.m2a.omegas.order <- order(lysin.m2a.omegas)
lysin.m2a.omegas <- lysin.m2a.omegas[lysin.m2a.omegas.order]
lysin.m2a.ps.cumsum <- cumsum(c(as.matrix(lysin.m2a.params[,2:4]))[lysin.m2a.omegas.order])

lysin.m8.params <- read.csv("../sba.git/doc/real_data_results/mles/lysin_m8_mles.csv",header=F)
@

<<CDH3,echo=F>>=
CDH3.m2a.params <- read.csv("../sba.git/doc/real_data_results/mles/CDH3_m2a_mles.csv",header=F)
CDH3.m2a.omegas <- c(as.matrix(CDH3.m2a.params[,5:7]))
CDH3.m2a.omegas.order <- order(CDH3.m2a.omegas)
CDH3.m2a.omegas <- CDH3.m2a.omegas[CDH3.m2a.omegas.order]
CDH3.m2a.ps.cumsum <- cumsum(c(as.matrix(CDH3.m2a.params[,2:4]))[CDH3.m2a.omegas.order])

CDH3.m8.params <- read.csv("../sba.git/doc/real_data_results/mles/CDH3_m8_mles.csv",header=F)
@

\clearpage

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(lysin.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(lysin.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M2a}
    \label{sfig:lysin_m2a_mles}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(lysin.m8.params[,12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    axis(1,cex.axis=3,padj=1)
    hist(lysin.m8.params[,23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:lysin_m8_mles}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(CDH3.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(CDH3.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M2a}
    \label{sfig:CDH3_m2a_mles}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,4))
    hist(CDH3.m8.params[,12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(CDH3.m8.params[,23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:CDH3_m8_mles}
  \end{subfigure}
  \caption{MLE distributions over bootstrap datasets for the lysin and \textit{CDH3} genes.  The distributions of the $p_{\omega>1}$ and $\omega_{>1}$ parameters associated with positive selection were estimated under models M2a and M8 for each of 100 bootstrap datasets.}
  \label{fig:hists_lysin_CDH3_mles}
\end{figure}

Next we investigated the degree to which the real data results obtained under BEB, NEB, and SBA were consistent with each other.  This is challenging, because the posterior probability thresholds for site classification are not calibrated to give comparable false positive rates.  Our solution was to measure the rank correlations of the site-specific posterior probability scores for positive selection between methods (BEB, NEB, and SBA).  As there are a large number of pairwise comparisons, we took the mean relationship between methods for both the genes representing \textit{regular} and \textit{irregular} model estimation (table \ref{tab:method_cors}).
<<sp_cors,echo=F>>=
## order for good genes: HIVpol, HIVvif, Lysin, nuoL3, RafL, TrbL-VirB3
sp.cor.good.m2a.neb.beb <- c(0.91,1.00,1.00,0.98,1.00,1.00)
sp.cor.good.m2a.neb.sba <- c(0.76,0.97,0.99,0.98,0.96,0.97)
sp.cor.good.m2a.beb.sba <- c(0.85,0.98,0.99,0.99,0.97,0.97)

sp.cor.good.m8.neb.beb <- c(0.97,0.99,1.00,1.00,0.99,1.00)
sp.cor.good.m8.neb.sba <- c(0.88,0.97,0.99,0.98,0.98,0.96)
sp.cor.good.m8.beb.sba <- c(0.95,0.99,1.00,0.99,0.99,0.94)

## order for bad genes: CDH3, MivN, pgpA, tax, TrbL-VirB2
## can't add tax, because all Prs are 1 for NEB
sp.cor.bad.m2a.neb.beb <- c(0.4,0.77,0.72,0.72)
sp.cor.bad.m2a.neb.sba <- c(0.4,0.78,0.72,0.72)
sp.cor.bad.m2a.beb.sba <- c(1.0,0.96,0.98,0.98)

sp.cor.bad.m8.neb.beb <- c(0.4,0.97,1.00,1.00)
sp.cor.bad.m8.neb.sba <- c(0.4,0.91,0.95,0.98)
sp.cor.bad.m8.beb.sba <- c(1.0,0.97,0.95,0.98)

## mean(sp.cor.good.m2a.neb.beb)
## sd(sp.cor.good.m2a.neb.beb)
## mean(sp.cor.good.m2a.neb.sba)
## sd(sp.cor.good.m2a.neb.sba)
## mean(sp.cor.good.m2a.beb.sba)
## sd(sp.cor.good.m2a.beb.sba)
## mean(sp.cor.good.m8.neb.beb)
## sd(sp.cor.good.m8.neb.beb)
## mean(sp.cor.good.m8.neb.sba)
## sd(sp.cor.good.m8.neb.sba)
## mean(sp.cor.good.m8.beb.sba)
## sd(sp.cor.good.m8.beb.sba)

## mean(sp.cor.bad.m2a.neb.beb)
## sd(sp.cor.bad.m2a.neb.beb)
## mean(sp.cor.bad.m2a.neb.sba)
## sd(sp.cor.bad.m2a.neb.sba)
## mean(sp.cor.bad.m2a.beb.sba)
## sd(sp.cor.bad.m2a.beb.sba)
## mean(sp.cor.bad.m8.neb.beb)
## sd(sp.cor.bad.m8.neb.beb)
## mean(sp.cor.bad.m8.neb.sba)
## sd(sp.cor.bad.m8.neb.sba)
## mean(sp.cor.bad.m8.beb.sba)
## sd(sp.cor.bad.m8.beb.sba)

## order for good genes: HIVpol, HIVvif, Lysin, nuoL3, RafL, TrbL-VirB3
sp.cor.good.neb <- c(0.91,1.00,1.00,1.00,0.99,0.97)
sp.cor.good.beb <- c(0.98,1.00,0.99,0.99,0.99,0.97)
sp.cor.good.sba <- c(1.00,1.00,1.00,1.00,1.00,1.00)

## order for bad genes: CDH3, MivN, pgpA, tax, TrbL-VirB2
sp.cor.bad.neb <- c(1.0,0.78,0.72,0.73)
sp.cor.bad.beb <- c(1.0,1.00,0.98,1.00)
sp.cor.bad.sba <- c(1.0,0.98,1.00,0.99)

## mean(sp.cor.good.neb)
## sd(sp.cor.good.neb)
## mean(sp.cor.good.beb)
## sd(sp.cor.good.beb)
## mean(sp.cor.good.sba)
## sd(sp.cor.good.sba)

## mean(sp.cor.bad.neb)
## sd(sp.cor.bad.neb)
## mean(sp.cor.bad.beb)
## sd(sp.cor.bad.beb)
## mean(sp.cor.bad.sba)
## sd(sp.cor.bad.sba)
@
\begin{table}[H]
  \begin{threeparttable}
    \caption{Spearman rank correlations between site posterior probabilities for different forms of classification.}
    \centering
    \begin{tabular}[h!]{l*{4}c}
      \toprule
      & \multicolumn{2}{c}{{\bf \textit{Regular}}} & \multicolumn{2}{c}{{\bf \textit{Irregular}}} \\
      \cmidrule(lr){2-3} \cmidrule(lr){4-5}
      & mean      & SD        & mean      & SD \\
      \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
      M2a                    &           &           &           &        \\
      \hspace{1em} NEB/BEB   & 0.98      & 0.04      & 0.65      & 0.17   \\
      \hspace{1em} NEB/SBA   & 0.94      & 0.09      & 0.66      & 0.17   \\
      \hspace{1em} BEB/SBA   & 0.96      & 0.05      & 0.98      & 0.02   \\
      M8                     &           &           &           &        \\
      \hspace{1em} NEB/BEB   & 0.99      & 0.01      & 0.84      & 0.30   \\
      \hspace{1em} NEB/SBA   & 0.96      & 0.04      & 0.81      & 0.27   \\
      \hspace{1em} BEB/SBA   & 0.98      & 0.03      & 0.98      & 0.02   \\
      \bottomrule
    \end{tabular}
    \label{tab:method_cors}
    \begin{tablenotes}
      \small
    \item The mean and standard deviation (SD) of the correlations are for real genes displaying \textit{regular} and \textit{irregular} estimation properties.
    \end{tablenotes}
  \end{threeparttable}
\end{table}
We found that when MLEs are well estimated (\textit{regular} genes), there is stronger agreement among all three methods in the ranking of sites according to the signal for positive selection.  In contrast, when the $\omega$ distributions are poorly estimated (genes representing \textit{irregular} estimation), BEB and SBA are generally consistent in their rankings, but differ from NEB.  These results suggest that NEB's inability to accommodate MLE uncertainty in such datasets has the largest effect on the posteriors.  However, the problem of calibration remains.  Our simulation studies revealed that using a common posterior probability threshold for classification does not guarantee a similar trade-off between accuracy and power for different methods.  Indeed, we see evidence of this in the real data.  Comparing the counts of positively selected sites identified in the genes using thresholds of 0.50 and 0.95 reveals differences between BEB and SBA (table \ref{tab:num_sites_good_bad_genes}), despite large rank correlations.
\begin{table}[H]
  \begin{threeparttable}
    \caption{Number of sites identified to be under positive selection for the real data.}
    \centering
    \begin{tabular}[!ht]{*{7}l}
      \toprule
      & \multicolumn{3}{c}{M2a}       & \multicolumn{3}{c}{M8}     \\
      \cmidrule(lr){2-4}              \cmidrule(lr){5-7}           \\
      Gene                   & NEB      & BEB   & SBA        & NEB     & BEB    & SBA     \\
      \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \\
      \textit{CDH3}          & 1/1      & 12/1  & 46/0       & 1/1     & 22/1   & 117/5   \\
      \textit{mivN}          & 1/0      & 7/0   & 1/0        & 4/1     & 12/1   & 28/0    \\
      \textit{pgpA}          & 1/0      & 4/0   & 4/0        & 5/1     & 5/1    & 17/0    \\
      \textit{tax}           & 181/181  & 181/0 & 181/0      & 181/181 & 181/21 & 181/21  \\
      \textit{TrbL-VirB6\_2} & 0/0      & 16/0  & 0/0        & 11/1    & 18/0   & 59/0    \\
      \hline
      \rule{0pt}{3ex}\textit{pol}& 12/6     & 19/6  & 94/4       & 22/10   & 33/13  & 83/16   \\
      lysin                  & 33/22    & 32/22 & 42/5       & 37/23   & 37/23  & 41/11   \\
      \textit{nuoL3}         & 18/9     & 18/8  & 85/18      & 19/10   & 20/10  & 83/20   \\
      \textit{RfaL}          & 20/2     & 20/1  & 70/1       & 33/4    & 41/3   & 74/3    \\
      \textit{TrbL-VirB6\_3} & 28/3     & 27/2  & 73/9       & 45/18   & 44/11  & 134/48  \\
      \textit{vif}           & 13/10    & 13/8  & 31/6       & 15/10   & 19/10  & 37/10   \\
      \hline
      \rule{0pt}{3ex}$\beta$-globin & 4/0& 5/0   & 11/0       & 8/4     & 8/4    & 17/4    \\
      \textit{ccmF}          & 7/1      & 11/1  & 112/0      & 15/3    & 79/5   & 114/5   \\
      \textit{ENAM}          & 9/1      & 21/1  & 184/0      & 44/2    & 31/1   & 78/1    \\
      \textit{env}           & 14/3     & 16/3  & 21/3       & 16/3    & 22/5   & 24/3    \\
      \textit{perM}          & 4/0      & 6/0   & 0/0        & 6/2     & 6/0    & 36/3    \\
      \bottomrule
    \end{tabular}
    \label{tab:num_sites_good_bad_genes}
    \begin{tablenotes}
      \small
    \item  The posterior probability thresholds are 0.5/0.95.  The top genes represent \textit{irregular} estimation, the middle \textit{regular}, and the bottom genes are not categorized.
    \end{tablenotes}
  \end{threeparttable}
\end{table}
Under M2a, there was a stark difference between the \textit{irregular} genes and all other genes.  ROC curves for simulations studies are better suited for comparing methods, because they give direct comparisons of power at the same false positive rate.

%However, the problem of calibration remains.  Our simulation studies revealed that using a common posterior probability threshold for classification does not guarantee a similar trade-off between accuracy and power for different methods.  Indeed, we see evidence of this in the real data.  Comparing the counts of positively selected sites identified in the genes using thresholds of 0.50 and 0.95 reveals differences between BEB and SBA (table \ref{tab:num_sites_good_bad_genes}), despite large rank correlations.  Under M2a, there was a stark difference between the \textit{irregular} genes and all other genes.  For the \textit{irregular} genes, using a threshold of 0.5, there was only one case where SBA identified more sites than BEB (\emph{CDH3}).  On the other hand, for all the other genes (\textit{irregular} and unclassified), there was only one case (perM) when SBA did not identify more sites than BEB.  When the same threshold was applied to the results obtained under M8, SBA identified more sites than BEB in all genes except for the \emph{tax}. When the threshold was raised to 0.95, BEB identified at least as many sites as SBA in 10 genes under M8.  The challenge of calibration still exists for real data.  For methods comparisons, such challenges can be avoided by using ROC curves in simulations which give direct comparisons of power at the same false positive rate.

% These results highlight the challenge of calibration in real data and thus the decision about which results to prefer will have to be based on the generalized measures of performance obtained via simulation.
%Both methods were conservative under M2a using a cut off of 0.95 with only one site flagged for \emph{CDH3} under BEB.  Similar differences in the number of sites flagged for positive selection were observed for the ``good'' genes.  Under both models, using a cutoff of 0.5, SBA flagged more sites than BEB at all 12 opportunities, whereas with a cutoff of 0.95, SBA flagged more sites in only 5 of the 12 opportunities.  These results highlight the challenge of calibration and thus, the decision about which results to prefer will have to be based on the generalized measures of performance obtained via simulation.
\label{subsec:roc_opt}
We also used rank correlation to investigate the robustness of the methods (BEB, NEB, and SBA) to the chosen model (M2a versus M8).  We did this by computing the rank correlation, between models, of the site posterior probabilities obtained by the same method (table \ref{tab:model_cors}).
\begin{table}[H]
  \begin{threeparttable}
    \caption{Spearman rank correlations between site posterior probabilities for models M2a and M8.}
    \centering
    \begin{tabular}[h!]{l*{4}c}
      \toprule
      & \multicolumn{2}{c}{{\bf \textit{Regular}}} & \multicolumn{2}{c}{{\bf \textit{Irregular}}} \\
      \cmidrule(lr){2-3} \cmidrule(lr){4-5}
      & mean      & SD        & mean      & SD \\
      \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
      \hspace{1em} NEB       & 0.98      & 0.04      & 0.81      & 0.13   \\
      \hspace{1em} BEB       & 0.99      & 0.01      & 1.00      & 0.01   \\
      \hspace{1em} SBA       & 1.00      & 0.00      & 0.99      & 0.00   \\
      \bottomrule
    \end{tabular}
    \label{tab:model_cors}
    \begin{tablenotes}
      \small
    \item The mean and standard deviation (SD) of the correlations are for real genes displaying \textit{regular} and \textit{irregular} estimation properties.
    \end{tablenotes}
    \end{threeparttable}
\end{table}
For the \textit{regular} genes, all three methods had high correlations with low variably.  For the genes representing \textit{irregular} estimation, the correlation was lower and the variability larger for NEB as compared to BEB and SBA.  The similarity across models that we observed for SBA may be a consequence of using nonparametric bootstrapping, which should show robustness to model misspecification.  It seems that BEB's application of uniform priors to the $\omega$ distribution achieved a similar effect.

Up to this point, bootstrapping has been used to obtain surrogates for posteriors. An alternative use of bootstrapping is to construct confidence intervals for posteriors to quantify the uncertainty at any given site about what the true posterior of positive selection is. For the real data, these confidence intervals differed substantially between M2a and M8, highlighting differences between the two modelling frameworks. For sites having a posterior of at least 0.9 under one or more methods, the M8 confidence intervals for those sites were never wider than the corresponding M2a intervals (table \ref{tab:interval_widths_ps_sites}).
\begin{table}[H]
  \begin{threeparttable}
    \caption{Average SBA posterior probability interval widths for sites with at least one method having a posterior probability over 0.9.}
    \centering
    \begin{tabular}[!ht]{*{4}l}
      \toprule
      Gene                       & \multicolumn{1}{c}{M2a}       & \multicolumn{1}{c}{M8} & \multicolumn{1}{c}{Difference} \\
      \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4}
      \textit{CDH3}              & 0.95     & 0.46  & 0.49    \\
      \textit{mivN}              & 1.00     & 1.00  & 0.00    \\
      \textit{pgpA}              & 1.00     & 1.00  & 0.00    \\
      \textit{tax}               & 0.87     & 0.31  & 0.56    \\
      \textit{TrbL-VirB6\_2}     & 1.00     & 1.00  & 0.00    \\
      \hline
      \rule{0pt}{3ex}\textit{pol}    & 0.78     & 0.78  & 0.00    \\
      lysin                      & 0.70     & 0.49  & 0.20    \\
      \textit{nuoL3}             & 0.26     & 0.21  & 0.05    \\
      \textit{RfaL}              & 0.68     & 0.48  & 0.19    \\
      \textit{TrbL-VirB6\_3}     & 0.66     & 0.10  & 0.57    \\
      \textit{vif}               & 0.36     & 0.14  & 0.21    \\
      \hline
      \rule{0pt}{3ex}$\beta$-globin  & 1.00     & 0.00  & 1.00    \\
      \textit{ccmF}              & 1.00     & 0.49  & 0.51    \\
      \textit{ENAM}              & 0.53     & 0.43  & 0.10    \\
      \textit{env}               & 0.51     & 0.27  & 0.24    \\
      \textit{perM}              & 0.91     & 0.14  & 0.77    \\
      \bottomrule
    \end{tabular}
    \label{tab:interval_widths_ps_sites}
    \begin{tablenotes}
      \small
    \item The top genes represent \textit{irregular} estimation properties, the middle \textit{regular}, and the bottom genes are not categorized.
    \end{tablenotes}
  \end{threeparttable}
\end{table}
This result reflects broad differences between the MLE distributions obtained under these two models; MLE distributions under M8 tend to be tighter, and more likely located away from a boundary (supplementary figs. S10, S11).  We believe this represents empirical support for the commonly held notion that M8 is more powerful than M2a \citep{wong2004accuracy}.  However, this relationship should not be assumed to hold when the MLEs are poorly estimated.  Confidence interval widths were at the maximum (1.0) for both M8 and M2a in three of the five genes representing \textit{irregular} estimation.  These findings highlight the importance of (1) inspecting bootstrap distributions to gain insights into the challenges posed by the data in hand, and (2) using SBA to accommodate MLE uncertainties (especially when they are poorly estimated).

% Lastly, we reviewed the ideas discussed for the simulated and real data results in the context of a special case: the \emph{tax} gene of human T-cell lymphotropic virus \citet{suzuki2004false,yang2005bayes}.  The dataset has 20 taxa and 181 sites with all but 23 of the sites constant.  Of these 23 sites, each has just one codon different from the others, the majority of which (21) correspond to a different amino acid.  Partly because of the extreme conservation of codons, the estimated $\omega$-distribution places all of its weight on the positive selection category and the null hypothesis of no positive selection is rejected using likelihood ratio tests.  Without accounting for errors in the estimates of the $\omega$ distribution, NEB must then infer all sites to be under positive selection with a posterior probability of 1.  Under BEB, the 21 sites that had a single non-synonymous substitution had posterior probabilities between 0.91 and 0.93 (M2a) and 0.96 and 0.97 (M8).  The two sites with a synonymous transition and the remaining conserved sites all had posterior probabilities between 0.55 and 0.73 for both models.  Similar results were observed for SBA with higher average posteriors (0.86 - 0.88, M2a; 0.99, M8) for the sites with a nonsynonymous substitution and lower average posteriors (M2a: 0.55 - 0.60, M8: 0.85 - 0.87) for the other sites.  The 0.95 interval widths for the site posteriors were larger under M2a (0.87) than M8 (0.31).  Considering only the sites with a nonsynonymous substitution, the intervals were much wider for M2a (1.00) than M8 (0.03).  Using larger bandwidth parameters to smooth the MLEs resulted in smaller posterior probabilities under SBA.

Lastly, we interpret our results for the \emph{tax} gene of the human T-cell lymphotropic virus. This gene warrants special attention because it has a highly unusual site-pattern distribution, extreme MLEs, and has been employed as a boundary case in several studies of the NEB and BEB classifiers \citep{suzuki2004false,yang2005bayes}. The dataset has 20 taxa and 181 sites, 158 (87\%) of which are invariant across all 20 lineages.  At each of the 23 variable sites, there is just one codon that differs from all the others with 21 of the 23 codon changes coding for a different amino acid.  This atypical site-pattern distribution corresponds to a relatively large number of nonsysnonymous substitutions over very short branch lengths (mean branch length: $0.0064$ under both M2a and M8). A very high probability of positive selection (i.e., large values for both the $p_{\omega>1}$ and $\omega_{>1}$ parameters) is required to account for the nonsynonymous substitutions when the branch lengths are so short. In fact, both models M2a and M8 estimate 100\% of the sites to be in the $\omega>1$ class. This result belies that fact that considerable instability is associated with those parameter estimates, as revealed by bootstrapping (supplementary figs. S10, S11). Since NEB ignores parameter value uncertainty, it must assign a conditional posterior probability of $\omega>1$ ($Pr=1.0$) for all sites, including those that are invariant. In contrast, the site posteriors for BEB and SBA were similar and depended on the site patterns (supplementary table S2).  As expected, the SBA signal for positive selection was strongest at the 21 sites with nonsynonymous changes (M2a: $0.87 < Pr < 0.89$; M8: $0.99 < Pr < 0.99$), as compared to all other sites (M2a: $0.55 < Pr < 0.60$; M8: $0.76 < Pr < 0.80$).  The SBA confidence intervals under M8 revealed that the estimates of $Pr$ for the 21 sites with a nonsynonymous change were more reliable (average width: 0.028) than for the invariant sites (average width: 0.418).  We suggest this result is appropriate for these data. Almost all the signal in this dataset is contained in those 21 sites, and it is difficult to reconcile this amount of nonsynonymous change over such short branches without strong positive selection.  Moreover, when branch lengths are very short, an invariant site can only be viewed as carrying no signal about whether the $\omega$ value would be small or large over longer evolutionary periods. This leads to very wide 95\% SBA $Pr$ confidence intervals for these sites.

\section{Discussion}
We have presented an approach, based on an unconventional use of the nonparametric bootstrap, for evaluating MLE instabilities and improving site-specific inference of positive selection.  For any given site in an alignment, conclusions about positive selection are based on the aggregation and distributions of many estimates of $\omega$ and many posterior probabilities.  An important step in our approach involves smoothing the bootstrap distributions of the parameter estimates using techniques borrowed from kernel density estimation.  This step is critical for overcoming instabilities in parameter estimation.  Kernel smoothing also has the benefit of reducing computational costs relative to procedures that use full bootstrap sampling to obtain comparable numbers of MLEs.

Application of BEB, NEB, and SBA using models M2a and M8 to 100 simulated datasets in each of 10 different simulation scenarios showed that, under difficult simulation conditions when regularity conditions have not been met, NEB often poorly controls false positive classification of sites, even when the number of taxa is large.  This is in contrast to past recommendations, which suggested NEB does well at controlling false positive rates when analyzing large datasets (many taxa and long sequences) \citep[e.g.,][]{yang2005bayes}.  By accounting for variability of estimation, both BEB and SBA achieve better control of the false positive rates.  However, SBA provided consistently better control under M8 when there was mild model misspecification (studies 5-8 under in table 1), and this was unaffected by pre-screening via the LR test.  We note that all real data are expected to be affected, to some degree, by model misspecification.

By accounting for variability of estimation, both BEB and SBA achieve good power relative to NEB as is evidenced by their tending to be closer to the expected upper limit of performance in the ROC curves.  Some of the simulation results suggest that M2a is a better-performing model. For instance, M2a gave 1) ROC curves closer to the expected upper bound in some cases (fig. \ref{fig:roc}) and 2) lower false positive rates (table \ref{tab:sim}). This may, however, be a consequence of the simulations conditions being more suitable for M2a than M8.  For example, in studies 3 and 7, half the sites were simulated with $\omega=1$, and M2a has a site class with $\omega=1$ fixed.  On the other hand, considering sites with larger posteriors in the real data analysis, the 95\% posterior confidence intervals were usually narrower (and never wider) for M8 than M2a.  This supports previous results that suggest M8 has more power to detect sites under positive selection \citep{wong2004accuracy}.  The $\beta$-globin gene serves as a good example.  Of the five sites in this gene where either NEB or BEB gave a posterior of at least 0.9, the SBA confidence interval widths were all 1 for M2a, but averaged 0.129 for M8.  Moreover, the $\omega_{>1}$ parameter distributions tended to be wider for M2a than M8, particularly for the genes that displayed properties suggesting regularity conditions were met.  This is probably because the beta distribution used by M8 to model $\omega<1$ has more flexibility in real data conditions compared to an M2a model with the same number of parameters.

An appealing attribute of BEB, relative to SBA, is its limited use of computational resources.  Each SBA bootstrap analysis may use similar computational resources as BEB does for the one original dataset.  However, SBA's greater computational requirements is a trade-off for a more rigorous assessment of the parameter estimation.  For example, SBA adjusts for the uncertainty in all model parameters, including branch lengths, while BEB does not.  A new BEB implementation that integrated over branch lengths would require costlier techniques because numerical integration does not scale well with higher dimension.  Moreover, because SBA estimates each set of bootstrap parameters independently, they can be estimated in parallel.  On a computing cluster with as many cores as bootstrap samples generated, the wall-clock times for BEB and SBA are comparable.

There are a limited number of BEB implementations for different models.  By contrast it is comparatively trivial to apply SBA to new models once the basic capacity for bootstrapping and parameter smoothing are in place.  This could facilitate the application of SBA to a wider variety of inference problems in molecular evolution than has occurred with BEB.  SBA for the popular branch-site codon model A \citep{yang2002bcodon,zhang2005evaluation} was implemented as a demonstration of the feasibility of SBA implementations for new models.  A new, preliminary implementation, which was completed within a few hours, can be found at https://github.com/Jehops/codeml\_sba.  An overview of the analysis of the \textit{NR1D1} gene \citep{baker2016functional} under SBA can be found in the supplementary file S2.

There are useful by-products of the SBA approach for classifying sites. The histograms of the distributions of the MLEs over bootstrap samples provide insight into the degree of irregularity of the estimation.  For several of the datasets, most notably the \emph{tax} gene dataset, these histograms provided a clear indication that the MLEs were unstable.  In such cases, site classifications should be accepted with caution.  Even when regularity conditions have been met, the confidence intervals of the posteriors provide an additional tool for assessing the certainty about the strength of the signal for positive selection at an individual site.  We suggest that future analyses of real data should include both visual inspection of bootstrap distributions and reporting of SBA-derived confidence intervals of the posterior probabilities associated with positive selection.

Bootstrapping has been shown to provide effective adjustments to EB methods in other settings.  For example, \citet{laird1987empirical} studied the application of bootstrapping with EB methods for random effects models where both the observations and random effects distributions were Gaussian.  They argued that confidence intervals produced from bootstrap posteriors were frequently narrower than they should be and that bootstrap averaging helped to ameliorate problems.  They speculated that bootstrapping would produce good EB inferences for a broad class of EB problems.  In a prediction setting, a procedure that aggregates predictors generated from bootstrap replicates was proposed by \citet{breiman1996bagging}, which was shown to move some unstable predictors closer to optimality.  The bagging procedure used in that paper is equivalent to using the median posterior to classify sites under SBA.  Our experiments (data not shown) indicated that the average is a better measure of the middle of the distribution of site posterior probabilities.

%Here, we propose an alternative to BEB that uses bootstrapping \citep{efron1979bootstrap} and avoids the need for priors while accommodating uncertainty in all the model parameters.  For inferring positive selection at sites, we estimate parameters for each of a number of bootstrap sequence alignments and an unmodified phylogenetic tree.  Each of these sets of parameters estimates can then be used to calculate posterior probabilities of positive selection for a given site pattern of interest.  By carrying these steps out for each bootstrap alignment, distributions of both the parameter estimates and the posterior probabilities of $\omega>1|x_h$ are obtained.  In principle, methods using bootstrapping should use as many bootstrap alignments as possible but in practice sampling is necessarily finite.  To better approximate the infinite-sample bootstrap distributions, we use smoothing methods borrowed from kernel density estimation.  We test our approach via simulation study and on three datasets previously analyzed: (about the data).

While using the data in hand to account for errors in MLE estimation is helpful for detecting sites under positive selection, refinements of the SBA approach are warranted.  Like other approaches, we have avoided the difficult process of calibrating for type I errors in real data.
%For comparing methods, the ROC curves adjust for this difficulty by providing comparisons of true positive rates with thresholds calibrated so that each method has the same FP rate.  The problem of calibration remains a challenge for real data.
Choosing an optimal bandwidth parameter for smoothing a distribution is also a difficult process.  Under-smoothing will leave spurious bumps and irregularities in the distribution and over smoothing will remove useful information and increase bias.  There are different theoretical suggestions for the size of the bandwidth parameter, but these can be challenging to apply as they may depend on the unknown density \citep[p. 176]{venables2013modern}.
%Repeated parameter estimation under SBA highlights problems when MLEs fall on or close to their boundaries, problems that are exacerbated in codon models that discretize the $\omega$ distribution.  We are hopeful that a penalized likelihood approach, which can push such estimates to the interior of the parameter space, will be helpful.
SBA uses bootstrap distributions to highlight problems when MLEs fall on or close to their boundaries.  We are hopeful that a penalized likelihood approach, which can push such estimates to the interior of the parameter space, will be helpful.  Bootstrapping does well to accommodate the variance in a parameter estimate, however, when estimates are very small, the variance, even under bootstrapping, may be underestimated.  This may be a problem we encountered with the branch lengths of the \emph{tax} gene.  Some preliminary experiments show that perturbing the very small branch length estimates of the \emph{tax} gene can cause large differences in the MLEs of the parameters of the $\omega$ distribution.  This suggests that applying kernel smoothing to parameters other than those defining the $\omega$ distribution may be helpful.

SBA can be applied to a wide variety of problems in molecular evolution where uncertainties or instabilities in MLEs impact inference based on empirical Bayes.  Examples where the method can be directly applied, with little or no modification, include: classification of sites into general rate categories \citep[e.g.,][]{mayrose2004comparison}, identification of positively selected sites in non-coding DNA \citep[e.g.,][]{haygood2007promoter}, identification codon sites subject to episodic change in selection pressure \citep[e.g.,][]{yang2002bcodon}, detection of Type-I functional divergence in protein sequences \citep[e.g.,][]{gaston2011phylogenetic}, detection of amino acid sites having shifts in the pattern of exchangeabilities \citep[e.g.,][]{le2012modeling}, and detection of amino acid sites evolving under a covarion-like evolutionary process \citep[e.g.,][]{penn2008evolutionary}.  With some modification, SBA could be applied to the task of ancestral state reconstruction.  As the field moves towards increasingly more complex models, there will be increasing demand for methods such as SBA that can account for parameter-estimate uncertainties.

\section{Theory and Methods}
%\subsection*{Likelihood of Site Data under Markov Models of Codon Evolution}
\subsection{Markov Models of Codon Evolution}
Consider an alignment of DNA sequences with $n$ codon sites and denote the codons in the sequences at site $h$ $(h=1, \dots, n)$ as $x_h$, the site pattern at site $h$.  The models considered here are described in \citep{nielsen1998likelihood} and define the relative, instantaneous substitution rate between codon $i$ and $j$ $(i \neq j)$ at site $h$ as
\begin{equation}
  Q_{ij} \propto \left\{
    \begin{array}{l l}
      0, &  \mbox{if \emph{i} and \emph{j} differ at two or three codon positions,}\\
      \pi_j, &  \mbox{if \emph{i} and \emph{j} differ by a synonymous transversion,}\\
      \kappa\pi_j, &  \mbox{if \emph{i} and \emph{j} differ by a synonymous transition,}\\
      \omega\pi_j, &  \mbox{if \emph{i} and \emph{j} differ by a nonsynonymous transversion,}\\
      \omega\kappa\pi_j, &  \mbox{if \emph{i} and \emph{j} differ by a nonsynonymous transition}
    \end{array} \right.
  \label{eq:model}
\end{equation}
where $Q$ is the rate matrix of a continuous-time, stationary, time-reversible Markov process.  The $\pi_j$ parameters above are the stationary frequencies of codon $j$ and the $\kappa$ parameter is the transition to transversion rate ratio.  The $\omega$ parameter, which has an interpretation as the non-synonymous to synonymous rate ratio, is the key parameter for the inference of positively selected sites \citep[pp. 48-68]{Yang2006Computational}.  The transition-probability matrix, $P(t)$, which gives the probabilities of state changes over time, $t$, relates to the rate matrix, $Q$, by $P(t) = e^{Qt}$.  For a phylogenetic tree with branch lengths, the likelihood of the data at codon site $h$ given the parameters $\theta$, $f(x_h|\theta)$, can be calculated using (\ref{eq:model}) and Felsenstein's pruning algorithm \citep{felsenstein1973maximum}.

Because sites are assumed to evolve independently, the log likelihood for a sequence alignment with more than one site pattern $(n>1)$ is the sum of the site log likelihoods, $\ell = \log(L) = \sum_{h=1}^{n}\log\{f(x_h|\theta)\}$.

To account for variability in selection pressure across sites, $\omega$ is usually allowed to vary.  The models we consider are a subset of the models described in Yang et al. (2000), which assume the value of $\omega$ at site $h$ comes from some distribution.  To avoid difficulties applying the pruning algorithm, this distribution is always discrete with weights $p_1,\dots,p_{k}$ on $\omega_1\dots,\omega_{k}$ values.  With $k$ classes, each with an estimate of the $\omega$ ratio and corresponding weight, the likelihood the data at site $h$ then becomes $f(x_h|\theta) = \sum_{i=1}^{k}p_if(x_h|\omega_i,\psi)$, where $\psi$ denotes the model parameters other than those describing the $\omega$ distribution.

Bayes formula is used to calculate a posterior probability that a given site evolved under site class $i$ with $Pr(\omega^{(h)} = \omega_i|x_h,\psi) = p_if(x_h|\omega_i,\psi) / \sum_{j=1}^{k}p_jf(x_h|\omega_j,\psi)$.  The NEB approach fails to account for sampling errors in any of the parameters estimated by ML.  To accommodate the uncertainties in the parameters of the $\omega$ distribution, \citet{yang2005bayes} used a hierarchical BEB approach by assigning prior probabilities to these parameters.

\subsection{Bootstrap Methods to Adjust for Uncertainty}
%A goal of statistical inference is to estimate a parameter, $\theta$, of a distribution $F$, where $\theta=t(F)$ is some function $t$ of $F$.  One way to do this is to draw a random sample, $\bm{x} = (x_1, x_2, \dots, x_n)$, from $F$ to obtain an empirical distribution $\hat{F}$ and calculate $\hat{\theta}=t(\hat{F})$ to estimate $\theta$.
%To access properties of $\hat{\theta}$, \citet{efron1979bootstrap} devised the bootstrap.
To construct confidence intervals for a parameter, $\theta$, and correct bias, \citet{efron1979bootstrap} devised the bootstrap.
%We obtain the \textit{bootstrap sample}, $\bf{x^*} = (x_1^*, x_2^*, \dots, x_n^*)$, by assigning equal probability, $1/n$, to each of the values of $\bf{x}$ and drawing values with replacement from $\bf{x}$.
A bootstrap sample, $\bm{x^*}$, is obtained by drawing the values, $x_1^*,\dots,x_n^*,$ with replacement from a random sample, $\bm{x}$.
For each of $b=1 \dots B$ bootstrap samples we can then calculate the bootstrap estimate, $\hat{\theta}^{*b}$, to obtain the bootstrap distribution of $\hat{\theta}$.  Bootstrap distributions are commonly used with phylogenetic data to test the topology of a proposed tree.
%When the data from a sample are assumed to be independent and identically distributed, a number of bootstrap samples, each of the same length as the observed sample, are generated by randomly sampling observational units with replacement from the sample.  The parameter is estimated for each bootstrap sample to obtain its bootstrap sampling distribution.  Such empirical distributions of the parameters can be used to check the stability of the estimates.  This approach is often applied to phylogenetic data to test the topology of a proposed tree.
%Put this first sentence in discussion
%\citet{breiman1996bagging} described a procedure to aggregate predictors generated from bootstrap replicates, which theoretical and experimental evidence shows can move some unstable predictors closer to optimality.
We applied the bootstrap to site patterns in a sequence alignment to adjust for the uncertainty in parameter estimates in EB classification.
%Although the context is different, the methods are similar to \citet{laird1987empirical} and \citet{breiman1996bagging}.
The procedure is illustrated in figure \ref{fig:bootstrap}:
\begin{enumerate}
  \item From an alignment of protein coding DNA sequences, $\bm{x}$, with $n$ codon sites, randomly sample site patterns with replacement to obtain a bootstrap sample, $\bm{x}^{*b}$, with $n$ sites.
  \item Estimate the MLEs, $\hat{\theta}^{*b}$, for bootstrap sample $\bm{x}^{*b}$.
  \item Use $\hat{\theta}^{*b}$ and $\bm{x}$ to calculate posterior probabilities, $Pr_h(\omega>1|x_h,\hat{\theta}^{*b})$, that each site, $h$, is under positive selection.
  \item Repeat steps 1 through 3 $B$ times to calculate $B$ sets of posterior probabilities for each codon site.
  \item Calculate an aggregate posterior probability that each site is under positive selection by, e.g., averaging posterior probabilities over bootstrap replicates, $\sum_{b=1}^BPr_h(\omega>1|x_h,\hat{\theta}^{*b})/B$.
% or by taking the median, $med_iPr_h^i(\omega>1|\bm{X},\theta^i)$.
\end{enumerate}
A preliminary implementation of the SBA method supporting codon models M2a, M8, and branch-site model A, built upon the codeml application from the PAML package \citep{yang2007paml}, can be found at \\ https://github.com/Jehops/codeml\_sba.

\subsection{Kernel Smoothing to Approximate the Bootstrap Distribution}
Kernel smoothing \citep{akaike1954approximation,parzen1962estimation,rosenblatt1956remarks,wand1994kernel} is class of nonparametric techniques that can improve estimation of a distribution.  The kernel density estimator for a continuous density $f$, $\hat{f}(x;h) = (nh)^{-1}\sum_{i=1}^nK([x-X_i]/h)$, includes a kernel density (probability) function, $K$, to locally average or smooth observations and the amount of smoothing is controlled by a bandwidth parameter, $h$.  For small $h$, each of the $h^{-1}K([x-X_i]/h)$ contributions are large only for $x$ close to some $X_i$ giving rise to a bumpy
distribution, whereas for $h$ large the $h^{-1}K([x-X_i]/h)$ contributions overlap giving a much smoother distribution \citep{silverman1987bootstrap}. We used kernel density estimation to create smoothed bootstrap distributions for the $p$ parameters of the $\omega$ distributions under models M2a and M8 using a uniform kernel.

Kernel density estimation requires a bandwidth parameter as input. One method for determining $h$ is using leave-one-out cross validation, $\hat{f}_{(-k)}(x;h) = (n-1)^{-1}h^{-1}\sum_{i\ne k}K([x - X_i]/h)$ \citep[p. 184]{venables2013modern}.  In this approach, $h$ is chosen to maximize the sum of the logged density
estimates $\sum_k \log \hat f_{(-k)}(x_k;h)$, where $\hat f_{(-k)}(x;h)$ is the kernel density estimate constructed from all of the $x_i$ except $x_k$.  However, our experiments using leave-one-out likelihood to choose an optimal bandwidth parameter for the $p$ parameters of M2a and M8 merely resulted in smoothed estimates of the biased bootstrap distributions.  To obtain conservative estimates of the $p$ parameters that suppressed the influence of instabilities we chose to over smooth by using a bandwidth parameter of $h=0.4$ for  all applications of SBA.

Adding kernel smoothing to the bootstrap algorithm increases the number of parameter estimates used in step 5 of the unsmoothed algorithm by sampling from a smoothed bootstrap distribution. The adjustment is in step 2 of the algorithm. The ML parameters estimated from bootstrap sample $b$, $\hat\theta^{*b}$, are replaced by $\theta^{sb}$ sampled from the smoothed bootstrap distribution.  The rest of the algorithm proceeds as in the unsmoothed version, but using $\theta^{sb}$ in place of $\hat\theta^{*b}$.

For model M8, the step 2 adjustment is as follows.  For each $\hat\theta^{*b}$, $p_{\omega < 1}^{sb}$ samples are repeatedly drawn from a univariate uniform distribution centered at $\hat p_{\omega < 1}^{*b}$ with width $2h$.  If necessary, the minimum and maximum points of the distribution are truncated to 0 and 1.  Let $\theta^{sb}$ denote $\hat\theta^{*b}$ with $p_{\omega<1}^{sb}$ replacing $\hat p_{\omega<1}^{*b}$ ($p_{\omega>1}^{sb} = 1-p_{\omega<1}^{sb}$).  The same procedure is used under model M2a, however, with three weight parameters, the sampling is done on a bivariate uniform distribution with the following additional restrictions: i) $p_{\omega<1}^{sb} + p_{\omega=1}^{sb} \le 1$, ii) $(\hat{p}_{\omega<1}^{*b} - h) \le p_{\omega<1}^{sb} \le (\hat{p}_{\omega<1}^{*b} + h)$, and iii) $(\hat{p}_{\omega=1}^{*b} - h) \le p_{\omega=1}^{sb} \le (\hat{p}_{\omega=1}^{*b} + h)$.  As with M8, if necessary, the minimum and maximum points of the distribution are truncated at 0 and 1, and $p_{\omega>1}^{sb} = 1 - p_{\omega<1}^{sb} - p_{\omega=1}^{sb}$.
\subsection{Simulation Studies}
Datasets were simulated using \emph{EvolverNSSites} from the PAML 4.8a package \citep{yang2007paml} and \emph{Indelible} \citep{fletcher2009indelible} following some of the settings described in \citet{wong2004accuracy}.  To compare the relative performance of BEB, NEB, and SBA for predicting sites under positive selection, 10 different simulation studies, divided into three scenarios, were used.  Table \ref{tab:sim} gives an overview of the $\omega$ distributions used to simulate the data.  The \textit{Correct Model Scenario} included four simulation studies where the nuisance parameters, $\kappa=1$ and $\pi_i=1/61$, matched the fitted model.  The \textit{Mild Misspecification} and \textit{Heavy Misspecification} scenarios included four simulation studies with mild misspecification and two studies with heavy misspecification of the fitted model, respectively.  The data in the \textit{Mild Misspecification Scenario} was simulated using $\kappa=8$ and empirical codon frequencies derived from application of the general time-reversible model \citep[p. 33]{Yang2006Computational} to the \textit{TrbL-VirB6-3} plasmid conjugative transfer protein of \textit{Rickettsia}.  In the fitted model, $\kappa$ was estimated, while the misspecification was introduced by using F3x4 (expected codon frequencies calculated using the nucleotide frequencies at the three codon positions).  For the \textit{Heavy Misspecification Scenario}, study 9 used the heavily biased codon frequencies from the Drosophila \textit{GstD1} gene and $\kappa=8$ to simulate the data.  In study 10, there were two heterogeneous classes of sites.  Half the sites were simulated using equal codon frequencies, $\kappa=1$, and $\omega=0.5$, while the other half with the Drosophila gsTD gene codon frequencies, $\kappa=8$, and $\omega=1$.  For both studies in this scenario, analysis was carried out using a single set of codon frequencies (set equal to $1/61$) and a single $\kappa$ parameter estimated for all sites in the data set.  For all studies in the three scenarios, 100 alignments, each having 500 codons, were simulated with the same 5-taxon tree from \citet{wong2004accuracy}.  The studies in the \textit{Correct Model Scenario} were repeated under model M2a with the 30-taxon tree from the same paper.

\subsection{Real Data Analysis}
Table \ref{tab:realDataResultsOverview} describes the real data sequences we analyzed under models M2a and M8 using NEB, BEB, and SBA.  Of the the 16 genes, eight code for transmembrane proteins in \textit{Rickettsia} (\textit{ccmF}, \textit{mivN}, \textit{perM}, \textit{pgpA}, \textit{RfaL}, \textit{TrbL-VirB6\_2}, and \textit{TrbL-VirB6\_3}) and were previously analyzed in \citet{bao2008likelihood}.  Three genes from the HIV-1 virus (\textit{env} \textit{pol}, and \textit{vif}) and a $\beta$-globin gene were described and analyzed in \citet{yang2000codon}, two primate genes (\textit{CDH3} encoding cadherin and \textit{ENAM} encoding enamelin), a lysin gene from \citet{yang2000maximum}, and the \emph{tax} gene from the human T-cell lymphotrophic virus (HTLV) that was analyzed by \citet{suzuki2004false}.  All data is available from the Dryad Digital Repository: http://dx.doi.org/10.5061/dryad.xxxxx.

\chapter{Exploring Branch-Site Codon Models}
\section{Introduction}

\chapter{Supplementary Materials}

\setcounter{table}{0}
\makeatletter
\renewcommand{\thetable}{S\@arabic\c@table}
\makeatother

\setcounter{figure}{0}
\makeatletter
\renewcommand{\thefigure}{S\@arabic\c@figure}
\makeatother

\setcounter{page}{1}

\begin{figure}[H]
    \centering
    <<CDF5taxaTL6,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.25_w0_0.25_5_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.25_w0_0.25_5_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.25_w0_0.25_5_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.25_w0_0.5_5_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.25_w0_0.5_5_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.25_w0_0.5_5_taxa_tl_6_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.5_w0_0.25_5_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.5_w0_0.25_5_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.5_w0_0.25_5_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.5_w0_0.5_5_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.5_w0_0.5_5_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.5_w0_0.5_5_taxa_tl_6_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.75_w0_0.25_5_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.75_w0_0.25_5_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.75_w0_0.25_5_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.75_w0_0.5_5_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.75_w0_0.5_5_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.75_w0_0.5_5_taxa_tl_6_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_5_taxa_m2a_lnl-p0_0.25_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_5_taxa_c2_lnl-p0_0.25_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_5_taxa_m2a_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_5_taxa_c2_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_5_taxa_m2a_lnl-p0_0.5_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_5_taxa_c2_lnl-p0_0.5_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_5_taxa_m2a_lnl-p0_0.5_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_5_taxa_c2_lnl-p0_0.5_w0_0.5_5_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_5_taxa_m2a_lnl-p0_0.75_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_5_taxa_c2_lnl-p0_0.75_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_5_taxa_m2a_lnl-p0_0.75_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_5_taxa_c2_lnl-p0_0.75_w0_0.5_5_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_5_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_5_taxa_m2a,lrs_p0_0.25_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_5_taxa_m2a,lrs_p0_0.25_w0_0.5_5_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_5_taxa_m2a,lrs_p0_0.5_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_5_taxa_m2a,lrs_p0_0.5_w0_0.5_5_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_5_taxa_m2a,lrs_p0_0.75_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_5_taxa_m2a,lrs_p0_0.75_w0_0.5_5_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{Cumulative distribution functions (CDF) of likelihood (C=0) and modified likelihood (C=2) ratio statistics under the nested model pair M1a/M2a for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 6.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF5taxaTL6}
\end{figure}

\clearpage

\begin{figure}[H]
    \centering
    <<CDF5taxaTL9,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.25_w0_0.25_5_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.25_w0_0.25_5_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.25_w0_0.25_5_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.25_w0_0.5_5_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.25_w0_0.5_5_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.25_w0_0.5_5_taxa_tl_9_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.5_w0_0.25_5_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.5_w0_0.25_5_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.5_w0_0.25_5_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.5_w0_0.5_5_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.5_w0_0.5_5_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.5_w0_0.5_5_taxa_tl_9_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.75_w0_0.25_5_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.75_w0_0.25_5_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.75_w0_0.25_5_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.75_w0_0.5_5_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.75_w0_0.5_5_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.75_w0_0.5_5_taxa_tl_9_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_5_taxa_m2a_lnl-p0_0.25_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_5_taxa_c2_lnl-p0_0.25_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_5_taxa_m2a_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_5_taxa_c2_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_5_taxa_m2a_lnl-p0_0.5_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_5_taxa_c2_lnl-p0_0.5_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_5_taxa_m2a_lnl-p0_0.5_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_5_taxa_c2_lnl-p0_0.5_w0_0.5_5_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_5_taxa_m2a_lnl-p0_0.75_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_5_taxa_c2_lnl-p0_0.75_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_5_taxa_m2a_lnl-p0_0.75_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_5_taxa_c2_lnl-p0_0.75_w0_0.5_5_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_5_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_5_taxa_m2a,lrs_p0_0.25_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_5_taxa_m2a,lrs_p0_0.25_w0_0.5_5_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_5_taxa_m2a,lrs_p0_0.5_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_5_taxa_m2a,lrs_p0_0.5_w0_0.5_5_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_5_taxa_m2a,lrs_p0_0.75_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_5_taxa_m2a,lrs_p0_0.75_w0_0.5_5_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 9.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF5taxaTL9}
\end{figure}

\clearpage

\begin{figure}[H]
    \centering
    <<CDF10taxaTL3,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.25_w0_0.25_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.25_w0_0.25_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.25_w0_0.25_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.25_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.25_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.25_w0_0.5_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.5_w0_0.25_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.5_w0_0.25_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.5_w0_0.25_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.5_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.5_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.5_w0_0.5_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.75_w0_0.25_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.75_w0_0.25_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.75_w0_0.25_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.75_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.75_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.75_w0_0.5_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_10_taxa_m2a_lnl-p0_0.25_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_10_taxa_c2_lnl-p0_0.25_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_10_taxa_m2a_lnl-p0_0.25_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_10_taxa_c2_lnl-p0_0.25_w0_0.5_10_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_10_taxa_m2a_lnl-p0_0.5_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_10_taxa_c2_lnl-p0_0.5_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_10_taxa_m2a_lnl-p0_0.5_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_10_taxa_c2_lnl-p0_0.5_w0_0.5_10_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_10_taxa_m2a_lnl-p0_0.75_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_10_taxa_c2_lnl-p0_0.75_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_10_taxa_m2a_lnl-p0_0.75_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_10_taxa_c2_lnl-p0_0.75_w0_0.5_10_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_10_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_10_taxa_m2a,lrs_p0_0.25_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_10_taxa_m2a,lrs_p0_0.25_w0_0.5_10_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_10_taxa_m2a,lrs_p0_0.5_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_10_taxa_m2a,lrs_p0_0.5_w0_0.5_10_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_10_taxa_m2a,lrs_p0_0.75_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_10_taxa_m2a,lrs_p0_0.75_w0_0.5_10_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a balanced, 10-taxon tree topology with branch lengths summing to 3.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF10taxaTL3}
\end{figure}

\clearpage

\begin{figure}[H]
    \centering
    <<CDF10taxaTL6,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.25_w0_0.25_10_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.25_w0_0.25_10_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.25_w0_0.25_10_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.25_w0_0.5_10_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.25_w0_0.5_10_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.25_w0_0.5_10_taxa_tl_6_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.5_w0_0.25_10_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.5_w0_0.25_10_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.5_w0_0.25_10_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.5_w0_0.5_10_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.5_w0_0.5_10_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.5_w0_0.5_10_taxa_tl_6_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.75_w0_0.25_10_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.75_w0_0.25_10_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.75_w0_0.25_10_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.75_w0_0.5_10_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.75_w0_0.5_10_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.75_w0_0.5_10_taxa_tl_6_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_10_taxa_m2a_lnl-p0_0.25_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_10_taxa_c2_lnl-p0_0.25_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_10_taxa_m2a_lnl-p0_0.25_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_10_taxa_c2_lnl-p0_0.25_w0_0.5_10_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_10_taxa_m2a_lnl-p0_0.5_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_10_taxa_c2_lnl-p0_0.5_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_10_taxa_m2a_lnl-p0_0.5_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_10_taxa_c2_lnl-p0_0.5_w0_0.5_10_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_10_taxa_m2a_lnl-p0_0.75_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_10_taxa_c2_lnl-p0_0.75_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_10_taxa_m2a_lnl-p0_0.75_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_10_taxa_c2_lnl-p0_0.75_w0_0.5_10_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_10_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_10_taxa_m2a,lrs_p0_0.25_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_10_taxa_m2a,lrs_p0_0.25_w0_0.5_10_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_10_taxa_m2a,lrs_p0_0.5_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_10_taxa_m2a,lrs_p0_0.5_w0_0.5_10_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_10_taxa_m2a,lrs_p0_0.75_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_10_taxa_m2a,lrs_p0_0.75_w0_0.5_10_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a balanced, 10-taxon tree topology with branch lengths summing to 6.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF10taxaTL6}
\end{figure}

\clearpage

\begin{figure}[H]
    \centering
    <<CDF10taxaTL9,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.25_w0_0.25_10_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.25_w0_0.25_10_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.25_w0_0.25_10_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.25_w0_0.5_10_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.25_w0_0.5_10_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.25_w0_0.5_10_taxa_tl_9_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.5_w0_0.25_10_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.5_w0_0.25_10_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.5_w0_0.25_10_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.5_w0_0.5_10_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.5_w0_0.5_10_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.5_w0_0.5_10_taxa_tl_9_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.75_w0_0.25_10_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.75_w0_0.25_10_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.75_w0_0.25_10_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.75_w0_0.5_10_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.75_w0_0.5_10_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.75_w0_0.5_10_taxa_tl_9_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_10_taxa_m2a_lnl-p0_0.25_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_10_taxa_c2_lnl-p0_0.25_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_10_taxa_m2a_lnl-p0_0.25_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_10_taxa_c2_lnl-p0_0.25_w0_0.5_10_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_10_taxa_m2a_lnl-p0_0.5_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_10_taxa_c2_lnl-p0_0.5_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_10_taxa_m2a_lnl-p0_0.5_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_10_taxa_c2_lnl-p0_0.5_w0_0.5_10_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_10_taxa_m2a_lnl-p0_0.75_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_10_taxa_c2_lnl-p0_0.75_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_10_taxa_m2a_lnl-p0_0.75_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_10_taxa_c2_lnl-p0_0.75_w0_0.5_10_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_10_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_10_taxa_m2a,lrs_p0_0.25_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_10_taxa_m2a,lrs_p0_0.25_w0_0.5_10_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_10_taxa_m2a,lrs_p0_0.5_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_10_taxa_m2a,lrs_p0_0.5_w0_0.5_10_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_10_taxa_m2a,lrs_p0_0.75_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_10_taxa_m2a,lrs_p0_0.75_w0_0.5_10_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a balanced, 10-taxon tree topology with branch lengths summing to 9.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF10taxaTL9}
\end{figure}

\clearpage

\begin{figure}[H]
    \centering
    <<CDF32taxaTL3,echo=F,warning=F>>=
    p0_0.25_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.25_w0_0.25_32_taxa_tl_3_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.25_w0_0.25_32_taxa_tl_3_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.25_w0_0.25_32_taxa_tl_3_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.25_w0_0.5_32_taxa_tl_3_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.25_w0_0.5_32_taxa_tl_3_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.25_w0_0.5_32_taxa_tl_3_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.5_w0_0.25_32_taxa_tl_3_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.5_w0_0.25_32_taxa_tl_3_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.5_w0_0.25_32_taxa_tl_3_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.5_w0_0.5_32_taxa_tl_3_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.5_w0_0.5_32_taxa_tl_3_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.5_w0_0.5_32_taxa_tl_3_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.75_w0_0.25_32_taxa_tl_3_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.75_w0_0.25_32_taxa_tl_3_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.75_w0_0.25_32_taxa_tl_3_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.75_w0_0.5_32_taxa_tl_3_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.75_w0_0.5_32_taxa_tl_3_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.75_w0_0.5_32_taxa_tl_3_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_32_taxa_m2a_lnl-p0_0.25_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_32_taxa_c2_lnl-p0_0.25_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_32_taxa_m2a_lnl-p0_0.25_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_32_taxa_c2_lnl-p0_0.25_w0_0.5_32_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_32_taxa_m2a_lnl-p0_0.5_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_32_taxa_c2_lnl-p0_0.5_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_32_taxa_m2a_lnl-p0_0.5_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_32_taxa_c2_lnl-p0_0.5_w0_0.5_32_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_32_taxa_m2a_lnl-p0_0.75_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_32_taxa_c2_lnl-p0_0.75_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_32_taxa_m2a_lnl-p0_0.75_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_32_taxa_c2_lnl-p0_0.75_w0_0.5_32_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_32_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_32_taxa_m2a,lrs_p0_0.25_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_32_taxa_m2a,lrs_p0_0.25_w0_0.5_32_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_32_taxa_m2a,lrs_p0_0.5_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_32_taxa_m2a,lrs_p0_0.5_w0_0.5_32_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_32_taxa_m2a,lrs_p0_0.75_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_32_taxa_m2a,lrs_p0_0.75_w0_0.5_32_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a balanced, 32-taxon tree topology with branch lengths summing to 3.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF32taxaTL3}
\end{figure}

\clearpage

\begin{figure}[H]
    \centering
    <<CDF32taxaTL6,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.25_w0_0.25_32_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.25_w0_0.25_32_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.25_w0_0.25_32_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.25_w0_0.5_32_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.25_w0_0.5_32_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.25_w0_0.5_32_taxa_tl_6_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.5_w0_0.25_32_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.5_w0_0.25_32_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.5_w0_0.25_32_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.5_w0_0.5_32_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.5_w0_0.5_32_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.5_w0_0.5_32_taxa_tl_6_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.75_w0_0.25_32_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.75_w0_0.25_32_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.75_w0_0.25_32_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.75_w0_0.5_32_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.75_w0_0.5_32_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.75_w0_0.5_32_taxa_tl_6_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_32_taxa_m2a_lnl-p0_0.25_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_32_taxa_c2_lnl-p0_0.25_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_32_taxa_m2a_lnl-p0_0.25_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_32_taxa_c2_lnl-p0_0.25_w0_0.5_32_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_32_taxa_m2a_lnl-p0_0.5_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_32_taxa_c2_lnl-p0_0.5_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_32_taxa_m2a_lnl-p0_0.5_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_32_taxa_c2_lnl-p0_0.5_w0_0.5_32_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_32_taxa_m2a_lnl-p0_0.75_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_32_taxa_c2_lnl-p0_0.75_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_32_taxa_m2a_lnl-p0_0.75_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_32_taxa_c2_lnl-p0_0.75_w0_0.5_32_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_32_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_32_taxa_m2a,lrs_p0_0.25_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_32_taxa_m2a,lrs_p0_0.25_w0_0.5_32_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_32_taxa_m2a,lrs_p0_0.5_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_32_taxa_m2a,lrs_p0_0.5_w0_0.5_32_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_32_taxa_m2a,lrs_p0_0.75_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_32_taxa_m2a,lrs_p0_0.75_w0_0.5_32_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a balanced, 32-taxon tree topology with branch lengths summing to 6.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF32taxaTL6}
\end{figure}

\clearpage

\begin{figure}[H]
    \centering
    <<w0MLEsM1a, echo=F,warning=F>>=
    p0_0.25_w0_0.25_m1a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.25_m1a_mles.csv",header=F)
    colnames(p0_0.25_w0_0.25_m1a_mles) <- c('k','p0','p1','w0','w1')
    p0_0.25_w0_0.5_m1a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m1a_mles.csv",header=F)
    colnames(p0_0.25_w0_0.5_m1a_mles) <- c('k','p0','p1','w0','w1')
    p0_0.5_w0_0.25_m1a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.25_m1a_mles.csv",header=F)
    colnames(p0_0.5_w0_0.25_m1a_mles) <- c('k','p0','p1','w0','w1')
    p0_0.5_w0_0.5_m1a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.5_m1a_mles.csv",header=F)
    colnames(p0_0.5_w0_0.5_m1a_mles) <- c('k','p0','p1','w0','w1')
    p0_0.75_w0_0.25_m1a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.25_m1a_mles.csv",header=F)
    colnames(p0_0.75_w0_0.25_m1a_mles) <- c('k','p0','p1','w0','w1')
    p0_0.75_w0_0.5_m1a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.5_m1a_mles.csv",header=F)
    colnames(p0_0.75_w0_0.5_m1a_mles) <- c('k','p0','p1','w0','w1')

    w0 <- c(p0_0.25_w0_0.25_m1a_mles$w0,p0_0.25_w0_0.5_m1a_mles$w0,
                  p0_0.5_w0_0.25_m1a_mles$w0,p0_0.5_w0_0.5_m1a_mles$w0,
                  p0_0.75_w0_0.25_m1a_mles$w0,p0_0.75_w0_0.5_m1a_mles$w0)

    N <- nrow(p0_0.25_w0_0.25_m1a_mles)

    mle.data <- data.frame(w0,
                           weight=rep(c(0.25,0.5,0.75),each=2*N),
                           omega=rep(c(0.25,0.5),each=N,times=3))

    cdf.plot <- ggplot(mle.data,aes(w0,y=..ncount..)) +
        geom_histogram(binwidth=.005,fill=I('black')) +
        labs(x=expression(omega[0]),y='') +
facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(2,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              text=element_text(size=16),
              axis.text.y=element_blank(),
              axis.ticks.y=element_blank())
@
\caption[]{MLEs of the $\omega_0$ parameter under model M1a for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 3.  The value of $\omega_0$ and its weight, $p_0$, used to simulate the data are shown as column and row labels.}
  \label{fig:w0MLEsM1a}
\end{figure}

\clearpage

\begin{figure}[H]
    \centering
    <<CDF5taxaPS, echo=F,warning=F>>=
    p0_0.25_w0_0.5_5_taxa_m0_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m0_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_c2_lnLs.csv",sep=',')

    lrt0 <- 2*(p0_0.25_w0_0.5_5_taxa_m1a_lnl-p0_0.25_w0_0.5_5_taxa_m0_lnl)
    lrt1 <- 2*(p0_0.25_w0_0.5_5_taxa_m2a_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl)
    lrt2 <- 2*(p0_0.25_w0_0.5_5_taxa_c2_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl)
    rej <- which(lrt0>=qchisq(.9,1))

    lrt0 <- sort(2*(p0_0.25_w0_0.5_5_taxa_m1a_lnl[rej]-p0_0.25_w0_0.5_5_taxa_m0_lnl[rej]))
    lrt1 <- sort(2*(p0_0.25_w0_0.5_5_taxa_m2a_lnl[rej]-p0_0.25_w0_0.5_5_taxa_m1a_lnl[rej]))
    lrt2 <- sort(2*(p0_0.25_w0_0.5_5_taxa_c2_lnl[rej]-p0_0.25_w0_0.5_5_taxa_m1a_lnl[rej]))

    N <- length(lrt0)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrt1,lrt2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- c(prob.t,1:N/N,1:N/N)

    cdf.data <- data.frame(lrs,cprob,model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="Likelihood Ratio Statistic",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.80,.7),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{Cumulative distribution functions (CDF) of likelihood ratio (LR) statistics without (C=0) and with (C=2) likelihood modification after pre-screening the data with M0/M1a LR tests.  The modified LR statistics were calculated under the nested model pair M1a/M2a for 4987 simulated sequence alignments that were rejected under the M0/M1a null hypothesis of only one $\omega$ site class.  The alignments were simulated with 25\% of the sites evolving under $\omega=0.5$ and the remaining sites evolving under $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 3.  A modified likelihood tuning parameters of $C=2$ was used.  A $\chi^2_0/2+\chi^2_1/2$ CDF is also included.}
  \label{fig:CDFPS}
\end{figure}

\clearpage

\begin{figure}[H]
  \centering
<<klp0.75w0.5, echo=F>>=
site.lik.truth <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3_10000_codons/data/sim_p0_0.75_w0_0.5/site_lik_truth.csv",header=F)
colnames(site.lik.truth) <- c('cnt','lik')
n <- sum(site.lik.truth$cnt)
cnt <- site.lik.truth$cnt
y.truth <- site.lik.truth$lik
y.truth[y.truth==0] <- 1e-12
kl <- numeric(0)
vkl <- numeric(0)
for (p in seq(0,1,.1)) {
    for (w in seq(0,1,.1)) {
        site.lik <- read.csv(paste("~/scm/modl.git/sim/null/5_taxa_bl_3_10000_codons/data/sim_p0_0.75_w0_0.5/site_like_p_",p,"_w_",w,".csv",sep=''),header=F)
        y <- site.lik[,2]
        y[y==0] <- 1e-12
        y.bar <- sum(cnt*log(y.truth / y))/n
        kl <- c(kl,y.bar)
        vkl <- c(vkl,sum(cnt*(log(y.truth / y) - y.bar)^2)/n)
    }
}
p <- rep(seq(0,1,.1),each=11)
w <- rep(seq(0,1,.1),times=11)
lb <- kl - 2*sqrt(vkl/10000)
ub <- kl + 2*sqrt(vkl/10000)
kl.data <- data.frame(kl,lb,ub,w,p)
kl.plot <- ggplot(kl.data,aes(p,kl)) +
    labs(x=expression("p"[0]),y="Kullback-Leibler Divergence") +
    geom_point() +
    geom_hline(yintercept=0) + facet_wrap(~w,ncol=3,labeller=label_bquote(cols=omega[0]*'='*.(w))) +
    coord_cartesian(ylim = c(-0.001, 0.01)) +
    geom_errorbar(aes(ymin=lb,ymax=ub))

kl.plot + theme(panel.spacing=unit(0,"lines"),
                panel.background=element_blank(),
                strip.background=element_blank(),
                legend.title=element_blank(),
                legend.key=element_rect(fill="transparent"),
                text=element_text(size=12),
                panel.border = element_rect(colour = "black", fill=NA, size=1))
@
\caption[]{Approximations of the Kullback-Leibler divergences between the distributions of site likelihoods for the generating model and other mixing distributions.  The approximations were obtained as the mean lnL difference between 10,000 site patterns generated under model M1a using a 5-taxon tree with branch lengths summing to 3 and the mixing distribution $(p_0,\omega_0)=(0.75,0.5)$, and other mixing distributions with varying weights on values of $\omega$ ranging from $0$ to $1$.  Error bars for two standard errors $(s_{KL}/\sqrt{10000})$ above and below each Kullback-Leibler estimate are included.  Points missing from each plot are above the visible range.}
\label{fig:KLp0.75w0.5}
\end{figure}

\section{Appendix I}
\singlespacing
\setcounter{equation}{0}
We prove below that the limiting distribution of the modified likelihood ratio statistic is $\chi_0^2/2 + \chi_1^2/2$.  We assume without proof that the codon model considered is identifiable: for a fixed tree, no two distinct sets of parameters give exactly the same distribution of site patterns.  Such results have not been established for codon models.  However, there are a number identifiability results for similar rates-across-sites \citep{allman2008identifiability} and covarion models \citep{allman2009identifiability} that suggest it is a plausible assumption.  We assume as well that third partial derivatives of the probability of a site pattern, over any set of parameters, is bounded in a neighbourhood of the true parameter values.  Finally we assume that the covariance matrix $V$ defined below is positive definite.

\subsection{Taylor's Series}
Let $\beta=(\omega_+,\psi^T)^T$ and let $\beta^{0}=[1,(\psi^{0})^T]^T$ where $\psi^0$ denotes the true generating parameter under the null hypothesis.  It follows similarly as in \cite{chen2004testing} that $\hat\beta \rightarrow \beta^0$ where $\hat\beta$ is the modified ML estimator.  Since the convergence of the modified ML estimator $\hat p_+$ of $p_+$ is at present unclear, we approximate modified likelihood ratios through Taylor's series approximation of the log likelihoods, with respect to $\beta$ at $\beta^0$, holding $p_+$ fixed:
\begin{equation}
  \label{eq:a1}
\tilde l(p_+,\omega_+,\psi) -  l_H(\psi^0) = L(\beta^0;p_+) + Q(\beta^0;p_+) + C(\beta^*;p_+) + C\log(p_+)
\end{equation}
where $L$, $Q$ and $C$ denote the linear, quadratic and cubic terms and $\beta^*$ is some value between $\beta$ and $\beta^0$.

\subsection{Linear Term}
The linear term is
\begin{equation}
  \nonumber
  L(\beta^0;p_+) = (\beta - \beta^0)^T \sum_h \pd{}{\beta} \log p(x_h;\beta^0,p_+)
\end{equation}
It is not difficult to show that the collection, $S(x_h)_\psi,$ of derivatives of $\log p(x_h;\beta^0,p_+)$ with respect to $\psi$ are independent of $p_+$.  The other derivative is
\begin{equation}
  \label{eq:a3}
  \pd{}{\omega_+} \log p(x_h;\beta^0,p_+) = p_+ (1-p_0) \pd{}{\omega} p(x|1;\zeta)/p(x_h;\beta^0,p_+) =: p_+ S(x_h)_{\omega_+}
\end{equation}
so that $S(x_h)^T=[S(x_h)_{\omega_+}, S(x_h)_\psi^T]$ is independent of $p_+$.  Standard likelihood theory gives that $E[S(X_h)]=0$, so by the Central Limit Theorem, $n^{-1/2}S_n=n^{-1/2}\sum S(X_h)$ is approximately normal with mean 0 and a covariance matrix we denote by $V$.  Let $\delta_n^T=\sqrt{n}[p_+(\omega_+-1), (\psi - \psi^0)^T].$  Then
\begin{equation}
  \label{eq:a4}
  L(\beta^0;p_+) = n^{-1/2}S_n^T \delta_n
\end{equation}

\subsection{Quadratic Term}
The quadratic term in (\ref{eq:a1}) is
\begin{equation}
  \label{eq:a5}
  Q(\beta^0;p_+) = \frac{1}{2} (\beta - \beta^0)^T l^{(2)}(\beta^0)(\beta - \beta^0)
\end{equation}
where
\begin{equation}
  \label{eq:a6}
  l^{(2)}(\beta^0) = \sum_h Q^{(1)}(x_h;\beta^0,p_+) + \sum_h \pd{}{\beta} \log p(x_h;\beta^0,p_+) \pd{}{\beta} \log p(x_h;\beta^0,p_+)^T
\end{equation}
and $p_H(x_h) Q^{(1)}(x_h;\beta^0,p_+)_{ij}$ is the partial derivative of $p(x_h;\beta^0,p_+)$ with respect to $\beta_i$ and $\beta_j$.  It is not difficult to see that $p_H(x_h) Q^{(1)}(x_h;\beta^0,p_+)_{ij}$ is independent of $p_+$ unless $i=j=1$, in which case it equals $p_+$ times a partial derivative of the form $\partial^2 p(x|1;\zeta)/\partial \omega^2$.  Standard likelihood theory gives that $E[Q^{(1)}(X_h;\beta^0,p_+)]=0$.  Thus the Central Limit Theorem gives that $Q_n^{(1)}:=\sum Q^{(1)}(x_h;\beta^0,p_+)=O_P(n^{1/2})$ for any fixed $p_+$.  Since $Q_n^{(1)}$ depends linearly on $p_+$, $Q_n^{(1)}=O_p(n^{1/2})$ uniformly in $p_+$.  Substituting in (\ref{eq:a6}), then (\ref{eq:a5}) and using the relationships between derivatives of $\log p(x_h;\beta^0,p_+)$ and $S(x_h)$ established earlier,
\begin{equation}
  \label{eq:a7}
  Q(\beta^0;p_+) = \frac{1}{2}(\beta - \beta^0)^TQ_n^{(1)}(\beta - \beta^0) + \frac{1}{2n}\delta_n^T \sum_h S(x_h) S(x_h)^T \delta_n
\end{equation}
Let $Q_n^{(2)}=\sum_h S(x_h) S(x_h)^T-nV$.  Since $E[S(x_h) S(x_h)^T]=V$, the Central Limit Theorem gives that $Q_n^{(2)}=O_P(n^{1/2})$.  Since
\begin{equation}
  \label{eq:a18}
  Q(\beta^0;p_+) = \frac{1}{2}(\beta - \beta^0)^TQ_n^{(1)}(\beta - \beta^0) + \frac{1}{2n}\delta_n^T Q_n^{(2)} \delta_n - \frac{1}{2}\delta_n^T V \delta_n
\end{equation}
for $\delta_n=O_P(1)$ we have that
\begin{equation}
  \label{eq:a8}
  Q(\beta^0;p_+) = - \frac{1}{2}\delta_n^T V \delta_n + O_P(n^{-1/2})
\end{equation}

\subsection{Cubic Term}
The cubic term in (\ref{eq:a1}) is
\begin{equation}
  \label{eq:a9}
  C(\beta^*;p_+) = \frac{1}{6} \sum_{ijk} l^{(3)}(\beta^*;p_+)_{ijk} (\beta - \beta^0)_i(\beta - \beta^0)_j (\beta - \beta^0)_k
\end{equation}
where $l^{(3)}(\beta^*;p_+)_{ijk}$ denotes the third partial derivative of the log likelihood with respect to $\beta_i$, $\beta_j$ and $\beta_k$.  Since we have assumed third partial derivatives of $\log p(x_h;\beta^0,p_+)$ are bounded in a neighbourhood of $\beta^0$ by, say, $M(x_h)$, $|l^{(3)}(\beta^*;p_+)|/n$ is bounded by $n^{-1}\sum_h M(X_h).$  It follows by the Law of Large Numbers that $l^{(3)}(\beta^*;p_+)=O_P(n)$ and consequently that for $\beta-\beta^0=O_P(n^{-1/2})$, $C(\beta^*;p_+)=O_p(n^{-1/2})$.  Combining (\ref{eq:a4}) and (\ref{eq:a8}) in (\ref{eq:a1}) gives that for $\delta_n=O_P(1)$,
\begin{equation}
  \label{eq:a10}
  \tilde l(p_+,\omega_+,\psi) - l_H(\psi^0) = n^{-1/2}S_n^T \delta_n - \frac{1}{2}\delta_n^T V \delta_n + C\log(p_+)+ O_P(n^{-1/2})
\end{equation}

\subsection{Approximation with the modified MLE}
Since $\hat\delta$ has not been shown to be equal to $O_p(1)$, (\ref{eq:a10}) does not immediately apply.  However, since $b_n=\hat\delta/|\hat\delta|=O_p(1)$, the argument for (\ref{eq:a10}) gives that
\begin{equation}
  \label{eq:a11}
  \tilde l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\psi^0) =  |\hat\delta|^2 \{n^{-1/2}S_n^T b_n/ |\hat\delta| - \frac{1}{2}b_n^T V b_n + O_P(n^{-1/2})\} + C\log(\hat p_+)
\end{equation}
Since $V$ is positive definite, the right-hand side of (\ref{eq:a11}) becomes negative if $|\hat\delta|$ diverges.  However, since $\hat\beta$ is a maximizer, the difference in (\ref{eq:a11}) is always positive.  Thus it must be the case that $\hat\delta=O_p(1)$.  This implies that $\hat\psi-\psi^0=O_P(n^{-1/2})$ and that $\hat p_+ (\hat\omega_+ - 1) =O_P(n^{-1/2})$.  Similarly as in Lemma 1 of \cite{chen2004testing}, with probability, converging to 1, $\hat p_+\ge \epsilon$ for some $\epsilon > 0$, so that $\hat\omega_+ - 1=O_P(n^{-1/2})$.  Thus the approximation (\ref{eq:a10}) applies with $\beta=\hat\beta$:
\begin{eqnarray}
  \nonumber
  \tilde l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\psi^0) &=& n^{-1/2}S_n^T \hat\delta_n - \frac{1}{2}\hat\delta_n^T V \hat\delta_n + C\log(\hat p_+)+ O_P(n^{-1/2})\\
\label{eq:a12}
&\le& \max_{\delta,p_+} \{n^{-1/2}S_n^T \delta - \frac{1}{2} \delta^T V \delta + C\log(p_+)\} + O_P(n^{-1/2})
\end{eqnarray}
The inequality (\ref{eq:a12}) holds when maximization is restricted so that the maximizing $\delta$ and $p_+$ correspond to a valid $\beta$ and $p_+$: $\delta_{\omega_+} \ge 0$ and $p_+ \le 1$.  If we denote the corresponding $\beta$ and $p_+$ as $\tilde\beta$ and $\tilde p_+$, since the maximizing $\delta$ and $p_+$ are $O_P(1)$, the expression in (\ref{eq:a12}) is the same as $\tilde l(\tilde p_+,\tilde\omega_+,\tilde\psi) - l_H(\psi^0)$ up to the order indicated.  Since $\tilde  l(\hat p_+,\hat\omega_+,\hat\psi)- l_H(\psi^0)$ is larger than $\tilde l(\tilde p_+,\tilde\omega_+,\tilde\psi) - l_H(\psi^0)$, the reverse inequality holds in (\ref{eq:a12}) as well, implying that it is an equality.  The maximized value of $C\log(p_+)$ is $C\log(1)=0$.  Thus (\ref{eq:a12}) is
\begin{equation}
  \label{eq:a13}
  \tilde l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\psi^0) = \max_{\delta} \{n^{-1/2}S_n^T \delta - \frac{1}{2} \delta^T V \delta\}+ O_P(n^{-1/2})
\end{equation}

\subsection{The log likelihood under the null}
No modification of the likelihood is considered under the null, so standard ML results gives that
\begin{equation}
  \label{eq:a20}
l_H(\psi)- l_H(\psi^0) = (n^{-1/2}S_{n\psi})^TV_{\psi\psi}^{-1}(n^{-1/2}S_{n\psi} )+  O_P(n^{-1/2})
\end{equation}
The difference between (\ref{eq:a12}) and (\ref{eq:a20}) gives that the modified likelihood ratio satisfies that
\begin{equation}
  \label{eq:a14}
 \tilde l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\psi) =  \max_{\delta} \{2n^{-1/2}S_n^T \delta - \delta^T V \delta\}
-  (n^{-1/2}S_{n\psi})^TV_{\psi\psi}^{-1}(n^{-1/2}S_{n\psi} )+  O_P(n^{-1/2})
\end{equation}

\subsection{Maximization under the alternative}
Omitting details, after simplification, maximizing over $\delta$ with $\delta_{\omega_+}$ fixed gives
\begin{equation}
  \label{eq:a15}
 \max_{\delta} \{2n^{-1/2}S_n^T \delta - \delta^T V \delta\}
=   (n^{-1/2}S_{n\psi})^TV_{\psi\psi}^{-1}(n^{-1/2}S_{n\psi} ) + 2 \delta_{\omega_+} n^{-1/2} S_{n\omega_+}^c - \delta_{\omega_+}^2 V_{\omega_+}^c
\end{equation}
where
\begin{equation}
  \label{eq:a16}
  S_{n\omega_+}^c = S_{n\omega_+} - V_{\omega_+\psi} V_{\psi\psi}^{-1} S_{n\psi}, ~~~ V_{\omega_+}^c = V_{\omega_+\omega_+} - V_{\omega_+\psi} V_{\psi\psi}^{-1} V_{\psi\omega_+}
\end{equation}
If $S_{n\omega_+}^c < 0$ in (\ref{eq:a15}) the right hand side is decreasing in $\delta_{\omega_+}$ and so, subject to the restriction that $\delta_{\omega_+}\ge 0$, the maximizing $\delta_{\omega_+}=0$.  Otherwise the maximizer is $n^{-1/2} S_{n\omega_+}^c/\sqrt{V_{\omega_+}^c}$.  Substituting in (\ref{eq:a15}) and (\ref{eq:a14}) gives
\begin{equation}
  \label{eq:a17}
 \tilde l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\psi) = [n^{-1/2} S_{n\omega_+}^c/\sqrt{V_{\omega_+}^c}]_+^2 + O_P(n^{-1/2})
\end{equation}

\subsection{Distribution of $S_{n\omega_+}^c$}
Because $n^{-1/2}S_{n\omega_+}^c$ is a linear transformation of $n^{-1/2}S_n$ which has an approximate normal distribution with mean 0 and covariance matrix $V$, it too has a normal distribution.  It has mean 0 and variance
\begin{eqnarray}
\nonumber
  {\rm Var}(n^{-1/2}S_{n\omega_+}^c) &=& {\rm Var}(n^{-1/2}S_{n\omega_+}) - 2 V_{\omega_+\psi} V_{\psi\psi}^{-1} {\rm Cov}(n^{-1/2}S_{n\psi},n^{-1/2}S_{n\omega_+}) \\
\nonumber
&&~~~~~~~~~
+ V_{\omega_+\psi} V_{\psi\psi}^{-1}  {\rm Var}(n^{-1/2}S_{n\psi})  V_{\psi\psi}^{-1} V_{\psi\omega_+}\\
\nonumber
&=& V_{\omega_+\omega_+} - 2 V_{\omega_+\psi} V_{\psi\psi}^{-1} V_{\psi\omega_+} + V_{\omega_+\psi} V_{\psi\psi}^{-1} V_{\psi\psi} V_{\psi\psi}^{-1} V_{\psi\omega_+} = V_{\omega_+}^c
\end{eqnarray}
Thus $n^{-1/2}S_{n\omega_+}^c/\sqrt{V_{\omega_+}^c}$ is approximately standard normal.  It follows from $Z\sim N(0,1)$ giving $Z^2\sim \chi_1^2$ and $Z^2$ being independent of the event that $Z>0$, that the limiting distribution of $W$ is $\chi_1^2/2 + \chi_0^2/2.$

\bibliographystyle{plain}
\bibliography{/home/jrm/scm/references.git/refs}

\end{document}
