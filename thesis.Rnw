\documentclass[12pt]{dalthesis}

\usepackage{amsmath}
\usepackage{bm} % for bold math symbols
\usepackage{booktabs} % better tables
%\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} % margins
% for subfigures (requires caption, breaks knitr w/o subfloat defined below)
\usepackage{caption,subcaption}
%\captionsetup{compatibility=false}
\usepackage{float}
\usepackage{glossaries}
\usepackage{graphicx} % obviously for graphics
% \usepackage{latexsym} % MBE template for some fonts
\usepackage{lineno} % line numbers
\usepackage{mathtools} % an extension to amsmath to fix bugs
\usepackage{multirow} % column cells that span multiple rows
\usepackage{natbib} % nicer references
%\usepackage[natbib=true,style=apa]{biblatex}
%\addbibresource{/home/jrm/scm/references.git/refs.bib}
\usepackage{paralist} % inline lists
%\usepackage[section]{placeins} % keep figures and table inside section
\usepackage{rotating} % for landscape tables
\usepackage{setspace} % for line spacing
% need subfloat b/c knitr's fig.subcap was built with deprecated subfig package
% \newcommand{\subfloat}[2][need a sub-caption]{\subcaptionbox{#1}{#2}}
\usepackage[table]{xcolor}
\definecolor{lightgray}{gray}{0.9}
\usepackage{texshade}
\usepackage[flushleft]{threeparttable} % description under table
\usepackage{tikz}
\usepackage{tikz-qtree}
\usepackage[noindentafter,tiny]{titlesec}
\titleformat{\subsection}{\itshape\small\bfseries}{\thesubsection}{1em}{}
\titlespacing{\section}{0pt}{6pt}{6pt}
\titlespacing{\subsection}{0pt}{5pt}{5pt}
\usepackage{verbatim} % comments

\usetikzlibrary{arrows}

% specialcell is for line breaks in table cells
\newcommand{\specialcell}[2][c]{\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

% partial derivative command from Dr. Susko
\newcommand{\pd}[2]{\frac{\partial#1}{\partial#2}}

\renewcommand{\figurename}{Fig.}

\raggedbottom

\makenoidxglossaries
\setacronymstyle{long-short}
\newacronym{beb}{BEB}{Bayes empirical Bayes}
\newacronym{cdf}{CDF}{cumulative distribution function}
\newacronym{cs}{CS}{codon substitution}
\newacronym{dna}{DNA}{deoxyribonucleic acid}
\newacronym{eb}{EB}{empirical Bayes}
\newacronym{jc69}{JC69}{Jukes and Cantor model}
\newacronym{lr}{LR}{likelihood ratio}
\newacronym{ml}{ML}{maximum likelihood}
\newacronym{mle}{MLE}{maximum likelihood estimate}
\newacronym{mrna}{mRNA}{messenger RNA}
\newacronym{neb}{NEB}{Na\"ive empirical Bayes}
\newacronym{roc}{ROC}{receiver operator characteristic}
\newacronym{rna}{RNA}{ribonucleic acid}
\newacronym{sba}{SBA}{smoothed bootstrap aggregation}
\newacronym{trna}{tRNA}{transfer RNA}
\newacronym{w}{$\omega$}{the ratio of nonsynonymous to synonymous codon substitutions}

\begin{document}

<<setup,include=F>>=
library(ape)
library(RColorBrewer)
library(ggplot2)
library(grid)
library(gtable)
library(knitr)
opts_chunk$set(fig.path='figures/',fig.align='center',fig.show='hold',cache=T,autodep=T) #$
options(formatR.arrow=TRUE,width=90)
setwd("/home/jrm/scm/thesis.git/")
@

\title{Assessing and Improving the Reliability of Models of Molecular Evolution}
\author{Joseph Mingrone}

\phd
\defencemonth{November}
\defenceyear{2021}

% \dedicate{Optionally, the thesis can be dedicated to someone, and the student can enter the dedication content here.}

\frontmatter

\begin{abstract}[s]
  Selection pressure that has acted upon proteins and amino acids can be revealed through a ratio of nonsynonymous to synonymous codon substitutions ($\omega$).  Markov models of codon substitution fitted to an alignment of homologous protein-coding DNA sequences estimate an $\omega$ mixing distribution in a likelihood framework in order to detect positive selection.  Publications describing codon substitution model implementations have nearly $10,000$ citations from research in a variety of fields, from vaccine design to mammalian physiology.

  The site models of codon substitution used to detect positive selection at amino acids sites first use a pre-screening likelihood ratio (LR) test for positive selection at the level of the protein.  Due to statistical irregularity, the large-sample distributions of the LR statistic are often not justified and thresholds determined from the distributions can give larger than expected type I error rates.  Presented in Chapter \ref{chap:modl} is a modified LR test for protein-level selection.  The modified LR test is shown to restore statistical regularity to give tractable LR statistic distributions.  No matter the parameter settings of the underlying null hypothesis, the modified LR gives approximately correct type I error probabilities when the number of codon sites is not too small.  Simulation results show that type I error rates are closer to expectations without loss of power.  Under certain data-generation settings, very different estimated $\omega$ distributions can give nearly identical site likelihoods when the number of taxa and the total tree length are not large enough.

  After the pre-screening LR test, most codon substitution models use an empirical Bayes approach to detect positive selection at individual amino acid sites.  After model parameters are estimated via maximum likelihood, they are passed to Bayes formula to compute the posterior probability that a site evolved under positive selection.  A difficulty with the empirical Bayes approach is that estimates with large errors can negatively impact classification.  Presented in Chapter \ref{chap:sba} is a new technique called smoothed bootstrap aggregation (SBA) that uses bootstrapping and kernel smoothing to accommodate uncertainty in the estimates.   Simulation results show that SBA balances accuracy and power at least as well as Bayes empirical Bayes (BEB), and when parameter estimates are unstable, the performance gap between BEB and SBA can widen in favour of SBA.

  Branch-site models of codon substitution, like the site models, can detect positive selection at a subset of amino acid sites.  Unlike the site models however, the branch-site pre-screening LR test limits positive selection to prespecified branches on the phylogeny.  Chapter \ref{chap:bs} includes new simulation studies, which show limitations to these widely used models.  The branch-site LR distributions under the null hypothesis are sometimes poorly approximated by those predicted by theory and can vary heavily according to factors such as the branches considered for positive selection and irregularity of certain parameter estimates.  Moreover, false positives are shown to be common when positive selection has occurred in the tree but not along on the prespecified branches.

\end{abstract}

\printnoidxglossary[title=List of Abbreviations and Symbols Used,nonumberlist]

%\begin{acknowledgements}
%I acknowledge Joe Bielawski and Edward Susko for their remarkable guidance and patience.  In particular, I acknowledge Ed's contribution of Appendix I.
%\end{acknowledgements}

\mainmatter

\chapter{Introduction}
Science must often be conducted without direct observation of that which we wish to explain.  The electron is hidden from the chemist, the cosmologist formulates theories about the genesis of the universe based clues that are billions of years old, and so on.  Likewise, as the evolution of the heritable characteristics of biological populations has occurred over billions of years and continues at a pace that spans generations, its direct observation is typically not possible.  Whether it be by studying the fossil record or comparing the physiology of extant populations, when studying evolution, one must also use inference techniques to understand the processes that gave rise to biological populations.  Molecular evolution is a subdiscipline of evolutionary biology in which statistical models and computational algorithms are used to make inferences about evolutionary processes.  The field is termed \emph{molecular} evolution because the questions are related to organic molecules such as proteins and amino acids using the molecules that store the genetic information for all life, nucleic acids.  The topic of this thesis is molecular protein evolution and the aims are twofold.  First, I aim to describe strengths and limitations of some commonly used models of molecular evolution.  Second, I build upon some of these models in order to improve the reliability of detection of positive selection at the level of proteins and amino acids.

Access to the data that underpins the field, genetic sequence data, became available with sequencing techniques developed in the 1970s \citep{gilbert1973nucleotide,sanger1977dna} and culminated in the Human Genome Project \citep{watson1990human}.  The project to sequence the entire human genome and to identify all its genes was completed in 2003 after 13 years and a multi-billion dollar budget \citep{hood2013human}.  Today, faster and cheaper sequencing techniques put whole genome sequencing within reach of small laboratories and there is an abundance of genetic data to study \citep{goodwin2016coming}.  With more data and increasingly sophisticated models, the computational demands to model molecular evolution have increased along with the data.  These demands are being met by exponential growth in computing power that has lasted for over half a century \citep{mack2011fifty}.  With an abundance of new data and access to powerful tools, it is an exciting time to study the evolutionary history of our world's remarkable biological diversity.  Along with the excitement, many also feel a sense of urgency to study the history of life \citep[e.g.,][]{forest2015phylogeny}.  The urgency is due to human activities that have altered the land, oceans, and atmosphere so profoundly that life is being re-ordered in ways not seen for millions of years \citep{lewis2015defining}.  Our extraction of resources, direct harvesting of species, fragmentation of habitats, introduction of non-native species, and spreading of pathogens have possibly hastened the sixth mass extinction \citep{barnosky2011has} and led to proposals for a new human-induced geochronological epoch called the Anthropocene \citep{crutzen2006anthropocene}.

\section{Beyond The Modern Synthesis}
In the mid-nineteenth century, foundational ideas were described about biological inheritance and the evolution of the heritable characteristics in biological populations.  Darwin presented compelling evidence of adaptive evolution, i.e., heritable traits which increase reproductive success become more common in populations.  Through his experiments with pea plants, Mendel showed that phenotype can be determined by the inheritance of discrete trait units, which we now know are variants of genes called alleles.  It wasn't until a half century later when these ideas were reconciled in what has been referred to as the \emph{Modern Synthesis} \citep{huxley1942evolution}.  Key to the development of the \emph{Modern Synthesis} was the new field of population genetics.   The development and application of statistical models of evolution to biological population data helped to determine the forces that drive changes in the allele frequencies in biological populations, i.e., evolution: mutation, gene flow, non-random mating, stochastic factors due to finite population size, and adaptive evolution  \citep{fisher1923xxi, fisher1931xvii, haldane1927mathematical, wright1931evolution, wright1942statistical, kimura1957some, kimura1962probability}.  In this thesis, the primary focus is on the stochastic forces called random genetic drift and adaptive evolution.  More specifically, the aim is to detect evidence of adaptive evolution in a background of random genetic drift.

\section{A Brief Introduction to Molecular Evolution}

\subsection{Biochemical Fundamentals}
Proteins are organic macromolecules that are a fundamental component of life.  They participate in nearly all cellular processes from catalyzing chemical reactions to transporting molecules.  They effect muscle contraction, form various support structures, and can act as toxins.  All proteins are composed of one or more long, linear chains containing different forms of a molecular unit or monomer called an amino acid.  The ordering of amino acids in a chain determines how a protein folds into a functioning three-dimensional structure \citep{anfinsen1972}.

The information about the precise order of a protein's amino acids is stored in another class of biological macromolecule called nucleic acids.  Both types of nucleic acids involved in protein synthesis, \gls{dna} and \gls{rna}, are composed of chains of monomers called nucleotides.  Each nucleotide contains a 5-carbon sugar, a phostphate group, and a nitrogen-containing base.  Linear chains of nucleotides are formed via bonds between the phostphate of one nucleotide and the sugar of another to form a sugar-phosphate backbone.  The information stored in nucleic acids is encoded by the ordering of four different forms of nucleotides in the chain with each form having a different nitrogen-containing base.  \gls{rna} molecules are composed of single chains of nucleotides containing four different bases: adenine (A), uracil (U), guanine (G), or cytosine (C).  \gls{dna} molecules are composed of two nucleotide chains that are bonded via specific nucleotide base pairings, C with G and A with thymine (T), the \gls{dna} analog of \gls{rna}'s U.  The two linked nucleotide chains of \gls{dna} form the well known double-helix structure.

The central dogma of molecular biology originally conveyed the idea that once the protein-building information contained in nucleic acids was transferred to a protein, that information could no longer be recovered from the protein.  Nowadays, the central dogma often refers to a more detailed flow of the information, i.e., organisms replicate \gls{dna}, transcribe \gls{dna} to \gls{rna}, and translate \gls{rna} to protein.  During transcription, it is the protein-encoding unit of \gls{dna}, the gene, that is used as a template to synthesize single-stranded \gls{rna} called \gls{mrna}.  During translation, three-nucleotide sequences within the \gls{mrna} called codons are bound to by the complementary anti-codon of \gls{trna}.  The \gls{trna} molecules continue to bind to codons along the length of \gls{mrna}, each time carrying a particular amino acid to transfer to an elongating polypeptide chain that will become a protein. %{\color{red}Figure here}

\subsection{The Genetic Code}
The genetic code refers to the mapping of codons to amino acids during protein synthesis.  The code (table \ref{tab:code}) was believed to be universal in that the same codon to amino acid mappings were always employed by all organisms, however some exceptions have been discovered.  For example, there are some nonstandard codons in vertebrate mitochondrial \gls{dna}, bacteria, and the nuclear genes of portozoans.  Aside from the codon to amino acid mappings there are two types of special codons.  In the standard genetic code, the codon ATG is referred to as a start codon, because it signals cellular machinery to start reading a gene for translation and also to begin the polypedite chain with the amino acid Methionine.  No \gls{trna} molecules have anti-codons for three codons, TAA, TAG, and TGA.  These three codons are called stop codons, because they signal the end of the polypeptide chain.

\input{./genetic_code.tex}

\subsection{Mutation, Fixation, and Selection}
Suppose all individuals in a population carry the same version of a gene, wild-type allele \emph{A}, when a heritable error occurs in the gene of one individual introducing mutant allele \emph{a} into the population.  It is generally rare for errors called mutations to occur in genes \citep{nachman2000estimate}, however when heritable mutations do occur, they provide the ultimate source of variation for evolution.  Mutations occur in different forms such as insertions, deletions, or substitutions of single nucleotides to whole chromosomes.  However, for the models discussed here, only single-nucleotide changes are considered such as the one shown in table \ref{tab:bglobin} where a nucleotide substitution changes codon GAG in wild-type allele \emph{A} to GTC to create mutant allele \emph{a}.  Because GAG and GTC encode different amino acids, Glutamate and Valine in the universal genetic code, such a nucleotide substitution will cause a change in the protein.
\input{./beta_globin.tex}

In the absence of mechanisms that direct evolution such as selection, the allele composition of the next generations can be considered the result of random sampling of alleles from the current generation.  Thus, whether the frequency of mutant allele \emph{a} in the population eventually goes to $0$ (elimination) or to $1$ (fixation) is determined by random genetic drift, i.e., chance.  Selection is one mechanism of evolution that causes sampling of alleles for subsequent generations to be non-random.  For example, when individuals carrying mutant allele \emph{a} have reduced fitness to pass on the allele, relative to carriers of \emph{A}, purifying selection acts to reduce the frequency of \emph{a} in the population.  When individuals carrying \emph{a} have increased fitness, positive selection makes fixation of \emph{a} more probable.

The time between the occurrence of a new mutation and its fixation can vary, but it is generally very small relative to the overall time considered in the models of evolution discussed here, i.e., only evolution over longer time scales (macro-evolution) is considered.    Thus, polymorphisms, two or more concurrently existing alleles in a population, are ignored and the genetic data for a taxon is considered representative of the population.

\section{Modelling Molecular Evolution}

\subsection{The Data}

The data for the models of evolution considered in this thesis are nucleotide sequences from protein coding genes and a bifurcating phylogenetic tree.  The order of the nucleotides for some number of homologous taxonomic units (taxa) are either obtained directly using genetic sequencing technology or indirectly from a genetic database such as GenBank \citep{benson2012genbank}.  The sequences are arranged so that the data for each taxon is a row in a data matrix, $\bm{X}$.  A goal of the alignment is to arrange homologous characters, either nucleotides or codons, in the columns of $\bm{X}$.  With short and highly conserved sequences this is a straightforward task, but software-implemented alignment algorithms \citep[e.g.,][]{altschul1990basic, buchfink2021sensitive} are usually required because different accumulated mutations in each of the sequences, such as deleted or inserted nucleotides, make manual alignment impractical.  For all models considered in this thesis, the aligned data at each site $\bm{x}_h$, a column in $\bm{X}$, is assumed to be an independent observational unit.  An example four-taxon alignment is shown in table \ref{tab:sampalign}.

Phylogenetic trees, such as those shown in figure \ref{fig:sampletrees}, are structures that represent the inferred evolutionary relationships among taxa.  Their two components are nodes and branches.  Each taxon is represented by a node, labelled $0$ to $6$ in figure \ref{fig:sampletrees}, and each evolutionary path between taxa is represented by a branch.  The branch lengths, labelled $t_1$ through $t_6$ in figure \ref{fig:sampletrees}, typically represent the genetic distance between adjoining taxa.  Rooted trees have a unique internal node called the root node, which is interpreted as the common ancestor of all other nodes.  The root node is labelled $0$ in subfigure \ref{sfig:samptreerooted} and is absent from the unrooted tree in subfigure \ref{sfig:samptreeunrooted} as unrooted trees do not have a root node and do not define the direction of evolution.  When the tree is bifurcating, all other internal nodes have two branches that each connect to a descendant node and a third branch that connects to an ancestral node.  Sequence data in the rows of $\bm{X}$ are only observed for external nodes (also referred to as tip or leaf nodes).  For the models considered in this thesis, evolution along a branch of the tree is assumed to independent of evolution along any other branch, conditional upon some value for any unknown states at the ends of the branch.

With the assumptions of independence across both sites and branches of the tree, the unit of evolution is simplified to substitution between states along a branch.  In addition, a property called time reversibility means, roughly, that that the probability of the data at a site is equal regardless whether either end of the branch is considered the ancestor.  Time reversibility will be discussed in further detail below.

\input{align.tex}

<<sepiatree,echo=F,warning=F>>=
example.tre <- read.tree(text="(((4:0.32,3:0.53)6:0.12,2:0.47)5:0.14,1:0.61)0;")
@

\begin{figure}[h]
  \centering
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    <<sepiatreerooted,echo=F,warning=F>>=
    plot(example.tre,show.node.label=T,show.tip.label=F,no.margin=T,cex=2)
    nodelabels("(Root)"    ,5,frame='n',cex=2,adj=c(-.35,0.5))
    nodelabels("1 ATGTTA...",4,frame='n',cex=2,adj=c(1,1.4))
    nodelabels("4 ATGTTA...",1,frame='n',cex=2,adj=c(1,-.4))
    nodelabels("3 ATGTTT...",2,frame='n',cex=2,c(1,-.4))
    nodelabels("2 ATGCTA...",3,frame='n',cex=2,adj=c(1,-.4))

    edgelabels(c(expression(t[5]),expression(t[6]),expression(t[4]),expression(t[3]),expression(t[2]),expression(t[1])),adj=c(0,1),bg='white',cex=2,col='black',frame="none")
    @
    \caption{Rooted tree}
    \label{sfig:samptreerooted}
  \end{subfigure}
  \begin{subfigure}[t]{.49\textwidth}
    \centering
    <<sepiatreeunrooted,echo=F,warning=F>>=
    plot(example.tre,type='unrooted',show.tip.label=F,show.node.label=F,no.margin=T,cex=2)
    nodelabels("4 ATGTTA...",1,frame='n',cex=2,adj=c(.9,1.05))
    nodelabels("3 ATGTTT...",2,frame='n',cex=2,adj=c(1.1,0.8))
    nodelabels("2 ATGCTA...",3,frame='n',cex=2,adj=c(.7,-.4))
    nodelabels("1 ATGTTA...",4,frame='n',cex=2,adj=c(-.2,.5))
    nodelabels("5",6,frame='n',cex=2,adj=c(.4,-.8))
    nodelabels("6",7,frame='n',cex=2,adj=c(.7,1.6))
    edgelabels(c(expression('','t'[6]),expression(t[4]),expression(t[3]),expression(t[2])),adj=2,bg='white',cex=2,col='black',frame="none")
    edgelabels(expression(t[1]+t[5]),6,adj=c(1,1),bg='white',cex=2,col='black',frame="none")
    @
    \caption{Unrooted tree}
    \label{sfig:samptreeunrooted}
  \end{subfigure}
  \caption[Sample Phylogenetic Trees]{A four-taxon, rooted tree (a) and the corresponding unrooted tree (b).  The rooted tree has 6 branches with lengths labelled $t_1$ to $t_6$, 3 internal nodes labelled 0, 5, and 6 and 4 external nodes labelled 1, 2, 3, and 4.  Node 0 is the root node, which is interpreted as the common ancestor all other nodes.  The unrooted tree has no root node.  Partial sequence data is shown for the extant taxa at the external nodes.}
  \label{fig:sampletrees}
\end{figure}

\subsection{Modelling Substitution as a Markov Process}
A stochastic process is a collection of random variables that are indexed by a set $T$, which often represents time.   If $X(t)=i$, the process $X$ is said to be in state $i$ at time $t$.  For the values $i$ and $j$ from some finite set of states, all $t \ge 0$, and all $s \ge 0$, if
\begin{equation}
  P[X(t+s) = j | X(s)=i, X(u) = x(u), 0 \le u < s]  = P[X(t+s) = j | X(s)=i]
  \label{eq:Markov}
\end{equation}
holds, then the stochastic process is a continuous-time Markov process and equation (\ref{eq:Markov}) is referred to as the Markov property, i.e., the conditional distribution of future states given present and past states depends only on the present state.  This makes Markov processes well suited for modelling nucleotide substitution as only the present nucleotides states are observable.  If equation (\ref{eq:Markov}) is independent of $s$, the Markov process is said to be time-homogeneous and the probability of transitioning from state $i$ to $j$ in time $t$ can be expressed as $p_{ij}(t)$.
Because transition probabilities satisfy the Chapman-Kolmogorov theorem,
\begin{equation}
  p_{ij}(t_1 + t_2) = \sum_kp_{ik}(t_1)p_{kj}(t_2),
  \label{eq:Chapman-Kolmogorov}
\end{equation}
the probability of transitioning from state $i$ to state $j$ in time $t_1+t_2$ is equal to the probably of first transitioning to any intermediate state in time $t_1$ before transitioning to state $j$ in time $t_2$.  Thus, using a Markov process to estimate, e.g., genetic divergence accounts for unobserved nucleotide transitions.% unlike using the proportion of different nucleotides.

The \gls{jc69} uses a continuous time Markov process to model nucleotide substitution \citep{JukesCantor1969}.  The model is useful for understanding properties that are shared with other models of molecular evolution, including more sophisticated models that will be presented below.  It assumes that any nucleotide, $i$, has the same instantaneous rate, $\lambda$, of transitioning to any other nucleotide state, $j$.  The rate matrix for the \gls{jc69} model is
\begin{equation}
Q = \{q_{ij}\} = \bordermatrix{~      & T          & C         & A          & G        \cr
                                    T & -3\lambda  & \lambda   & \lambda   & \lambda   \cr
                                    C & \lambda    & -3\lambda & \lambda   & \lambda   \cr
                                    A & \lambda    & \lambda   & -3\lambda & \lambda   \cr
                                    G & \lambda    & \lambda   & \lambda   & -3\lambda \cr}.
\end{equation}
The probability of transitioning from nucleotide state $i$ to state $j$ within some small time interval $h$ is $p_{ij}(h) = \lambda h + o(h)$ with $o(h)$ representing some function $g(h)$ such that $g(h)/h \rightarrow 0$ as $h \rightarrow 0$.  The probably of not transitioning away from state $i$ is $p_{ii}(h) = 1 - \sum_{j \ne i} p_{ij}(h) = 1 - 3\lambda h + o(h)$, so the transition probabilities within the small time interval $h$ can be expressed as
\begin{equation}
  \label{eq:Ph}
  P(h) = \bordermatrix{~
    & T               & C            & A            & G           \cr
    T & 1-3\lambda h  & \lambda h    & \lambda h    & \lambda h   \cr
    C & \lambda h     & 1-3\lambda h & \lambda h    & \lambda h   \cr
    A & \lambda h     & \lambda h    & 1-3\lambda h & \lambda h   \cr
    G & \lambda h     & \lambda h    & \lambda h    & 1-3\lambda h \cr}
  + o(h) = I + Qh + o(h).
\end{equation}
When $h$ is 0, the current nucleotide state can not change and the transition probability matrix is the identity matrix.

The transition probability matrix $P(t)$, the probability of transitioning from nucleotide $i$ to $j$ in any time $t>0$, can be obtained by directly solving the Kolmorgorov backward equation.
\begin{align}
  P(h+t) &= P(h)P(t) \nonumber \\
         &= [I + Qh + o(h)]P(t) \nonumber \\
  P(t+h)-P(t) &= QhP(t) + o(h)P(t) \nonumber \\
  [P(t+h)-P(t)]/h &= QP(t) + o(h)P(t)/h \nonumber \\
  P'(t) &= QP(t)  \label{eq:kb}  \hskip 100pt \text{(as $h \rightarrow 0$)}
\end{align}
A more general solution involves exponentiation of $Qt$.  Using the Chapman-Kolmogorov theorem, the transition probabilities can be expressed as
\begin{align*}
  P_t &= P_{t\frac{n-1}{n}}P_{t\frac{1}{n}} \\
      &= \left(P_{t\frac{n-2}{n}}P_{t\frac{1}{n}}\right)P_{t\frac{1}{n}} = P_{t\frac{n-2}{n}}\left(P_{t\frac{1}{n}}\right)^2 \\
      &= \left(P_{t\frac{n-3}{n}}P_{t\frac{1}{n}}\right)\left(P_{t\frac{1}{n}}\right)^2 =  P_{t\frac{n-3}{n}}\left(P_{t\frac{1}{n}}\right)^3 \\
      &\vdotswithin{ = } \\
      &= \left(P_{t/n}\right)^n,
\end{align*}
where the argument is indicated as a subscript for clarity.  When $n$ is large, the time interval $t/n$ is small, and from equation (\ref{eq:Ph}), the transition probabilities are $P(t) = lim_{n \to \infty}[I + Qt/n + o(t/n)]^n$.  Because $t$ is fixed, $o(t/n) = o(1/n)$, and $P(t)$ can be expressed as $\sum_{k=0}^{\infty}(Qt)^k/k!$, a Taylor series expansion of an exponential, and thus
\begin{equation}
  P(t) = e^{Qt}. \label{eq:Pt}
\end{equation}

A common approach for solving equation \ref{eq:Pt} is by eigen decomposition.  An $N \times N$ matrix, $A$, with $N$ linearly independent eigenvectors can be expressed in its eigen decomposition, $A = U \Lambda U^{-1}$, where $U$ is the $N \times N$ matrix whose $i^{th}$ column is the eigenvector $u_i$ of $A$ and $\Lambda$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues.  Note that $A^2=(U \Lambda U^{-1})(U \Lambda U^{-1}) = U \Lambda (U^{-1}U) \Lambda U^{-1} = U \Lambda^2 U^{-1}$ and in general $A^n = U \Lambda^n U^{-1}$.  The eigen decomposition of the rate matrix, $Q = U \Lambda U^{-1}$, allows $P(t)$ to be obtained by exponentiating the diagonal matrix entries.
\begin{equation}
  \label{eq:PtEig}
  \begin{split}
    P(t) &= \sum_{k=0}^{\infty}\frac{(Qt)^k}{k!}\\
    &= \sum_{k=0}^{\infty}\frac{(U \Lambda U^{-1}t)^k}{k!}\\
    &= U \left[\sum_{k=0}^{\infty}\frac{(\Lambda t)^k}{k!}\right] U^{-1}\\
    &= U e^{\Lambda t} U^{-1} \\
    &= U \text{diag}\{e^{\lambda_1 t}, e^{\lambda_2 t}, \dots , e^{\lambda_c t}\} U^{-1}
  \end{split}
\end{equation}
The solution of $P(t)$ for the JC69 model is
% See page 12 of Yang's book for the diagonalization.
\begin{equation}
P(t) = \bordermatrix{~  & T         & C           & A           & G    \cr
               T & p_{0}(t)  & p_{1}(t)   & p_{1}(t)   & p_{1}(t)  \cr
  C & p_{1}(t)    & p_{0}(t) & p_{1}(t)   & p_{1}(t)  \cr
  A & p_{1}(t)    & p_{1}(t)   & p_{0}(t) & p_{1}(t)  \cr
  G & p_{1}(t)    & p_{1}(t)   & p_{1}(t)   & p_{0}(t) \cr}
\text{, with}\left\{
\begin{array}{l l}
  p_{0}(t) = \frac{1}{4}+\frac{3}{4}e^{-4\lambda t}\\
  p_{1}(t) = \frac{1}{4}-\frac{1}{4}e^{-4\lambda t}
\end{array} \right.
%\caption{The transition probability matrix for the Jukes-Cantor model.}
\label{eq:JC69_Pmat}
\end{equation}

Each row of the transition-probability matrix is a probability distribution, and thus sums to $1$.  When $t=0$ the transition-probability matrix is the identity matrix, i.e., over time $t=0$ the current nucleotide state can not change.  The limiting distribution when $\lim_{t \rightarrow \infty} p_{ij}(t) = \pi_j$ represents the probability that the process is in state $j$ after infinite time.  For the JC69 model, the limiting probabilities, $\bm{\pi} = (\pi_T,\pi_C,\pi_A,\pi_G)$ are $(1/4,1/4,1/4,1/4)$, i.e., when enough time has passed and so many substitutions have occurred, the probably of observing any nucleotide at a site is equal, regardless of the nucleotide state at time $t=0$.  When the vector of states, $\bm{\pi}$, satisfies the $\bm{\pi} = \bm{\pi} P(t)$ for all $t \ge 0$, $\bm{\pi}$ is referred to as the stationary distribution and if such a distribution of a Markov process exists, it is unique.  The stationary distribution for the JC69 model is the limiting distribution, $\bm{\pi} = (\pi_T,\pi_C,\pi_A,\pi_G) = (1/4,1/4,1/4,1/4)$.  Another notable property of the \gls{jc69} model is that time, $t$ and rate, $\lambda$ are present as a product, and thus only distance, the expected number of nucleotide substitutions per nucleotide site, $d = \lambda t$, can be estimated.

Most conventional models of evolution are time reversible.  A Markov process is said to be time reversible if the long-run rate of transitioning from state $i$ to state $j$ is equal to the long-run rate of transitioning from state $j$ to state $i$, i.e., $\pi_ip_{ij}(t) = \pi_jp_{ji}(t)$, for all states, $i$ and $j$.  There are two useful implications for time-reversibility with respect to models of evolution.  First, a time-reversible Markov process is guaranteed to have real eigen values, so the transition probability matrix can be obtained using equation (\ref{eq:PtEig}).  The second implication relates to site probabilities and the evolutionary relationship of the sequences.  Given two sequences, the site probabilities are equivalent when either sequence is the ancestor, or if both sequences are descendants of some ancestral sequence.  With more than two sequences and their evolutionary relationship described by a phylogenetic tree, time-reversibility means the probabilities of the data do not depend on the root node.

\subsection{An Excess of Nonsynonymous Codon Substitutions is a Signature for Positive Selection}
The \gls{jc69} and related models \citep[e.g.,][]{kimura1980simple, tamura1993estimation} describe substitutions between nucleotides using a Markov process.  Other models of molecular evolution describe substitutions between either amino acids or codons.  Those models of amino acid or codon substitution follow the same Markov theory described previously, but the state space of their Markov processes are either the 20 amino acids in proteins or the 61 sense codons of the universal genetic code.  Models of codon evolution are often more powerful for detecting positive selection, because they can harness additional information contained in the \gls{dna} sequences about whether a codon substitution resulted in a change in the encoding amino acid or not.

With four possible nucleotides in each of the three positions in a codon, there are $4^3=64$ different codons.  Of these $64$ codons, the $61$ codons that code for amino acids are referred to as sense codons.  There are 20 different amino acids commonly found in proteins, thus the genetic code is redundant.  In the standard code, only tryptophan and methionine are encoded by single codons and all other amino acids are encoded by two, four, or six different codons (table \ref{tab:code}).  Because of this redundancy in the genetic code, a nucleotide substitution within a codon can either result in a change in the amino acid (nonsynonymous substitution) or no change in the amino acid (synonymous substitution).

Under the assumption that natural selection acts only on proteins, all synonymous substitutions must be selectively neutral and will be fixed in the population by chance alone.  By contrast, because nonsynonymous substitutions cause changes to proteins, selection may affect their fixation.  Thus, a comparison of nonsynonymous and synonymous substitutions can be used to detect selection.  Consider two models, an alternative model that permits selection and a null model that is nested within the alternative model and does not permit selection.  Under both the alternative and null models, the ratio of the long-run proportions of nonsynonymous and synonymous substitutions, $p_N/p_S$ and $p_N^0/p_S^0$, can be determined.  The ratio of these ratios, $(p_N/p_S)/(p_N^0/p_S^0)$ over a fixed period of time, often referred to as $d_N/d_S$ or \gls{w}, can be used to infer the strength and direction of selection.  If under the alternative model $\omega$ is less than it would be under a comparable null model, this is a signature of purifying selection.  If under the alternative model $\omega$ is larger than it would be under a comparable null model, this is a signature of positive selection.  This is a fundamental concept considered throughout this thesis.

\subsection{A Model of Codon Evolution}

The model of codon evolution described in \citep{nielsen1998likelihood} defines the relative, instantaneous substitution rate between codon $i$ and $j$ $(i \neq j)$ at site $h$ as
\begin{equation}
  Q_{ij} \propto \left\{
    \begin{array}{l l}
      0, &  \mbox{if \emph{i} and \emph{j} differ at two or three codon positions,}\\
      \pi_j, &  \mbox{if \emph{i} and \emph{j} differ by a synonymous transversion,}\\
      \kappa\pi_j, &  \mbox{if \emph{i} and \emph{j} differ by a synonymous transition,}\\
      \omega\pi_j, &  \mbox{if \emph{i} and \emph{j} differ by a nonsynonymous transversion,}\\
      \omega\kappa\pi_j, &  \mbox{if \emph{i} and \emph{j} differ by a nonsynonymous transition}
    \end{array} \right.
  \label{eq:cm}
\end{equation}
where $Q$ is the rate matrix of a continuous-time, stationary, time-reversible Markov process.  The $\pi_j$ parameters are the stationary frequencies of codon $j$, which can be estimated using different methods:
\begin{itemize}
\item Fequal: All sense codons have the frequency $1/61$.
\item F61: Each codon frequency is a parameter with the constraint that all frequencies sum to $1$ (60 free parameters)
\item F1x4: Use nucleotide frequencies such that the frequency of, e.g., codon ACG is $\pi_{ACG} = (1/C)\pi^*_A\pi^*_C\pi^*_G$ where $C$ is a scale factor to ensure the frequencies sum to $1$ and $\pi^*_A$, $\pi^*_C$, $\pi^*_G$, and $\pi^*_T = 1 - \pi^*_A -  \pi^*_C - \pi^*_G$ are the nucleotide frequencies (3 free parameters)
\item F3x4: Use separate nucleotide frequencies for each of the three codon position (9 free parameters).
\end{itemize}
Nucleotides can be categorized into two different groups based on whether their nitrogen-containing base has one or two rings.  The pyrimidines, C and T, have a nitrogen-containing base with a single ring, whereas the base of purines, A and G, have two fused rings.  Substitutions within each group, i.e., $C \leftrightarrow T$ or $A \leftrightarrow G$, is referred to as a transition, whereas substitutions between groups, i.e., $C \leftrightarrow A$ or $G \leftrightarrow T$, is called a transverion.  The $\kappa$ parameter, the transition to transversion rate ratio, accounts for the different rates of the two types of substitutions.  The $\omega$ parameter, discussed above, is the key parameter for the inference of sites under selection.

\subsection{Parameter Estimation using Maximum Likelihood}

\Gls{ml} is a widely used method of statistical inference that is used with all models discussed in this thesis.  With data $\bm{X} = (x_1,x_2,\dots,x_n)^T$ sampled from a population with a known distribution, the likelihood function, $L(\theta)$, is proportional to the joint density function of the data, but treated as a function of the unknown parameter, $\theta$.  A goal of \gls{ml} estimation is to determine the parameter value, \(\hat{\theta}\), that maximizes $L(\theta)$, i.e., to determine the parameter value that makes the observed data most probable.  The parameter may be a single value, or a vector of values, $\theta = (\theta_1,\theta_2,\dots,\theta_k)^T$.  The value of $\theta$, $\hat{\theta}$, that maximizes $L(\theta)$ is the \gls{mle} of $\theta$.  Some of the favourable properties of estimation by \gls{ml} include the likelihood principle, efficiency, consistency, and asymptotic normality \citep{kalbfleisch1985probability,bickel2015mathematical}.  % Properties of MLE: transformation invariance, consistency, efficiency (low bound of variance of estimators), convenient distribution of \hat{\theta} ~ AN[\theta, J(hat{\theta}^-1)]

Using ML to estimate, e.g., the distance between two sequences under a JC69 model involves estimation of the single parameter, $d$.  The probability that a site differs between two homologous sequences separated by some distance $d$ is $p = 3p_1 = 3/4 - 3/4 e^{-4d/3}$, where the distance between two sequences separated by time $t$ is $d=3\lambda t$.  The probability of observing $x$ substitutions out of $n$ sites is the binomial probability, $L(d) = {n \choose x} \left(3/4 - 3/4 e^{-4d/3}\right)^x \left(1/4 - 3/4 e^{-4d/3}\right)^{n-x}$.  The estimate of $d$, $\hat{d}$, is the value of $d$ that maximizes $L(d)$.  Setting $L'(d)$ (or equivalently $log[L'(d)]$) to $0$ and solving for $d$ gives $\hat{d} = -\frac{3}{4} log \left(1 -\frac{4}{3} \frac{x}{n}\right)$.  Thus, under a JC69 model, two aligned sequences of $350$ nucleotides that differ at $29$ positions have an \gls{ml} estimated distance of $\hat{d} = 0.0878$.

Likelihood calculation with more than two taxa is an extension of likelihood calculation with a pair of taxa.  The evolutionary history of the taxa is described using a phylogenetic tree whose topology is always considered fixed.  Consider a codon model, such as the one described in equation (\ref{eq:cm}), fitted to an alignment of \gls{dna} sequences with $n$ codon sites.  Denote the codons in the sequences at site $h$ $(h=1, \dots, n)$ as $\bf{x}_h$, the site pattern at site $h$.  Because sites are assumed to evolve independently, the likelihood of the data is the product of site probabilities, $f(\bm{x}_h|\theta)$,
\begin{equation}
  \label{eq:L}
  L(\theta) = \prod_{h=1}^{n}f(\bm{x}_h|\theta).
\end{equation}
It is equivalent and usually more convenient to maximize the log transformation of the likelihood,
\begin{equation}
  \label{eq:ll}
  \ell = \log(L) = \sum_{h=1}^{n}\log\{f(\bm{x}_h|\theta)\}.
\end{equation}

Consider calculation of $f(\bm{x}_h|\theta)$ over tree topology in figure \ref{sfig:samptreerooted}.  To calculate site probabilities, it is necessary to condition on all possible unknown ancestral states.  Under a codon model, this requires summation over all $61$ possible codons for each internal node.  Given the ancestral state, substitution along a branch of the tree is assumed to be independent of the substitution processes along the other branches.  This means calculation of $f(\bm{x}_h|\theta)$ is the product of the substitution probabilities over branches of the tree,
\begin{equation}
  \label{eq:sl}
    f(\bm{x}_h|\theta) = \sum_{x_0}\sum_{x_5}\sum_{x_6}\left[\pi_{x_0}p_{x_0,TTA}(t_1)p_{x_0,x_5}(t_5)p_{x_5,CTA}(t_2)p_{x_5,x_6}(t_6)p_{x_6,TTT}(t_3)p_{x_6,TTA}(t_4)\right].
\end{equation}
Here $x_i$ represents the ancestral state and the $h$ subscript is omitted.  At the root node, the stationary distribution of the substitution process is assumed.  Using equations (\ref{eq:PtEig}) and (\ref{eq:cm}) along with the universal genetic code in table \ref{tab:code}, transitioning from, e.g., GTA at node $5$ to CTA at node $2$ over branch length $t_2$ is given by $p_{GTA,CTA}(t_2) = \pi_{GTA}e^{\omega \pi_{CTA}t_2}$.  The quantity in the exponent is not scaled by $\kappa$ because G is a purine and C is a pyrimidine, but is scaled by $\omega$ because GTA and CTA do not code for the same amino acid, i.e., this codon substitution is a nonsynonymous transversion.

Because the substitution process described in equation (\ref{eq:cm}) is reversible and branch length estimation is unconstrained, the likelihood calculation over the tree is invariant to the placement of the root node \citep[p.~256]{felsenstein2004inferring}.  Consider the tree in figure \ref{sfig:samptreerooted} and a set of model parameters.  If the root node $x_0$ were shifted, the tree topology and branch length parameters would change, but the likelihood of the data will remain unchanged.  When the root node is shifted in either direction until either branch length $t_1$ or $t_5$ is $0$, the tree becomes unrooted.

Algorithms designed to obtain \glspl{mle}, such as $\hat{\theta} = (\hat{\kappa},\hat{\omega},\hat{t_i})$ from the model described in equation (\ref{eq:cm}), explore parameter space by adjusting parameter values and recalculating the likelihood.  The goal is to ascend a metaphorical likelihood landscape to reach the peak where the parameter values maximize the likelihood function.  Each such iteration requires recalculation of the likelihood of the data (equation \ref{eq:ll}) which in turn requires recalculation of the transition probabilities over the tree.  In the four-taxon tree in figure \ref{sfig:samptreerooted}, there are $61^3$ unobserved codon states to sum over.  Increasing the number taxa quickly makes likelihood maximization a computationally expensive task.

To economize on the computation of the likelihood over phylogenetic trees, the pruning algorithm was developed \citep{felsenstein1973maximum}, an application of Horner's method.  Horner's method describes an optimal algorithm for evaluation of polynomials such that a polynomial of degree $n$ is calculated with $n$ multiplications and $n$ additions.  For example, calculation of $a_0$ + $a_1x$ + $a_2x^2$ + $a_3x^3$ as $a_0 + a_1 \cdot x + a_2 \cdot x \cdot x + a_3 \cdot x \cdot x \cdot x$ requires three additions and six multiplications.  Calculation following Horner's method, $a_0 + x \cdot (a_1 + x \cdot (a_2 + a_3 \cdot x))$, requires only three additions and three multiplications.   The number of computations can be similarly reduced in equation \ref{eq:sl} by pushing the summations as far right as possible,
\begin{equation}
  \label{eq:sl}
  \begin{split}
    f(\bm{x}_h|\theta) = \sum_{x_0} \biggl[\pi_{x_0}p_{x_0,TTA}(t_1)\biggl[\sum_{x_5}p_{x_0,x_5}(t_5)p_{x_5,CTA}(t_2)\\ \biggl[\sum_{x_6}p_{x_5,x_6}(t_6)p_{x_6,TTT}(t_3)p_{x_6,TTA}(t_4)\biggr]\biggr]\biggr].
  \end{split}
\end{equation}
For $m$ terminal nodes, the number of computations is exponential in $m$ using a naive calculation as in equation \ref{eq:sl}, but is reduced to linear in $m$ when the pruning algorithm is followed.

Formal hypothesis tests can be conducted using the likelihoods of two competing models, a null model and an alternative model.  The likelihood under the alternative model is found by maximizing over the entire parameter space, whereas constraints are imposed on the null model making it a special case of the alternative model.  For example, a test for positive selection could be formulated from equation (\ref{eq:cm}) with the null model imposing the constraint $\omega=1$ and the alternative model allowing $\omega \ge 1$.  A standard \gls{lr} test calls for twice the difference in log likelihoods between the null and alternative model to be compared to thresholds from a $\chi^2$ distribution with degrees of freedom equal to the difference in the number of parameters, $\Delta_p$, between the models,
\begin{equation}
  2\Delta\ell = 2 [ log(L_a) - log(L_0) ] \sim \chi^2_{\Delta_p},
\end{equation}
where $L_0$ and $L_a$ are the likelihood scores under the null and alternative models.

\subsection{Selection Pressure at Amino Acid Sites}
Because proteins subjected to positive selection must still maintain the capacity to fold into complex structural and functional domains, most amino acid sites in a protein will be subjected to purifying selection pressure or neutral evolution.  This means a single \(\omega\) estimated as an average over all sites of such a protein would rarely be large enough to detect positive selection.  Including a separate $\omega$ parameter for each site would require a large number of taxa, which is often not practical.  An alternative approach is to use site classes that are subject to different levels of selection pressure such as \(\omega<1\), \(\omega=1\), \(\omega>1\) \citep{nielsen1998likelihood}.  The $\omega$ at a site is treated as coming from a probability distribution, with various distributional forms allowed \citep{yang2000codon}.  A null model includes some number of $\omega<=1$ site classes, but does not permit $\omega$ values larger than $1$, i.e., positive selection is not permitted.  The probability of site pattern $\bm{x}_h$ can be expressed as a mixture over choices of $\omega$,
\begin{equation}
  \label{eq:nullmm}
  p(\bm{x}_h|\theta) = \sum_i p_ip(\bm{x}_h|\zeta,\omega_i)
\end{equation}
with $\theta$ denoting all model parameters including branch lengths and $\zeta$ denoting all parameters other than those describing the $\omega$ distribution, namely $p_i$ and $\omega_i$.  The alternative model includes the $\omega$ site classes of the null model and also a site class for $\omega>1$,
\begin{equation}
  \label{eq:altmm}
  p(\bm{x}_h|\theta) = \sum_i p_ip(\bm{x}_h|\zeta,\omega_i) + \left(1-\sum_ip_i\right)p(\bm{x}_h|\zeta,\omega > 1).
\end{equation}

As the null model is nested within the alternative model, a standard \gls{lr} test calls for twice the difference in log likelihoods to be compared to thresholds from a $\chi^2$ distribution with degrees of freedom equal to the difference in the number of parameters between the models.  For the models in equations (\ref{eq:nullmm}) and (\ref{eq:altmm}), one additional $\omega>1$ site class in the alternative model gives two additional parameters, $p_{\omega>1}$ and $\omega>1$, which would suggest the \gls{lr} statistic follows a $\chi_2^2$.  However, because certain regularity conditions of the test are not satisfied, the specific distribution is unknown and in practice either a $\chi_0^2/2 + \chi_1^2/2$ or a $\chi_1^2$ is used.

If the \gls{lr} test for positive selection within a gene is rejected, evidence of positive selection at each site can be gathered.  To calculate the posterior probability that site $h$ evolved under $\omega$ site class $i$, the \gls{neb} approach passes the \glspl{mle} to the Bayes formula,
\begin{equation}
  \label{eq:neb}
  Pr(\omega^{(h)} = \omega_i|\bm{x}_h,\zeta) = p_if(\bm{x}_h|\omega_i,\zeta) / \sum_{j=1}^{k}p_jf(\bm{x}_h|\omega_j,\zeta).
\end{equation}
A large posterior probability that site $h$ evolved under the $\omega>1$ site class is evidence that site $h$ evolved under positive selection.

Because the site posterior probabilities always depend on the fitted values of the model parameters (shape parameters of the distribution, branch lengths, etc.), the reliability of \gls{neb} inference depends on the accuracy of the fitted values. If they have been accurately estimated, as is often the case with large, information-rich datasets, they can simply be treated as known without errors.  However, when the fitted values are subject to large errors, the detection of positive selection according to the posterior probabilities can be negatively impacted and in some cases the false positive rate can be unacceptably high \citep{wong2004accuracy}. \gls{beb}, is used to adjust for uncertainty in the parameters of the $\omega$ distribution by assigning priors to those parameters and using numerical integration to average over the uncertainty.

\section{Thesis Outline}
Standard likelihood theory calls for the \gls{lr} test for positive selection within a gene used by mixture models of codon evolution to compare the \gls{lr} statistic to thresholds determined from a $\chi^2$  distribution with degrees of freedom equal to the difference in the number of parameters between the null and alternative models.  This is not justified due to a lack of statistical regularity \citep{anisimova2001accuracy}, so the \gls{lr} test is often applied with thresholds determined from $\chi^2$ or mixture of $\chi^2$ distributions with degrees of freedom selected to make the test more conservative with the hope that not much power is lost.

Results from Chapter \ref{chap:modl} show that these commonly used thresholds need not yield conservative tests, but instead give larger than expected type I error rates.  Statistical regularity can be restored by using a modified \gls{lr} test.  Theoretical results are provided to prove that, if the number of sites is not too small, the modified \gls{lr} test gives approximately correct type I error probabilities regardless of the parameter settings of the underlying null hypothesis.  Simulations show that modification gives type I error rates closer to those stated without a loss of power.  The simulations also show that parameter estimation for mixture models of codon evolution can be challenging in certain data-generation settings with very different mixing distributions giving nearly identical site pattern distributions unless the number of taxa and tree length are large.  Because mixture models are widely used for a variety of problems in molecular evolution, the challenges and general approaches to solving them presented here are applicable in a broader context.

To mitigate problems with classification of selection pressure at sites when parameter are estimated with large errors, \gls{beb} assigns prior probabilities to some parameters.  However, as implemented, it imposes uniform prior probabilities, which causes it to be overly conservative in some cases.  When standard regularity conditions are not met and parameter estimates are unstable, inference, even under \gls{beb}, can be negatively impacted.

In Chapter \ref{chap:sba}, an alternative to \gls{beb} called \gls{sba} is presented.  \gls{sba} uses bootstrapping of site patterns from an alignment of protein coding \gls{dna} sequences to accommodate the uncertainty in the parameter estimates.  Deriving the correction for parameter uncertainty from the data in hand along with kernel smoothing techniques improves site specific inference of positive selection.  Included is a comparison of \gls{beb} and \gls{sba} by simulation and real data analysis.  Simulation results show that \gls{sba} balances accuracy and power at least as well as \gls{beb}, and when parameter estimates are unstable, the performance gap between \gls{beb} and \gls{sba} can widen in favour of \gls{sba}.  \gls{sba} is applicable to a wide variety of other inference problems in molecular evolution.

\chapter{A Modified Likelihood Approach to Explore and Restore Regularity when Testing for Positive Selection}
\label{chap:modl}
\section{Introduction}
Detecting positive selection within proteins is important for understanding the processes of molecular evolution \citep{nielsen1998likelihood}.  The likelihood methods used in codon-based models developed in \cite{yang2000codon} are among the most widely used to test for positive selection.  An important component of these models is the \gls{lr} test, which is used to test for evidence of positive selection within a gene or as a filter before testing for positive selection at amino acid sites (sites).  Standard likelihood theory gives that, when certain regularity conditions are satisfied, the distribution of an LR statistic under the null hypothesis is that of a chi-square random variable with degrees of freedom equal to the difference between the number of parameters fit under the alternative hypothesis and the number under the null hypothesis.  LR tests of positive selection usually employ two additional parameters under the alternative model, often an $\omega>1$ parameter to quantify the positive selection and another parameter for the proportion of sites evolving under $\omega>1$.  This suggests the LR statistics follows a $\chi_2^2$ null distribution.  However, it has long been recognized that the regularity conditions required for standard likelihood theory are not satisfied for such LR tests of positive selection \citep{anisimova2001accuracy}.

Simulations suggest that a $\chi_2^2$ distribution will give 5\% thresholds for the LR test that are too large \citep{anisimova2001accuracy,wong2004accuracy}.  Drawing upon the non-standard likelihood theory of \cite{self1987asymptotic}, \cite{swanson2003pervasive} indicate that, for model comparison they describe as M8a vs M8, theory supports a 50:50 mixture of a point mass at 0 and a $\chi_1^2$ distribution or, more concisely, a $\chi_0^2/2+\chi_1^2/2$ distribution.  However, \cite{wong2004accuracy} and \cite{anisimova2001accuracy} raised concerns about whether this is the appropriate distribution for comparison.  Nevertheless, the $\chi_0^2/2+\chi_1^2/2$ distribution and, to be more conservative, the $\chi_1^2$ distribution are the most frequently used distributions.  While there have been some simulation studies indicating that the $\chi_1^2$ distribution is indeed conservative in the sense that LR statistics generated under the null tend to be smaller than predicted by a $\chi_1^2$ distribution \citep{anisimova2001accuracy,wong2004accuracy,berlin2005testing}, some of these same studies have found settings where the false positive rates are larger than 5\% \citep{wong2004accuracy,berlin2005testing}.

That the null distribution of the LR statistic is neither $\chi^2$ nor a mixture of $\chi^2$ distributions is a theoretical possibility, even with large samples, because of a regularity condition violation in both standard likelihood theory and the non-standard likelihood theory of Self and Liang.  If the only regularity condition violation were that parameters are on the boundary of the parameter space, the Self and Liang theory would hold and the LR test using the mixture of $\chi^2$ distributions would be conservative.  We describe why a lack of identifiability under the null hypothesis makes it possible that the LR test will be anti-conservative (under the null, LR statistics tend to be larger than is predicted by a $\chi_0^2/2+\chi_1^2/2$ distribution) and we confirm this through simulation.

A dramatic illustration of how a lack of this type of identifiability with mixture models can cause LR tests to be anti-conservative is provided by \cite{hardigan1985} \citep[see also][]{chen2017}.  The setting was a test of $N(0,1)$ against the alternative hypothesis of a mixture of $N(0,1)$ and $N(\theta,1)$.  The testing problem suffers from a similar irregularity problem to the one described here: a lack of identifiability of the full model under the null hypothesis.  When a mixing weight is $0$ any $\theta$ gives the null hypothesis and \cite{hardigan1985} shows that the LR statistic approaches $\infty$ with probability $1$.

To obtain tractable limiting $\chi_0^2/2+\chi_1^2/2$ null distributions, we introduce a modified LR test, which borrows from similar methods in mixture model tests of heterogeneity \citep{chen2001modified}.  The test statistic is obtained as it is for the standard LR test, but with the likelihood replaced by one that penalizes small mass on $\omega>1$ relative to $\omega=1$.  This strategy has been effective under a variety different mixture settings \citep[cf.][and references therein]{chen2001modified,chen2004testing,fu2009modified}.

\section{Theory and Methods}
The base model of \cite{yang2000codon} is a conventional stationary time-reversible Markov model of codon sequence evolution described in \cite{goldman1994codon} with instantaneous rate matrix for transitions from codon $i$ to $j$ given by
\[ Q_{ij}  = \left\{ \begin{array}{ll}
0 & \mbox{if $i$ and $j$ differ at two or three nucleotide positions} \\
\pi_j & \mbox{if $i$ and $j$ differ by one synonymous transversion} \\
\kappa\pi_j& \mbox{if $i$ and $j$ differ by one synonymous transition} \\
\omega\pi_j& \mbox{if $i$ and $j$ differ by one nonsynonymous transversion} \\
\omega\kappa\pi_j& \mbox{if $i$ and $j$ differ by one nonsynonymous transition} \\ \end{array} \right. \]
where $\kappa$ is the transition/transversion parameter, $\pi_j$ is the stationary frequency of codon $j$ and $\omega$ is the parameter quantifying selection pressure as purifying ($\omega<1$), neutral ($\omega=1$) or positive ($\omega>1$).  To model varying selection pressure at sites, the $\omega$ at a site is treated as coming from a probability distribution, referred to here as the mixing distribution, with various distributional forms allowed \citep{yang2000codon}.  The null hypothesis of interest is that there is no positive selection, which corresponds to the distribution of $\omega$ having all of its mass between 0 and 1.  The alternative is that the distribution allows some positive probability of an $\omega >1$.  For example, following the naming conventions of \cite{yang2000codon} and \cite{berlin2005testing}, null model M1a uses a distribution with mass at an $\omega_0<1$ and at $\omega_1=1$.  The corresponding alternative model, M2a, adds an $\omega_2>1$ to the M1a distribution.

For any of the models considered, the probability of a site pattern $x$ can be expressed as a mixture over choices of $\omega$ of the following form
\begin{equation}
\label{eq:mixmod}
p(x;\beta,p_+) = p_0 p(x|\omega < 1; \zeta,\lambda) + (1-p_+)(1-p_0) p(x|1;\zeta) + p_+ (1-p_0) p(x|\omega_+;\zeta).
\end{equation}
Theoretical derivations are simpler with this unconventional parameterization.  Usually the weights on $\omega$ values are parameters.  For instance, model M2a replaces $(1-p_+)(1-p_0)$ and $p_+(1-p_0)$ with $p_1$ and $p_2$.  For both models M1a and M2a there is a single $\omega_0<1$,
so $p(x|\omega<1;\zeta,\lambda)=p(x|\omega_0;\zeta)$.  Here $\zeta$ denotes parameters common to each $\omega$ and includes edge-lengths and substitution model parameters.  The parameter $\omega_+$ is restricted to be at least 1 and the parameters in $\lambda$ are those involved in the mixture model under purifying selection.  For instance, for model M8a, $\lambda$ gives the parameters of the beta distribution.  Let $\psi=(\zeta^T,\lambda^T,p_0)^T$ be the parameters that are common to both null and alternative models.  The \gls{lr} statistic is
\begin{equation}
  \label{eq:lrs}
  2\{l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\hat\psi_H)\}
\end{equation}
where $l$ and $l_H$ denote the log likelihoods under the alternative and null models, and $\hat p_+$, $\hat\omega_+$, $\hat\psi$, and $\hat\psi_H$ denote the \glspl{mle} under the alternative and null hypotheses.

The likelihood theory of \cite{self1987asymptotic} gives appropriate null distributions in a number of cases where usual regularity conditions do not hold, but it does not generally apply to (\ref{eq:lrs}).  This is because there can be multiple parameter values under the alternative hypothesis that give the null model.  If the alternative model allows mass at $\omega=1$, any $\omega_+>1$ and $p_+=0$ gives the null model.  In addition, for null models that allow mass at $\omega=1$, $\omega_+=1$ and any $p_+$ gives the null model.  The M8a vs M8 comparison considered in \cite{swanson2003pervasive} finesses this difficulty by not allowing the alternative model to have mass at both an $\omega=1$ and an $\omega_+>1$.  Because of this restriction, whenever the true null generating model has mass on $\omega=1$, the only alternative model parameterization giving the generating distribution has $\omega_+=1$; $p_+=0$ and $\omega_+>1$ no longer gives the generating model.  The \cite{swanson2003pervasive} approach restores regularity, but may make it more difficult to model settings where the alternative is true but there is also appreciable mass near $\omega=1$.  In what follows, the model is allowed to have mass at $\omega=1$ under both the null and alternative
model, with additional mass at an $\omega_+>1$ under the alternative hypothesis.

The regularity problems for the Self and Liang theory does not arise if $\omega_+>1$ is fixed, in which case the \gls{lr} statistic is
\begin{equation}
  \label{eq:lrs_sl}
  2\{l(\hat p_+(\omega_+),\omega_+,\hat\psi(\omega_+)) - l_H(\hat\psi_H)\}
\end{equation}
where $\hat p_+(\omega_+)$ and $\hat\psi(\omega_+)$ denote the \glspl{mle} of $p_+$ and $\psi$ holding $\omega_+$ fixed.  With $\omega_+$ fixed, the only parameter giving a null model is $p_+=0$.  Because that value is on the boundary of the parameter space, standard chi-square results for the limiting distribution of the \gls{lr} statistic do not apply.  However, case 5 of \cite{self1987asymptotic} gives that the large sample distribution is $\chi_0^2/2 + \chi_1^2/2$.  This allows us to say something about the distribution of the usual \gls{lr} statistic (\ref{eq:lrs}).  Because (\ref{eq:lrs}) can be obtained by maximizing (\ref{eq:lrs_sl}) over $\omega_+\ge 1$, it is sure to be larger than any test statistic (\ref{eq:lrs_sl}) that uses a fixed $\omega$.  Thus, since (\ref{eq:lrs_sl}) has a $\chi_0^2/2 + \chi_1^2/2$ distribution, usual \gls{lr} statistic values (\ref{eq:lrs}) will tend to be larger than values predicted by the $\chi_0^2/2 + \chi_1^2/2$ distribution.  How much larger \gls{lr} statistic values tend to be depends upon how much (\ref{eq:lrs_sl}) tends to vary over $\omega_+>1$ which in turn likely depends on how much of the mass of the generating distribution is near $\omega=1$.  Thus, using a $\chi_0^2/2+\chi_1^2/2$ distribution to calculate thresholds for the \gls{lr} test can generally be expected to give an anti-conservative test: the null hypothesis is rejected too frequently when it is true.

The main reason that the null distribution of the \gls{lr} statistic is intractable is that $p_+=0$ and any $\omega_+ > 1$ gives the null model.  A similar difficulty arises when testing for mixture structure or heterogeneity in mixture models.  The distribution for the data, $x$, is $\gamma p(x;\theta_1)+(1-\gamma) p(x;\theta_2)$ where $p(x;\theta)$ is a parametric distribution.  A hypothesis of particular interest is that the data corresponds to a single distribution $p(x;\theta)$.  If this is the case, the population might be considered homogeneous when it is otherwise heterogeneous with $\gamma \times 100\%$ of the individuals having parameter $\theta_1$ and the rest having parameter $\theta_2$.  As with tests for positive selection, the reason for a non-standard \gls{lr} statistic distribution in mixture models is that multiple parameter settings correspond to the null hypothesis: (i) $\gamma=0$ and any $\theta_1$ or (ii) $\theta_1=\theta_2$ and any $\gamma$.  To restore simple limiting distributions while maintaining a test statistic similar to the \gls{lr} statistic, \cite{chen2001modified} replace log likelihoods with modified log likelihoods that add a term, $C\log[\gamma(1-\gamma)]$ where $C>0$ is a tuning parameter.  Because this term gets very large in magnitude but negative when $\gamma$ is close to 0 or 1, the modified log likelihood is maximized by values with $\gamma$ away from these boundaries, implying that the only way modified \glspl{mle} under the null can approach true values is if $\hat \theta_1 \approx \hat \theta_2$, which restores the sort of regularity needed for chi-square or mixture of chi-square distributions.  The strategy has been effective in a number of different settings \citep[cf.][and references therein]{chen2001modified,chen2004testing,fu2009modified} and a similar approach here is presented here.

The modified log likelihood under the alternative hypothesis is
\begin{equation}
  \label{eq:modlnL}
  \tilde l(p_+,\omega_+,\psi) = l(p_+,\omega_+,\psi) + C \log(p_+)
\end{equation}
The modified \gls{lr} statistic is then
\begin{equation}
  \label{eq:modlrs}
  2\{\tilde l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\hat\psi_H)\}
\end{equation}
where now the estimates denote the maximizers of the modified log likelihood.
Shown in Appendix I (section \ref{sec:modlProof}) is that for $C>0$ the large sample distribution of (\ref{eq:modlrs}) under the null hypothesis is $\chi_0^2/2+\chi_1^2/2$.
Here $C>0$ is a tuning parameter.  While the theory holds for any $C>0$, choosing $C$ too small makes the modified \gls{lr} statistic too similar to the \gls{lr} statistic, leading to similar difficulties in behaviour.  The sensitivity to $C$ through simulations is investigated.

Simulation is used to estimate \gls{lr} and modified \gls{lr} statistic \glspl{cdf} under the null hypothesis.  For each of six simulation scenarios, 10,000 sequence alignments $500$ codons long were generated using 5-, 10-, and 32-taxon trees with branch lengths summing to 3, 6, and 9.  The $5$-taxon tree (figure {\ref{sfig:5tree}}) was the same one used in the simulation studies of \cite{wong2004accuracy} and \cite{mingronesba} and the $10$-  and $32$-taxon trees have caterpillar (figure {\ref{sfig:10tree}}) and balanced (figure {\ref{sfig:32tree}}) topologies.  Sites were simulated to evolve under the M1a model \citep[described in][]{wong2004accuracy,yang2005bayes}, which places weight $p_0$ on a single $\omega_0<0$, with the remaining weight, $1-p_0$, placed on $\omega=1$; thus, the mixing distribution is determined by $(p_0,\omega_0)$.  Each simulation scenario used $\kappa=1$ and equal codon frequencies, but $(p_0,\omega_0)$ varied over scenarios.
\begin{figure}
  \centering
  \begin{subfigure}[t]{.49\textwidth}
    \centering
        <<5ttree,echo=F,warning=F>>=
    phylo.5.t <- read.tree(text="((1:0.333333,2:0.333333):0.333333,3:0.666667,4:0.666667,5:0.666667);")
    plot(phylo.5.t,cex=2)
    edgelabels(c('x','x','x','2x','2x','2x'),adj=c(0,1),bg='white',cex=1.5,col='black',frame="none")
        @
\caption{5-taxon tree}
\label{sfig:5tree}
\end{subfigure}
\begin{subfigure}[t]{.49\textwidth}
  \centering
      <<10ttree,echo=F,warning=F>>=
    phylo.10.t <- read.tree(text="(((((1:0.1,2:0.1):0.1,3:0.2):0.1,4:0.3):0.1,5:0.4):0.1,((((6:0.1,7:0.1):0.1,8:0.2):0.1,9:0.3):0.1,10:0.4):0.1);")
    plot(phylo.10.t,cex=2)
      edgelabels(c('x','x','x','x','x','x','2x','3x','4x','x','x','x','x','x','x','2x','3x','4x'),adj=c(0,1),bg='white',cex=1.5,col='black',frame="none")
        @
\caption{10-taxon tree}
\label{sfig:10tree}
\end{subfigure}
\begin{subfigure}[t]{.75\textwidth}
  \centering
      <<32ttree,echo=F,warning=F>>=
    phylo.32.t <- read.tree(text="(((((1:0.04839,2:0.04839):0.04839,(3:0.04839,4:0.04839):0.04839):0.04839,((5:0.04839,6:0.04839):0.04839,(7:0.04839,8:0.04839):0.04839):0.04839):0.04839,(((9:0.04839,10:0.04839):0.04839,(11:0.04839,12:0.04839):0.04839):0.04839,((13:0.04839,14:0.04839):0.04839,(15:0.04839,16:0.04839):0.04839):0.04839):0.04839):0.04839,((((17:0.04839,18:0.04839):0.04839,(19:0.04839,20:0.04839):0.04839):0.04839,((21:0.04839,22:0.04839):0.04839,(23:0.04839,24:0.04839):0.04839):0.04839):0.04839,(((25:0.04839,26:0.04839):0.04839,(27:0.04839,28:0.04839):0.04839):0.04839,((29:0.04839,30:0.04839):0.04839,(31:0.04839,32:0.04839):0.04839):0.04839):0.04839):0.04839);")
    par(mar=c(0,0,2,0))
    plot(phylo.32.t)
        @
\caption{32-taxon tree}
\label{sfig:32tree}
\end{subfigure}
\caption[Phylogenetic tree topologies used in simulation studies]{Phylogenetic tree topologies used in simulation studies, with relative edge lengths shown for the 5- and 10-taxon trees.  All edge lengths are equal in the rooted 32-taxon tree.}
\label{fig:trees}
\end{figure}

To determine the effect of likelihood modification on power, sequence alignments $500$ codons long were simulated under the M2a alternative model \citep{wong2004accuracy,yang2005bayes}, which, by comparison with the M1a mixing distribution, has an additional component, $\omega_2>1$.  The mixing distributions used in simulations had $(p_0,\omega_0)=(0.45,0.5)$ with $(p_2,\omega_2)$ varying over simulation settings.  Codon frequencies were $1/61$ and $\kappa=1$, as was the case for simulations under the null hypothesis and the tree toplogies also matched those used in the simulations under the null.  For each $\omega$-distribution scenario, $10,000$ alignments were generated for the $5$- and $10$-taxon trees and $1000$ alignments for the $32$-taxon tree.  To ensure that comparisons of power with and without likelihood modification corresponded to the same false positive rate, the thresholds for significant \gls{lr} statistics were calibrated.  For this, $10,000$ sequences were generated under the null with the weight on $\omega>1$ under the alternative settings added to $\omega=1$.  The $95$th percentiles of these \gls{lr} statistic distributions under both M1a/M2a (C=$0$) and M1a/M2a (C=$2$) were used as the thresholds for calculating power.

\section{Results and Discussion}
\subsection{Modified LR Distribution Approximations are Accurate for Most Settings}
Figure \ref{fig:CDF32taxaTL9} shows the estimated \gls{lr} and modified \gls{lr} statistic \glspl{cdf} for the M1a/M2a nested model pair for the simulations under the null using the 32-taxon tree with branch lengths summing to $9$.  With likelihood modification, a tuning parameter of $C=2$ was used.  Other tuning parameters were tested, but the \gls{lr} statistic \glspl{cdf} for values of $C$ between $2$ and $5$ were indistinguishable from those with $C=2$, and \glspl{cdf} for values of $C<2$ were always between the one for $C=2$ and the one for the unmodified \gls{lr} statistics.  \glspl{cdf} for $\chi_0^2/2 + \chi_1^2/2$ are also included in each plot.  For all of the \glspl{cdf} in Figure \ref{fig:CDF32taxaTL9}, the modified \gls{lr} statistic distributions are better approximated by a $\chi_0^2/2 + \chi_1^2/2$ distribution than the corresponding distributions without likelihood modification.  Tree topology made little difference as both the \gls{lr} statistic and modified \gls{lr} statistic \glspl{cdf} were similar when data were simulated with different topologies.  Figure \ref{fig:CDF5taxaTL3} and supplementary Figures \ref{fig:CDF5taxaTL6} - \ref{fig:CDF32taxaTL6} contain the \gls{lr} statistic \glspl{cdf} for the remaining simulation scenarios.
\begin{figure}
    \centering
<<CDF32taxaTL9,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.25_w0_0.25_32_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.25_w0_0.25_32_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.25_w0_0.25_32_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.25_w0_0.5_32_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.25_w0_0.5_32_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.25_w0_0.5_32_taxa_tl_9_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.5_w0_0.25_32_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.5_w0_0.25_32_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.5_w0_0.25_32_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.5_w0_0.5_32_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.5_w0_0.5_32_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.5_w0_0.5_32_taxa_tl_9_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.75_w0_0.25_32_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.75_w0_0.25_32_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.75_w0_0.25_32_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.75_w0_0.5_32_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.75_w0_0.5_32_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_9/data/p0_0.75_w0_0.5_32_taxa_tl_9_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_32_taxa_m2a_lnl-p0_0.25_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_32_taxa_c2_lnl-p0_0.25_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_32_taxa_m2a_lnl-p0_0.25_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_32_taxa_c2_lnl-p0_0.25_w0_0.5_32_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_32_taxa_m2a_lnl-p0_0.5_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_32_taxa_c2_lnl-p0_0.5_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_32_taxa_m2a_lnl-p0_0.5_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_32_taxa_c2_lnl-p0_0.5_w0_0.5_32_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_32_taxa_m2a_lnl-p0_0.75_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_32_taxa_c2_lnl-p0_0.75_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_32_taxa_m2a_lnl-p0_0.75_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_32_taxa_c2_lnl-p0_0.75_w0_0.5_32_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_32_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_32_taxa_m2a,lrs_p0_0.25_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_32_taxa_m2a,lrs_p0_0.25_w0_0.5_32_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_32_taxa_m2a,lrs_p0_0.5_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_32_taxa_m2a,lrs_p0_0.5_w0_0.5_32_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_32_taxa_m2a,lrs_p0_0.75_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_32_taxa_m2a,lrs_p0_0.75_w0_0.5_32_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a balanced, 32-taxon tree topology with branch lengths summing to 9.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF32taxaTL9}
\end{figure}
\begin{figure}
    \centering
    <<CDF5taxaTL3,echo=F,warning=F>>=
    p0_0.25_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.25_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.25_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.25_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.25_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.25_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.25_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.5_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.25_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.25_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.25_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.5_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_5_taxa_m2a_lnl-p0_0.25_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_5_taxa_c2_lnl-p0_0.25_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_5_taxa_m2a_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_5_taxa_c2_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_5_taxa_m2a_lnl-p0_0.5_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_5_taxa_c2_lnl-p0_0.5_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_5_taxa_m2a_lnl-p0_0.5_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_5_taxa_c2_lnl-p0_0.5_w0_0.5_5_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_5_taxa_m2a_lnl-p0_0.75_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_5_taxa_c2_lnl-p0_0.75_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_5_taxa_m2a_lnl-p0_0.75_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_5_taxa_c2_lnl-p0_0.75_w0_0.5_5_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_5_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_5_taxa_m2a,lrs_p0_0.25_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_5_taxa_m2a,lrs_p0_0.25_w0_0.5_5_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_5_taxa_m2a,lrs_p0_0.5_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_5_taxa_m2a,lrs_p0_0.5_w0_0.5_5_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_5_taxa_m2a,lrs_p0_0.75_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_5_taxa_m2a,lrs_p0_0.75_w0_0.5_5_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 3.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF5taxaTL3}
\end{figure}

\subsection{False Positive Rates are too Large Without Modified LR Tests}
The false positive rates for each of the \gls{lr} tests of positive selection under nested models M1a/M2a with and without likelihood modification are shown in Table \ref{tab:FPRates}.  The threshold used to reject each \gls{lr} test was determined from the $95$th percentile of the $\chi_0^2/2 + \chi_1^2/2$ distribution.  Thus, when the $\chi_0^2/2 + \chi_1^2/2$ does well to approximate the \gls{lr} statistic distribution, the expected false positive rate is $0.05$.  For each simulation setting under the null hypothesis, the rates were closer to the expected value using the modified likelihood than with the unmodified likelihood.  Excluding the simulation scenario with $5$ taxa and $(p_0,\omega_0)=(0.25,0.5)$ where parameters are almost unidentifiable (discussed below), the false positive rates were between $0.06$ and $0.1$ (average $0.09$) without likelihood modification and between $0.05$ and $0.07$ (average $0.06$) with likelihood modification.  While the false positive rate of the modified \gls{lr} statistic was usually close to $0.05$, there is a small sample bias using sequences of length 500.  Analyzing datasets simulated under the same settings, but with sequences $1500$ codons long confirms this bias.  All but one of the false positive rates that were $0.06$ with sequences 500 codons long dropped to $0.05$ with sequences $1500$ codons long and the false positive rate for the simulation setting with $(p_0,\omega_0)=(0.25,0.5)$ dropped to $0.06$ with the longer sequences.
\begin{sidewaystable}
  \begin{threeparttable}
    \caption{False positive rates.}
    \centering
    \begin{tabular}[h!]{*{2}l*{18}c}
      \toprule
      &  & \multicolumn{18}{c}{Tree Length 3} \\
      \cmidrule(lr){3-20}
      &  & \multicolumn{6}{c}{5 taxa} & \multicolumn{6}{c}{10 taxa} & \multicolumn{6}{c}{32 taxa} \\
      \cmidrule(lr){3-8} \cmidrule(lr){9-14} \cmidrule(lr){15-20}
      &  & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} \\
      \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20}
      Model                & $p_0=$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ \\
      \cmidrule(lr){1-1} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12} \cmidrule(lr){13-13} \cmidrule(lr){14-14} \cmidrule(lr){15-15} \cmidrule(lr){16-16} \cmidrule(lr){17-17} \cmidrule(lr){18-18} \cmidrule(lr){19-19} \cmidrule(lr){20-20}
      M2a (C=0)          &        & .10   & .09  & .09   & .10   & .08  & .07   & .09   & .09  & .08   & .09   & .08  & .06   & .09   & .08  & .08   & .08   & .08  & .07 \\
      M2a (C=2)          &        & .06   & .06  & .05   & .08   & .06  & .06   & .06   & .06  & .05   & .07   & .06  & .05   & .06   & .06  & .05   & .07   & .06  & .06 \\
      \rule{0pt}{4ex}
      &  & \multicolumn{18}{c}{Tree Length 6} \\
      \cmidrule(lr){3-20}
      &  & \multicolumn{6}{c}{5 taxa} & \multicolumn{6}{c}{10 taxa} & \multicolumn{6}{c}{32 taxa} \\
      \cmidrule(lr){3-8} \cmidrule(lr){9-14} \cmidrule(lr){15-20}
      &  & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} \\
      \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20}
                         & $p_0=$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ \\
      \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12} \cmidrule(lr){13-13} \cmidrule(lr){14-14} \cmidrule(lr){15-15} \cmidrule(lr){16-16} \cmidrule(lr){17-17} \cmidrule(lr){18-18} \cmidrule(lr){19-19} \cmidrule(lr){20-20}
      M2a (C=0)          &        & .10   & .09  & .08   & .10   & .09  & .08   & .10   & .09  & .08   & .10   & .08  & .07   & .09   & .10  & .09   & .09   & .09  & .07 \\
      M2a (C=2)          &        & .06   & .06  & .05   & .08   & .06  & .07   & .06   & .05  & .05   & .07   & .06  & .05   & .05   & .06  & .06   & .07   & .07  & .05 \\
      \rule{0pt}{4ex}
      &  & \multicolumn{18}{c}{Tree Length 9} \\
      \cmidrule(lr){3-20}
      &  & \multicolumn{6}{c}{5 taxa} & \multicolumn{6}{c}{10 taxa} & \multicolumn{6}{c}{32 taxa} \\
      \cmidrule(lr){3-8} \cmidrule(lr){9-14} \cmidrule(lr){15-20}
      &  & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} & \multicolumn{3}{c}{$\omega_0=.25$} & \multicolumn{3}{c}{$\omega_0=.5$} \\
      \cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14} \cmidrule(lr){15-17} \cmidrule(lr){18-20}
                         & $p_0=$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ & $.25$ & $.5$ & $.75$ \\
      \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10} \cmidrule(lr){11-11} \cmidrule(lr){12-12} \cmidrule(lr){13-13} \cmidrule(lr){14-14} \cmidrule(lr){15-15} \cmidrule(lr){16-16} \cmidrule(lr){17-17} \cmidrule(lr){18-18} \cmidrule(lr){19-19} \cmidrule(lr){20-20}
      M2a (C=0)          &        & .10   & .09  & .08   & .10   & .09  & .07   & .09   & .09  & .08   & .10   & .08  & .06   & .09   & .09  & .08   & .09   & .09  & .08 \\
      M2a (C=2)          &        & .07   & .05  & .06   & .08   & .07  & .06   & .06   & .06  & .05   & .06   & .06  & .05   & .05   & .05  & .05   & .06   & .06  & .06 \\
      \bottomrule
    \end{tabular}
    \label{tab:FPRates}
    \begin{tablenotes}
      \small
    \item  False positive rates for \gls{lr} tests of positive selection under nested models M1a/M2a with and without likelihood modification.  For each of six simulations scenarios with varying weights and values for two site classes, $\omega<1$ and $\omega=1$, 10,000 sequence alignments $500$ codons long were generated using 5-, 10-, and 32-taxon tree topologies with branch lengths summing to 3, 6, or 9.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown in column and row labels.  Modified likelihood tuning parameters of $C=0$ (no likelihood modification) and $C=2$ were used.  The \gls{lr} statistics were compared to the $95$th percentile of the $\chi_0^2/2 + \chi_1^2/2$ distribution.
    \end{tablenotes}
  \end{threeparttable}
\end{sidewaystable}

\subsection{Power of the Modified LR Tests is Comparable to Re-calibrated LR Tests}
\gls{lr} tests are generally expected to have power that is in some sense optimal \citep[cf Section 5.4.4 of ][]{bickel2015mathematical}.  By modifying the \gls{lr}s, it is possible that some loss of power will accrue.  Figure \ref{fig:powerTl3} shows the power curves, using a threshold calibrated to have Type I error rate $0.05$, with and without likelihood modification.  The plots suggest that likelihood modification has minimal impact on power.
\begin{figure}
  \centering
<<powerPlots,echo=FALSE>>=
lnLs.5taxa.tl3 <- read.csv("~/scm/modl.git/sim/alt/5_taxa_balanced_tree/data/lnLs.csv")
lnLs.10taxa.tl3 <- read.csv("~/scm/modl.git/sim/alt/10_taxa_balanced_tree/data/lnLs.csv")
lnLs.32taxa.tl3 <- read.csv("~/scm/modl.git/sim/alt/32_taxa_balanced_tree/data/lnLs.csv")

power.df <- data.frame(power=0,tl=rep(c(3,6,9),each=90),
                       ntaxa=rep(c(5,10,32),each=30,times=3),
                       p2=rep(c(0.02,0.06,0.1),each=10,times=9),
                       w2=rep(c(1.05,1.2,1.5,3,5),each=2),
                       model=rep(c('C=0','C=2'),times=135))

i <- 1
## num taxa == 5
for (p2 in c('0.02','0.06','0.1')) {
    for (w2 in c('1.05','1.2','1.5','3','5')) {
        for (mod in c('m2a','c2')) {
            lrs <- 2*(lnLs.5taxa.tl3[,paste('p2_',p2,'_w2_',w2,'_',mod,sep='')] - lnLs.5taxa.tl3[,paste('p2_',p2,'_w2_',w2,'_m1a',sep='')])
            lrs.cal <- 2*(lnLs.5taxa.tl3[,paste('p2_0_',mod,sep='')] - lnLs.5taxa.tl3[,'p2_0_m1a'])
            power <- length(lrs[lrs >= sort(lrs.cal)[9500]])/10000
            power.df[i,1] <- power
            i <- i+1
        }
    }
}

## num taxa == 10
for (p2 in c('0.02','0.06','0.1')) {
    for (w2 in c('1.05','1.2','1.5','3','5')) {
        for (mod in c('m2a','c2')) {
            lrs <- 2*(lnLs.10taxa.tl3[,paste('p2_',p2,'_w2_',w2,'_',mod,sep='')] - lnLs.10taxa.tl3[,paste('p2_',p2,'_w2_',w2,'_m1a',sep='')])
            lrs <- lrs[!is.na(lrs)]
            lrs.null <- 2*(lnLs.10taxa.tl3[,paste('p2_0_',mod,sep='')] - lnLs.10taxa.tl3[,'p2_0_m1a'])
            th <- sort(lrs.null)[9500]
            power <- length(lrs[lrs >= th])/length(lrs)
            power.df[i,1] <- power
            i <- i+1
        }
    }
}

## num taxa == 32
for (p2 in c('0.02','0.06','0.1')) {
    for (w2 in c('1.05','1.2','1.5','3','5')) {
        for (mod in c('m2a','c2')) {
            lrs <- 2*(lnLs.32taxa.tl3[,paste('p2_',p2,'_w2_',w2,'_',mod,sep='')] - lnLs.32taxa.tl3[,paste('p2_',p2,'_w2_',w2,'_m1a',sep='')])
            lrs <- lrs[!is.na(lrs)]
            lrs.null <- 2*(lnLs.32taxa.tl3[,paste('p2_0_',mod,sep='')] - lnLs.32taxa.tl3[,'p2_0_m1a'])
            th <- sort(lrs.null)[950]
            power <- length(lrs[lrs >= th])/length(lrs)
            power.df[i,1] <- power
            i <- i+1
        }
    }
}

power.tl3.data <- subset(power.df, tl==3)

power.tl3.plot <- ggplot(power.tl3.data,aes(w2,power)) +
    #ggtitle("Tree Length 3") +
    coord_cartesian(xlim=c(1,5), ylim=c(0,1)) +
    labs(x=expression(omega[2]),y="Power") +
    geom_point(aes(group=model,shape=model),size=2) +
    geom_line(aes(linetype=model),size=.4) +
    scale_linetype_manual(values=c("dotted","solid"),labels=c('C=0','C=2')) +
    ##scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
    scale_x_continuous(trans='log2',breaks=scales::pretty_breaks(n=5)) +
    ##scale_y_continuous(breaks=c(scales::pretty_breaks(n=5)),trans='log2') +
    ##scale_x_log10() +
    facet_grid(p2~ntaxa,labeller=label_bquote(rows=p[2]*'='*.(p2),cols=.(ntaxa)*' Taxa'))

power.tl3.plot +
    theme(panel.spacing=unit(0,"lines"),
          panel.background=element_blank(),
          strip.background=element_blank(),
          plot.title = element_text(hjust = 0.5),
          legend.title=element_blank(),
          legend.text.align=0,
          legend.key=element_rect(fill="transparent"),
          legend.position=c(0.895,.729),
          legend.key.width=unit(2.5,"line"),
          axis.line=element_line(colour="black"),
          text=element_text(size=14),
          panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[Comparison of power under model M2a without (C=0) and with (C=2) likelihood modification]{Comparison of power under model M2a without (C=0) and with (C=2) likelihood modification.  For each simulation setting, 10,000 (5 and 10 taxa) or 1,000 (32 taxa) alignments were generated with 500 codons and $45\%$ weight on $\omega=0.5$, $p_2$ weight on $\omega_2$ and the remaining weight on $\omega=1$.}
  \label{fig:powerTl3}
\end{figure}

\subsection{Modified Likelihood Improves Estimation for Difficult Real Data Settings}
The same $16$ genes described and analyzed in \cite{mingronesba} were analyzed and the results are summarized in Table \ref{tab:realDataResultsOverview}.  For each of the genes \cite{mingronesba} described as \textit{regular} cases with ML estimation showing no evidence of instabilities and bootstrap parameter distributions having low variance (lysin, \textit{nuoL3}, \textit{pol}, \textit{RafL}, \textit{TrbL-VirB6\_3}, and \textit{vif}), the \gls{lr} statistics, $\hat{p}_2$, and $\hat{\omega}_2$ are comparable with and without likelihood modification.  On the other hand, for $4$ of the $5$ genes for which the $\omega$ distribution had been poorly estimated in \cite{mingronesba} (\textit{CDH3}, \textit{mivN}, \textit{pgpA}, \textit{tax}, and \textit{TrbL-VirB6\_2}), the results are very different with and without likelihood modification.  Without likelihood modification, an estimated $\hat{p}_2=0.006$ of the sites in \textit{pgpA} were estimated to have evolved under $\hat{\omega}_2=34.7$ and the \gls{lr} test was rejected.  With likelihood modification, $(p_2,\omega_2)$ was estimated to be $(0.09,1.00)$, the likelihoods under both the null and alternative models are the same, and the \gls{lr} test was not rejected.  With the exception of \textit{tax}, the estimates of $p_2$ were always larger using modified likelihoods and the corresponding estimates of $w_2$ were always smaller with average decreases in the estimated $\omega_2$ equal to $16.85$, $2.22$, and $0.29$ for the genes described in \cite{mingronesba} as \textit{irregular} (excluding \textit{tax}), \textit{uncategorized}, and \textit{regular}, respectively.  Differences in the branch length and $\kappa$ estimates were minor in all cases.  The only \textit{irregular} gene with estimates that did not vary between the two likelihood approaches was the well-known \textit{tax} gene \citep{suzuki2004false,yang2005bayes}.  Its highly unusual site-pattern distribution gives extreme \glspl{mle} with $100\%$ weight ($\hat{p}_2=1$) placed on $\omega>1$.  Because the modified likelihood penalizes against small weight on $\omega>1$, it is not surprising that likelihood modification has no impact on likelihood estimation for the \textit{tax} gene.
\begin{threeparttable}
  \caption{Genes analyzed under models M1a and M2a without (C=0) and with (C=2) likelihood modification.}
  \centering
  \begin{tabular}[]{*{9}l}
    \toprule
    & & & \multicolumn{2}{c}{p-value} & \multicolumn{2}{c}{Tree Length} & \multicolumn{2}{c}{$\hat{p}_2$/$\hat{\omega}_2$} \\
    \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
    \multicolumn{1}{c}{Gene} & \multicolumn{1}{c}{$N_t$} & \multicolumn{1}{c}{$N_c$} & \multicolumn{1}{c}{C=0} & \multicolumn{1}{c}{C=2} & \multicolumn{1}{c}{C=0} & \multicolumn{1}{c}{C=2} & \multicolumn{1}{c}{C=0} & \multicolumn{1}{c}{C=2} \\
    \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9}
    \textit{CDH3}          & 11    & 176  & 1.40e-04 & 8.39e-03 & 0.56 & 0.54 & 0.00/24.57 & 0.08/2.01 \\
    \textit{mivN}          & 5     & 504  & 1.54e-01 & 5.00e-01 & 1.62 & 1.60 & 0.00/5.95  & 0.07/1.00 \\
    \textit{pgpA}          & 5     & 198  & 2.33e-02 & 5.00e-01 & 2.93 & 2.06 & 0.01/34.70 & 0.09/1.00 \\
    \textit{TrbL-VirB6\_2} & 5     & 657  & 4.03e-01 & 5.00e-01 & 2.12 & 2.11 & 0.00/6.17  & 0.11/1.00 \\
    \hline
    lysin                  & 25    & 134  & 0.00e+00 & 0.00e+00 & 8.81 & 8.92 & 0.26/3.25  & 0.27/3.24 \\
    \textit{nuoL3}         & 5     & 499  & 8.26e-14 & 9.63e-14 & 4.58 & 4.75 & 0.04/12.53 & 0.04/12.03\\
    \textit{pol}           & 23    & 947  & 4.33e-15 & 5.61e-15 & 1.31 & 1.32 & 0.02/5.59  & 0.02/5.14 \\
    \textit{RfaL}          & 5     & 403  & 6.20e-06 & 7.89e-06 & 3.46 & 3.50 & 0.07/4.34  & 0.08/3.94 \\
    \textit{TrbL-VirB6\_3} & 5     & 938  & 2.05e-09 & 2.36e-09 & 3.06 & 3.12 & 0.03/5.99  & 0.04/5.76 \\
    \textit{vif}           & 29    & 192  & 2.86e-13 & 3.47e-13 & 2.90 & 2.95 & 0.08/3.56  & 0.10/3.43 \\
    \hline
    $\beta$-globin         & 17    & 144  & 3.69e-03 & 5.84e-03 & 8.40 & 8.62 & 0.03/2.94  & 0.05/2.72 \\
    \textit{ccmF}          & 5     & 635  & 2.54e-05 & 4.40e-05 & 3.41 & 3.28 & 0.01/15.47 & 0.03/8.41 \\
    \textit{ENAM}          & 11    & 1142 & 7.66e-04 & 9.73e-04 & 0.46 & 0.46 & 0.02/5.69  & 0.08/3.41 \\
    \textit{env}           & 13    & 91   & 2.59e-05 & 1.33e-04 & 2.04 & 2.03 & 0.18/3.63  & 0.33/2.79 \\
    \textit{perM}          & 5     & 351  & 1.71e-01 & 2.16e-01 & 1.78 & 1.77 & 0.02/2.57  & 0.04/1.89 \\
    \textit{tax}           & 20    & 181  & 4.17e-03 & 4.17e-03 & 0.13 & 0.13 & 1.00/4.87  & 1.00/4.87 \\
    \bottomrule
  \end{tabular}
  \label{tab:realDataResultsOverview}
  \begin{tablenotes}
    \small
  \item $N_t$: number of taxa; $N_c$: sequence length in number of codons; p-value of the \gls{lr} test for the presence of positive selection using a $\chi^2_0/2 + \chi^2_1/2$ distribution; estimated total tree length; estimated proportion of sites evolving under $\omega>1$: $\hat{p}_2$/$\hat{\omega}_2$.   The top genes represent \textit{irregular} estimation, the middle \textit{regular}, and the bottom genes are uncategorized.
  \end{tablenotes}
\end{threeparttable}

\subsection{Real Data Results Show that Using Modified Likelihood Improves Estimation and Detection of Sites Under Positive Selection}
Although site classification was not a focus of this study, evidence of positive selection at individual sites was checked in order to assess differences using the two likelihood approaches and three site classifiers.  Spearman correlations for the site posteriors are summarized in Table \ref{tab:sitecors} for \gls{neb}, \gls{beb} \citep{yang2005bayes} and \gls{sba}, the smoothed bootstrap method from \cite{mingronesba}, each with and without likelihood modification.  Site classification was nearly identical using \gls{beb} with both likelihood approaches.  This is to be expected since \gls{beb} integrates over the uncertainties in the estimates of the $\omega$ distribution using discretized uniform and Dirichlet priors.  Thus, the only parameters under \gls{beb} that differ with or without modified ML estimation are the edge-lengths and some parameters in the rate matrix, which tended to change much less than the parameters of the mixing distribution.  By contrast, \gls{neb} directly uses the ML estimates of the mixing distribution, which differ considerably with and without likelihood modification.  Consequently, site classification differs substantially under \gls{neb} with and without the modified likelihood.  Given that previous studies have indicated that \gls{beb} and \gls{sba} do better than \gls{neb} at balancing accuracy and power for identifying sites under positive selection \citep[e.g., ][]{anisimova2002accuracy, mingronesba}, the stronger agreement between \gls{beb} and \gls{sba} with \gls{neb} using modified likelihood than \gls{neb} without modified likelihood suggests modified likelihood is beneficial for detecting sites under positive selection.
\begin{table}
  \centering
  \begin{threeparttable}
    \caption{Spearman rank correlations of site posterior probabilities for different methods of classification under model M2a.}
    \begin{tabular}[!ht]{*{10}l}
      \toprule
      Gene                       & N*/N & N*/B & N*/S & B*/N & B*/B & B*/S & N/B  & N/S  & B/S \\
      \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10}
      \textit{CDH3}              & 0.40 & 1.00 & 1.00 & 0.40 & 1.00 & 1.00 & 0.40 & 0.40 & 1.00 \\
      \textit{mivN}              & 0.76 & 0.99 & 0.97 & 0.77 & 1.00 & 0.96 & 0.77 & 0.78 & 0.96 \\
      \textit{pgpA}              & 0.71 & 0.99 & 0.99 & 0.72 & 1.00 & 0.98 & 0.72 & 0.73 & 0.98 \\
      \textit{TrbL-VirB6\_2}     & 0.72 & 1.00 & 0.98 & 0.72 & 1.00 & 0.98 & 0.72 & 0.72 & 0.98 \\
      \hline
      lysin                      & 1.00 & 1.00 & 0.99 & 0.99 & 1.00 & 0.99 & 1.00 & 0.99 & 0.99 \\
      \textit{nuoL3}             & 1.00 & 0.99 & 0.90 & 0.99 & 1.00 & 0.93 & 0.99 & 0.90 & 0.93 \\
      \textit{pol}               & 0.94 & 0.96 & 0.79 & 0.91 & 1.00 & 0.85 & 0.91 & 0.76 & 0.85 \\
      \textit{RfaL}              & 1.00 & 1.00 & 0.97 & 1.00 & 1.00 & 0.97 & 1.00 & 0.96 & 0.97 \\
      \textit{TrbL-VirB6\_3}     & 0.98 & 0.98 & 0.91 & 1.00 & 1.00 & 0.93 & 1.00 & 0.93 & 0.93 \\
      \textit{vif}               & 1.00 & 1.00 & 0.97 & 1.00 & 1.00 & 0.98 & 1.00 & 0.97 & 0.98 \\
      \hline
      $\beta$-globin             & 0.96 & 0.94 & 0.85 & 0.90 & 1.00 & 0.90 & 0.90 & 0.82 & 0.90 \\
      \textit{ccmF}              & 0.93 & 0.93 & 0.87 & 0.86 & 1.00 & 0.96 & 0.86 & 0.80 & 0.96 \\
      \textit{ENAM}              & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
      \textit{env}               & 0.99 & 1.00 & 0.99 & 1.00 & 1.00 & 0.99 & 1.00 & 0.98 & 0.99 \\
      \textit{perM}              & 0.99 & 1.00 & 0.92 & 0.99 & 1.00 & 0.93 & 0.99 & 0.91 & 0.93 \\
      \bottomrule
    \end{tabular}
    \label{tab:sitecors}
    \begin{tablenotes}
      \small
    \item  N: NEB; B: BEB; S: SBA; *: parameter estimation with modified likelihood. The top genes represent \textit{irregular} estimation, the middle \textit{regular}, and the bottom genes are uncategorized.
    \end{tablenotes}
  \end{threeparttable}
\end{table}

\subsection{Investigation of a Problematic Setting}
Estimation and inference becomes more challenging with smaller evolutionary distances or fewer taxa, but, perhaps surprisingly, the results show that the true mixing distribution is at least as important for determining whether a setting is challenging.  This is most evident in the \glspl{cdf} for the null simulation settings using a 5-taxon tree of length $3$ (figure \ref{fig:CDF5taxaTL3}).  Note that the mixing distribution for all of these scenarios is determined by $(p_0,\omega_0)$.  Overall, except for the $(p_0,\omega_0)=(0.25,0.5)$ case, the modified \gls{lr} statistic distribution is still well approximated by a $\chi_0^2/2 + \chi_1^2/2$ distribution, but for this one setting, neither the \gls{lr} statistic nor the modified \gls{lr} statistic distribution is well approximated by the $\chi_0^2/2 + \chi_1^2/2$ distribution.

Histograms of the $\omega_0$ estimates under models M1a and M2a with modified likelihood show the largest variation when $(p_0,\omega_0)=(0.25,0.5)$ (figure \ref{fig:w0MLEs}).  Of the $10,000$ sets of modified likelihood \glspl{mle}, under M2a $2315$ had $90\%$ or more weight on an $\hat{\omega}_0 \ge 0.65$.  Since the true mixing distribution had two well-separated $\omega$ values, $\omega_0=0.5$ and $\omega_1=1$, the expectation was that the estimated distribution would also have well-separated components with appreciable weight.  The theory leading to the $\chi_0^2/2 + \chi_1^2/2$ approximation relies on this being highly likely with sufficiently large sequence lengths.  It is clear from the simulations that sequence lengths of 500 are not long enough to guarantee well-separated components, which lead to the discrepancy for $(p_0,\omega_0)=(0.25,0.5)$ in Figure \ref{fig:CDF5taxaTL3}.  After removing the $2315$ sets of modified \glspl{mle} that had $90\%$ or more weight on an $\hat{\omega}_0 \ge 0.65$, the $\chi_0^2/2+\chi_1^2/2$ \gls{cdf} provides a good approximation to the actual \gls{cdf} of the modified \gls{lr} statistic (figure \ref{fig:CDFFiltered}).  This indicates the estimates with $\hat p_0 \ge 0.9$ and $\hat{\omega}_0 \ge 0.65$ were the source of anomalously larger than expected \gls{lr} statistics.
\begin{figure}
    \centering
    <<w0MLEs, echo=F,warning=F>>=
    p0_0.25_w0_0.25_m2a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.25_m2a_mles.csv",header=F)
    colnames(p0_0.25_w0_0.25_m2a_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.25_w0_0.5_m2a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m2a_mles.csv",header=F)
    colnames(p0_0.25_w0_0.5_m2a_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.5_w0_0.25_m2a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.25_m2a_mles.csv",header=F)
    colnames(p0_0.5_w0_0.25_m2a_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.5_w0_0.5_m2a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.5_m2a_mles.csv",header=F)
    colnames(p0_0.5_w0_0.5_m2a_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.75_w0_0.25_m2a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.25_m2a_mles.csv",header=F)
    colnames(p0_0.75_w0_0.25_m2a_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.75_w0_0.5_m2a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.5_m2a_mles.csv",header=F)
    colnames(p0_0.75_w0_0.5_m2a_mles) <- c('k','p0','p1','p2','w0','w1','w2')

    p0_0.25_w0_0.25_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.25_c2_mles.csv",header=F)
    colnames(p0_0.25_w0_0.25_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.25_w0_0.5_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_c2_mles.csv",header=F)
    colnames(p0_0.25_w0_0.5_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.5_w0_0.25_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.25_c2_mles.csv",header=F)
    colnames(p0_0.5_w0_0.25_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.5_w0_0.5_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.5_c2_mles.csv",header=F)
    colnames(p0_0.5_w0_0.5_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.75_w0_0.25_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.25_c2_mles.csv",header=F)
    colnames(p0_0.75_w0_0.25_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')
    p0_0.75_w0_0.5_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.5_c2_mles.csv",header=F)
    colnames(p0_0.75_w0_0.5_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')

    w0 <- c(p0_0.25_w0_0.25_c2_mles$w0,p0_0.25_w0_0.5_c2_mles$w0,
                  p0_0.5_w0_0.25_c2_mles$w0,p0_0.5_w0_0.5_c2_mles$w0,
                  p0_0.75_w0_0.25_c2_mles$w0,p0_0.75_w0_0.5_c2_mles$w0)

    N <- nrow(p0_0.25_w0_0.25_m2a_mles)

    mle.data <- data.frame(w0,
                           weight=rep(c(0.25,0.5,0.75),each=2*N),
                           omega=rep(c(0.25,0.5),each=N,times=3))

    cdf.plot <- ggplot(mle.data,aes(w0,y=..ncount..)) +
        geom_histogram(binwidth=.005,fill=I('black')) +
        labs(x=expression(omega[0]),y='') +
facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(2,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              text=element_text(size=16),
              axis.text.y=element_blank(),
              axis.ticks.y=element_blank())
              #panel.border = element_rect(colour = "black", fill=NA, size=0))
@
\caption[MLEs of the $\omega_0$ parameter under model M2a using a modified likelihood parameter of $C=2$ for six simulation settings]{MLEs of the $\omega_0$ parameter under model M2a using a modified likelihood parameter of $C=2$ for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 3.  The value of $\omega_0$ and its weight, $p_0$, used to simulate the data are shown as column and row labels.}
  \label{fig:w0MLEs}
\end{figure}
\begin{figure}
    \centering
    <<CDF5taxaFiltered, echo=F,warning=F>>=
    p0_0.25_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_c2_lnLs.csv",sep=',')

    p0_0.25_w0_0.5_c2_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_c2_mles.csv",header=F)
    colnames(p0_0.25_w0_0.5_c2_mles) <- c('k','p0','p1','p2','w0','w1','w2')

    drop <- which(p0_0.25_w0_0.5_c2_mles$p0>0.9)

    p0_0.25_w0_0.5_c2_filtered_mles <- p0_0.25_w0_0.5_c2_mles[-drop,]
    colnames(p0_0.25_w0_0.5_c2_filtered_mles) <- c('k','p0','p1','p2','w0','w1','w2')


    p0_0.25_w0_0.5_5_taxa_m1a_filtered_lnl <- p0_0.25_w0_0.5_5_taxa_m1a_lnl[-drop]
    p0_0.25_w0_0.5_5_taxa_c2_filtered_lnl <- p0_0.25_w0_0.5_5_taxa_c2_lnl[-drop]
    lrs_p0_0.25_w0_0.5_5_taxa_c2_filtered <- sort(2*(p0_0.25_w0_0.5_5_taxa_c2_filtered_lnl-p0_0.25_w0_0.5_5_taxa_m1a_filtered_lnl))

    N <- length(p0_0.25_w0_0.5_5_taxa_m1a_filtered_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.5_5_taxa_c2_filtered)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- c(prob.t,1:N/N)

    cdf.data <- data.frame(lrs,cprob,model=rep(c('Theory','M2a (C=2) (Filtered)'),each=N))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="Likelihood Ratio Statistic",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dotted","solid"),labels=c('M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.80,.7),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[CDF of filtered, modified LR statistics (C=2)]{CDF of filtered, modified LR statistics (C=2).  The modified LR statistics were calculated under the nested model pair M1a/M2a for 10,000 simulated sequence alignments.  The alignments were simulated with 25\% of the sites evolving under $\omega=0.5$ and the remaining sites evolving under $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 3.  A modified likelihood tuning parameters of $C=2$ was used and 2315 LR statistics associated with ML estimates with greater than 90\% of the sites estimated in the $\omega<1$ site class were excluded from the plot.  A $\chi^2_0/2+\chi^2_1/2$ CDF is also included.}
  \label{fig:CDFFiltered}
\end{figure}

Whether a pre-screen would be useful for filtering out datasets with $\hat p_0\ge 0.9$ and $\hat \omega_0\ge 0.65$ was tested.  The pre-screen considered was to ignore datasets failing to reject M1a in an M0 versus M1a test.  While this was effective in that it filtered all the $2315$ datasets described above, it also filtered many other datasets.  Consequently, the distribution of M1a/M2a \gls{lr} statistics remaining after the pre-screen was not as well approximated by the $\chi_0^2/2+\chi_1^2/2$ \gls{cdf} as the one with the $2315$ datasets manually filtered (figure \ref{fig:CDFPS}).  As a second check, the data was re-simulated under the same settings, but with codon frequencies derived from abolone sperm lysin \citep{yang2000maximum}, however the M1a/M2a \gls{lr} statistic distribution was, again, not well approximated by a $\chi_0^2/2+\chi_1^2/2$ CDF.

\subsection{Parameters can be Almost Unidentifiable for Codon Models}
To further investigate why $(p_0,\omega_0)=(0.25,0.5)$ was a difficult setting, Kullback-Leibler (KL) divergences were approximated between pattern distributions coming from $(p_0,\omega_0)=(0.25,0.5)$ and pattern distributions from other mixing distributions (figure \ref{fig:KLp0.25w0.5}).  When $KL=0$, two mixing distributions give exactly the same pattern probabilities and the mixing distributions are said to be unidentifiable.  When $KL>0$ but small, distinguishing between the two mixing distributions will be difficult.  A number of the KL divergences in Figure \ref{fig:KLp0.25w0.5} are close to $0$, including those for $(p_0,\omega_0)=(0.5,0.75)$ and $(p_0,\omega_0)=(0.75,0.8)$.  To determine whether the $KL$ was indeed $0$, attention was restricted to all site patterns for pairs of taxa to give tractable calculations.  $KL_s$, calculated using site pattern distributions for a subset of the taxa, satisfies $KL_s\le KL$.  Thus, if any pair of taxa gives $KL_s>0$, then the KL for all taxa must be positive.  For $(p_0,\omega_0)=(0.5,0.75)$ this gives $KL>0.00018$, the maximium KL over pairs.
\begin{figure}
  \centering
<<klp0.25w0.5, echo=F>>=
site.lik.truth <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3_10000_codons/data/sim_p0_0.25_w0_0.5/site_lik_truth.csv",header=F)
colnames(site.lik.truth) <- c('cnt','lik')
n <- sum(site.lik.truth$cnt)
cnt <- site.lik.truth$cnt
y.truth <- site.lik.truth$lik
y.truth[y.truth==0] <- 1e-12
kl <- numeric(0)
vkl <- numeric(0)
for (p in seq(0,1,.1)) {
    for (w in seq(0,1,.1)) {
        site.lik <- read.csv(paste("~/scm/modl.git/sim/null/5_taxa_bl_3_10000_codons/data/sim_p0_0.25_w0_0.5/site_like_p_",p,"_w_",w,".csv",sep=''),header=F)
        y <- site.lik[,2]
        y[y==0] <- 1e-12
        y.bar <- sum(cnt*log(y.truth / y))/n
        kl <- c(kl,y.bar)
        vkl <- c(vkl,sum(cnt*(log(y.truth / y) - y.bar)^2)/n)
    }
}
p <- rep(seq(0,1,.1),each=11)
w <- rep(seq(0,1,.1),times=11)
lb <- kl - 2*sqrt(vkl/10000)
ub <- kl + 2*sqrt(vkl/10000)
kl.data <- data.frame(kl,lb,ub,w,p)
kl.plot <- ggplot(kl.data,aes(p,kl)) +
    labs(x=expression("p"[0]),y="Kullback-Leibler Divergence") +
    geom_point() +
    geom_hline(yintercept=0) + facet_wrap(~w,ncol=3,labeller=label_bquote(cols=omega[0]*'='*.(w))) +
    coord_cartesian(ylim = c(-0.001, 0.01)) +
    geom_errorbar(aes(ymin=lb,ymax=ub))

kl.plot + theme(panel.spacing=unit(0,"lines"),
                panel.background=element_blank(),
                strip.background=element_blank(),
                legend.title=element_blank(),
                legend.key=element_rect(fill="transparent"),
                text=element_text(size=12),
                panel.border = element_rect(colour = "black", fill=NA, size=1))

@
\caption[Approximations of the Kullback-Leibler divergences between the distributions of site likelihoods for the generating model and other mixing distributions]{Approximations of the Kullback-Leibler divergences between the distributions of site likelihoods for the generating model and other mixing distributions.  The approximations were obtained as the mean lnL difference between 10,000 site patterns generated under model M1a using a 5-taxon tree with branch lengths summing to 3 and the mixing distribution $(p_0,\omega_0)=(0.25,0.5)$, and other mixing distributions with varying weights on values of $\omega$ ranging from $0$ to $1$.  Error bars for two standard errors $(s_{KL}/\sqrt{10000})$ above and below each Kullback-Leibler estimate are included.  Points missing from each plot are above the visible range.}
\label{fig:KLp0.25w0.5}
\end{figure}

Consideration of KL divergences for mixing distributions $(p_0,\omega_0)=(1,0.75)$ and $(p_0,\omega_0)=(0.5,0.5)$ allows us to show that there are ranges of distributions that are almost unidentifiable.  The maximum KL divergence over pairs of taxa from the $5$-taxon tree was small $(0.00085)$, thus \[p(x;\omega=0.75,\zeta) \approx p(x;\omega=0.5,\zeta)/2 + p(x;\omega=1,\zeta)/2.\]  Multiplying this equation by $p_0^{'}$ and rearranging, one can show \[p(x;\omega=0.75,\zeta)p_0^{'} + p(x;\omega=1,\zeta)(1-p_0^{'})  \approx p(x;\omega=0.5,\zeta)p_0^{'}/2 + p(x;\omega=1,\zeta)/(1-p_0^{'}/2).\]  Thus, mixing distributions $(p_0,\omega_0)=(p_0^{'},0.75)$ give pattern probabilities that are difficult to distinguish from $(p_0,\omega_0)=(p_0^{'}/2,0.5)$.  This holds for the range of mixing distributions with $0 \le p_0^{'} \le 1$ (figure \ref{fig:KLp0.25w0.5}).

While it has been shown that there are regions of mixing-distribution parameter space that can make estimation and inference difficult, the results also show that there are regions where distinguishing between mixing distributions is not difficult, even for the $(p_0^{'},0.75) \approx (p_0^{'}/2,0.5)$ comparison above.  This is because $p_0^{'}/2 \le 0.5$, so pattern probabilities generated from any $(p_0,\omega_0)=(p_0^{'},\omega=0.75)$ can not be consistent with e.g., $(p_0,\omega_0)=(0.75,\omega=0.5)$ (figure \ref{fig:KLp0.75w0.5}).  Finally, the good behaviour of the modified \gls{lr} statistic when used with trees with more taxa and longer branch lengths (e.g., figure \ref{fig:CDF32taxaTL9}) suggests these problems are likely restricted to trees with fewer taxa and shorter branch lengths.

\subsection{Concluding Remarks}
Challenging issues have been described with mixture models of codon evolution that result in null distributions of LR statistics that are not tractable when testing for positive selection.  A common violation of the regularity conditions under the widely employed M2a model is for small weight to be placed on $\omega>1$.  This results in LR statistics that, when compared to thresholds predicted by a $\chi^2_0/2 + \chi^2_1/2$ distribution, tend to give inflated false positive rates.  By including a penalty in likelihood calculations for small weight on $\omega>1$, in most cases, LR statistic distributions are well approximated by a $\chi^2_0/2 + \chi^2_1/2$ distribution and false positive rates are adequately controlled.  Simulations under the alternative hypothesis show that modifying the LR statistic has minimal impact on power.

The likelihood modification may introduce small positive bias to the estimated weight on $\omega>1$. But, the results here show that when there is signal in the data for the weight to be close to 0 and the sample size or the number of taxa are large, the contribution from the likelihood overwhelms the penalty term.  In small sample cases, it may be difficult to overwhelm the penalty term in the modified LR test.  However, when the weight on $\omega>1$ is estimated to be close to $0$ and the sample size is small, the standard LR tests are unreliable.  Thus, whether samples sizes are small or large, and whether the weight on $\omega>1$ is actually close to 0 or not, the modified LR test is appropriate.

For values of the tuning parameter, $C$, substantially smaller than $2$, the behaviour of the modified LR statistic was too similar to that of the standard LR statistic.  As values of $C$ larger than $2$ gave indistinguishable LR statistic CDFs to those with $C=2$, we used a simple, default value of $C=2$, which showed good performance.  However, further improvements might be obtained with different data-dependent choices of $C$; the optimal choice is a topic for future research.  A possible approach for making an optimal, data-dependent choice is cross-validation.  One would choose $C$ to make the average likelihood on test samples as large as possible.

The problematic behaviour of LR statistics discussed here arises more broadly with mixtures and is not usually amenable to solutions like those discussed in \cite{self1987asymptotic}.  For instance, \cite{chernoff1995asymptotic} outline a class of problems using a mixture of binomial distributions to evaluate genetic markers for genes representing heterogeneous traits.  Similar to how any $\omega_2>1$ with a weight of $p_2=0$ gives the null under model M2a, any mixing distribution gives the null when the parameters of the two binomials are estimated to be the same.
% The distribution of the LR statistic was shown to be the same as that of the maximum of a Gaussian process.
For many of the settings considered, no closed form expression for the distribution was available and, by contrast with usual $\chi^2$ distributions, depended on unknown parameters in the true model under the null hypothesis.  Generally, it appears that when dealing with mixtures in molecular evolution settings, simple limiting distributions like those of \cite{self1987asymptotic} are not likely without some modification to the likelihood.

The simulation results expose an additional difficulty.  For certain generating mixing distributions like $(p_0,\omega_0)=(0.25,0.5)$ under model M2a, there are other mixing distributions that can give very similar pattern probabilities when the number of taxa is small and edge lengths are short.  Models that are almost unidentifiable will give flat likelihood surfaces for subsets of parameters.  Perturbations caused by unwanted influences such as sequencing error and model misspecification will be more pronounced than on peaked likelihood surfaces.  Thus, the identifiability problems exposed when edge lengths are short might, for instance, help to explain the elevated false positive rates found by \cite{schneider2009estimates} in the presence of sequencing error and short edge lengths, and the finding of \cite{venkat2018multinucleotide} that model-misspecification in the human lineage led to higher false positives.

That some models were found to be almost unidentifiable suggests that there may be mixing distributions and trees that lead to a complete lack of identifiability.  There has been some work to explore identifiability of mixture models of molecular evolution \citep[e.g.,][]{allman2008identifiability,allman2009identifiability,chai2011rogers}, but none of these results apply directly to codon models.  Determining the extent to which these types of issues affect codon models of evolution will be a topic for future research.

Implementation of a similar modified likelihood approach to other models is straightforward and can be expected to offer similar advantages.  As there is an abundance of mixture models used to solve a variety of problems in the field of molecular evolution, potential candidates are numerous and include models used to 1. identify functionally divergent protein residues, 2. detect pattern-heterogeneity in gene sequence or character-state data, 3. infer protein phylogenies, and 4. identify across-site heterogeneities in the amino-acid replacement process \citep{gaston2011phylogenetic,pagelpyhlogentic,wang2008class,lartillotbayesian}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Challenging issues with mixture models of codon evolution result in null distributions of \gls{lr} statistics that are not tractable when testing for positive selection.  A common violation of the regularity conditions under the widely employed M2a model is for small weight to be placed on $\omega>1$.  This results in \gls{lr} statistics that, when compared to thresholds predicted by a $\chi^2_0/2 + \chi^2_1/2$ distribution, tend to give inflated false positive rates.  By including a penalty in likelihood calculations for small weight on $\omega>1$, in most cases, \gls{lr} statistic distributions are well approximated by a $\chi^2_0/2 + \chi^2_1/2$ distribution and false positive rates are adequately controlled.  Simulations under the alternative hypothesis show that modifying the \gls{lr} statistic has minimal impact on power.

% The problematic behaviour of \gls{lr} statistics discussed here arises more broadly with mixtures and is not usually amenable to solutions like those discussed in \cite{self1987asymptotic}.  For instance, \cite{chernoff1995asymptotic} outline a class of problems using a mixture of binomial distributions to evaluate genetic markers for genes representing heterogeneous traits.  Similar to how any $\omega_2>1$ with a weight of $p_2=0$ gives the null under model M2a, any mixing distribution gives the null when the parameters of the two binomials are estimated to be the same.
% For many of the settings considered, no closed form expression for the distribution was available and, by contrast with usual chi-square distributions, depended on unknown parameters in the true model under the null hypothesis.  Generally, it appears that when dealing with mixtures in molecular evolution settings, simple limiting distributions like those of \cite{self1987asymptotic} are not likely without some modification of the likelihood.

% The simulation results expose an additional difficulty.  For certain generating mixing distributions like $(p_0,\omega_0)=(0.25,0.5)$, there are other mixing distributions that can give very similar pattern probabilities when the number of taxa is small and edge lengths are short.  That some models were found to be almost unidentifiable suggests that there may be mixing distributions and trees that lead to a complete lack of identifiability.  There has been some work to explore identifiability of mixture models of molecular evolution \citep[e.g.,][]{allman2008identifiability,allman2009identifiability,chai2011rogers}, but none of these results apply directly to codon models.  Determining the extent to which these types of issues affect codon models of evolution will be a valuable topic for future research.

% The utility of mixture models to solve a variety of problems in the field of molecular evolution has been well established \citep[e.g.,][]{gaston2011phylogenetic,lartillotbayesian,pagelpyhlogentic,wang2008class}.  The modified likelihood proposed here does not eliminate all potential pitfalls associated with the limiting distributions of \gls{lr} statistics for mixture models of codon evolution, but it does do well to correct for one common issue.  Implementations for a variety of mixture models would be straightforward to implement, thus there is no compelling reason not add a penalty term for small mixing weights, especially, as in the case with model M2a, there is little impact on power.

\chapter{Smoothed Bootstrap Aggregation}
\label{chap:sba}
\section{Introduction}
Identifying positively selected amino acid sites is a challenging statistical task that is important for investigating the functional consequences of molecular change \citep{yang2005power}.  Several approaches have been developed to detect positive selection within a protein \citep[reviewed in][]{pond2005not,anisimova2009investigating}, but their reliability varies according to the properties of the data in hand.  The most widely used methods employ a codon model to detect an excess in the rate of nonsynonymous substitutions relative to synonymous substitutions ($dN/dS = \omega > 1$), which is an indication of evolution by positive selection.  Proteins evolving under positive selection must retain the capacity to fold into complex structural and functional domains, so the majority of amino acid substitutions will be subject to purifying selection pressure, with $\omega < 1$ \citep{kimura1968evolutionary}.  From extensive surveys of positive selection in real genes, the expectation is that only a small fraction of amino acid sites will be subject to adaptive change and exhibit an $\omega > 1$ \citep[e.g., ][]{anisimova2007phylogenomic,ge2008protein}.  The sparseness of these sites makes them challenging to identify.

%\subsection*{Counting and Fixed-Effect Methods}
Two general categories of methods for detecting positively selected amino acid sites include counting and fixed-effect methods.  Counting methods employ ancestral reconstruction of codon states for all internal nodes of a phylogenetic tree to obtain counts of the synonymous and nonsynonymous changes along each of its branches.  The counts inferred for a given site are used to test if $\omega \neq 1$.  Some counting methods use parsimony \citep{fitch1997long,bush1999positive,suzuki1999method}, and others likelihood \citep{suzuki2004new,nielsen2002mapping,nielsen2002detecting,suzuki2004false,pond2005not} to infer the ancestral codon states.  The reconstructions are often similar, but under the likelihood approach uncertainty about the inference can be summarized via the posterior probabilities of the ancestral states.  Thus, the parsimony based methods must assume that these uncertainties are irrelevant to the statistical test.  While this makes the approach attractive for very large datasets where reliable reconstructions can be obtained relatively quickly \citep{lemey2012counting}, widespread use is hindered by a lack of power when the level of divergence is too low or by the negative impact of substitutional saturation when the level of divergence is too high \citep{pond2005not}.

An alternative approach is to treat each site as independently relevant to the question of evolution by positive selection, and attempt to fit an $\omega$ parameter to the data at each site.  Thus, the effect of each site on the task of $\omega$ inference is fixed.  Model based testing for $\omega \neq 1$ can be carried out via a standard \gls{lr} test, and no assumptions are required about the distribution of selection pressure, $\omega$.  Although $\omega$ is treated as a site-specific variable, other important variables in the codon model (e.g., branch lengths) are shared among sites, with their values estimated jointly from the complete set of sites.  Results obtained by using these modelling ideas \citep{pond2005not, massingham2005detecting} are encouraging, and it is expected that this family of methods will continue to have a role in real data analyses \citep{scheffler2014validity}.  However, $\chi^2$ approximations to the distribution of the test statistic assume relatively large numbers of taxa, which is often not the case.  The lack of independence of data across taxa that is due to phylogeny creates further difficulties for $\chi^2$ approximations.

A third approach for detecting positive selection at amino acid sites, which is the focus of this article, treats the value of \(\omega\) at a site as the realized value of a random variable.  A particular model for the distribution of \(\omega\) is chosen and \gls{ml} is used to fit the distribution to the data as part of an explicit model of codon evolution.  There are recommendations \citep[e.g.,][]{yang1998synonymous} to use a pre-screen that fits two models: one with a distribution that excludes values of \(\omega>1\), and another with the same distribution, except with weight on values of \(\omega>1\) permitted.  This nested-model pre-screening is used to test if the data conveys any evidence of positive selection.  When the null hypothesis of no positive selection is rejected using a \gls{lr} test, site-wise analysis is warranted.  Site-wise analysis is carried out using Bayes rule to calculate the posterior probability that a site \(h\) evolved under some estimated value of \(\omega\), given the data at site \(h\).  This approach is referred to as empirical Bayes (EB) because the marginal distribution of \(\omega\) is determined from the data.  Conclusions regarding the evolution at a site are made based on the estimated \(\omega\)-values along with their associated posterior probabilities conditioned on the data at the site.  For example, when the largest posterior probability for a site is associated with a value of \(\omega>1\), this is taken as evidence of positive selection at that site.

Because the marginal distribution of $\omega$ is determined from the data and the site posterior probabilities always depend on the fitted values of the model parameters (shape parameters of the distribution, edge lengths, etc.), the reliability of EB inference depends on the accuracy of the fitted values.  If they have been accurately estimated, as is often the case with large, information-rich datasets, they can simply be treated as known without errors.  This approach is known as the \gls{neb} \citep{nielsen1998likelihood}.  However, when the fitted values are subject to large errors, the detection of positive selection according to %%$Pr(\omega^{(h)}>1|x_h)$
the posterior probabilities can be negatively impacted and in some cases the false positive rate can be unacceptably high \citep{wong2004accuracy}.  \gls{beb} has been used to adjust for uncertainty in the parameters of the $\omega$ distribution by assigning priors to those parameters and using numerical integration to average over the uncertainty represented by the priors \citep{yang2005bayes}.  Because this tactic can substantially reduce the false positive rate relative to \gls{neb} in problematic datasets, \gls{beb} has become a popular method for inferring the action of selection at individual sites.  A fully Bayesian approach that also assigns priors to edge-lengths and other parameters is available for the inference of positive selection at sites \citep{aris2003bayes, huelsenbeck2004bayesian}, but it is not as widely employed as EB because it is available for a limited set of models.

\gls{beb} does have limitations.  As currently implemented, the \gls{beb} approach only accommodates uncertainty in the parameters of the $\omega$ distribution, leaving all others fixed to their fitted values.  Furthermore, only uniform priors are used, which means the adjustment for uncertainty is independent of the signal in the data.  Although these will not be serious limitations for many analyses of real data, it is shown here through simulation and real data analysis that deriving the adjustment for parameter uncertainty from the data can improve inference for some datasets.  To avoid the need for priors, a new approach is presented here that uses bootstrapping \citep{efron1979bootstrap,efron1982jackknife} of site patterns to simulate dataset variability and adjust for the uncertainty in the data.  From bootstrap datasets, the distribution of the \glspl{mle} can be estimated.  The posterior probabilities %%$\omega^{(h)}>1|x_h$
for positive selection at a site is then obtained using an aggregate value coming from \glspl{mle} over bootstrapped data sets, rather than according to a single posterior probability obtained under \gls{neb} or \gls{beb}.  In principle, bootstrap-based methods should use as many replicates as possible to approximate the infinite-sample bootstrap distribution.  As this is computationally expensive, smoothing techniques borrowed from kernel density estimation \citep[Section 3.4]{silverman1987bootstrap,davison1997bootstrap} are used to obtain an approximation with less computational cost.  I refer to this new approach as \gls{sba}.  Simulation results show that \gls{sba} balances accuracy and power at least as well as \gls{beb}.

Also investigated is the behaviour of ML estimation when standard regularity conditions, such as the requirement for true parameter values to be in the interior of the parameter space, are not met.
Codon models fit $\omega$ distributions that, for some data-generating settings, violate regularity conditions, which leads to substantial instability in parameter estimation.  These instabilities have a negative impact on the inference of positive selection under EB, and it is shown here that the new approach is an improvement over both \gls{neb} and \gls{beb} in such cases.  Also show here is that results previously reported for the \textit{tax} gene of HTLV \citep{suzuki2004false} are likely a consequence of such instabilities.  The \textit{tax} gene is a well known example where EB is widely considered unreliable, and it has been used to criticize the overall approach.  An explanation is provided for the previous results obtained under EB methods for the \textit{tax} gene, and show that \gls{sba} can help diagnose such dubious inferences.

\section{New Approaches}
A new approach for classifying sites call \gls{sba} is presented here.  \gls{sba} uses bootstrapping and kernel smoothing techniques to accommodate uncertainties in \glspl{mle}.  Site patterns from a sequence alignment are sampled with replacement to create a number of bootstrap sequence alignments.  For each of the bootstrap sequence alignments, \glspl{mle} are calculated.  The usual bootstrap distribution is the empirical distribution of the calculated \glspl{mle}.  To avoid difficulties due to 1) low information content in the data, 2) necessarily limited bootstrap sampling and 3) instabilities in the parameter estimates, a kernel density estimate of the bootstrap distribution coming from the \glspl{mle} is instead used.  The smoothness of the distribution is controlled by a bandwidth parameter, which is set larger than conventional values to give greater smoothing.

While typical applications of bootstrapping use \glspl{mle} to calculate confidence intervals and standard errors, we, instead, use the bootstrap to accommodate uncertainty in the posterior probabilities of positive selection at sites.  For any given site in the original sequence alignment, many parameter values are generated from the smoothed bootstrap distribution and substituted into posterior probability formulas to give a distribution of posterior probabilities which reflects parameter uncertainty.  The mean or median of these posteriors is a more stable estimate of the true posterior and is used for classification.  See figure \ref{fig:bootstrap} for an overview of \gls{sba}.

\tikzstyle{line} = [draw,-stealth]
\tikzstyle{block} = [draw=none]
\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node[block] (x) {\(\bm{x}\)};
    \node[block,below of=x] (x2) {\(\bm{x}^{*2}\)};
    \node[block,right of=x2] (dots1) {\(\dots\)};
    \node[block,left of=x2,xshift=-1cm] (x1) {\(\bm{x}^{*1}\)};
    \node[block,right of=dots1] (xB) {\(\bm{x}^{*B}\)};
    \node[block,below of=x1] (t1) {\(\hat{\theta}^{*1}\)};
    \node[block,below of=x2] (t2) {\(\hat{\theta}^{*2}\)};
    \node[block,below of=dots1] (dots2) {\(\dots\)};
    \node[block,below of=xB] (tB) {\(\hat{\theta}^{*B}\)};
    \node[block,below of=t1,xshift=-3cm] (p1) {\(Pr_h(\omega>1|x_h,\hat{\theta}^{*1})\)};
    \node[block,below of=t2] (p2) {\(Pr_h(\omega>1|x_h,\hat{\theta}^{*2})\)};
    \node[block,right of=p2,xshift=1.5cm] (dots3) {\(\dots\)};
    \node[block,below of=tB,xshift=3cm] (pB) {\(Pr_h(\omega>1|x_h,\hat{\theta}^{*B})\)};
    \node[block,below of=p2,yshift=-1cm] (agg) {\(\sum_{b=1}^BPr_h(\omega>1|x_h,\hat{\theta}^{*b})/B\)};
    \path[line] (x) -- (x1);
    \path[line] (x) -- (x2);
    \path[line] (x) -- (xB);
    \path[line] (x1) -- (t1);
    \path[line] (x2) -- (t2);
    \path[line] (xB) -- (tB);
    \path[line] (t1) -- (p1);
    \path[line] (t2) -- (p2);
    \path[line] (tB) -- (pB);
    \path[line] (p1) -- (agg);
    \path[line] (p2) -- (agg);
    \path[line] (pB) -- (agg);
  \end{tikzpicture}
  \caption[Bootstrapping site patterns in a codon sequence alignment to classify selection pressure at codon sites]{Bootstrapping site patterns in a codon sequence alignment to classify selection pressure at codon sites.  From an alignment of protein coding DNA sequences, $\bm{x}$, with $n$ codon sites, site patterns are randomly sampled with replacement to obtain a bootstrap sample, $\bm{x}^{*b}$ with $n$ sites.  MLEs, $\hat{\theta}^{*b}$, are then estimated for bootstrap sample $\bm{x}^{*b}$.  Using $\hat{\theta}^{*b}$ and $\bm{x}$, the posterior probability $Pr_h(\omega>1|x_h,\hat{\theta}^{*b})$, that site $h$ is under positive selection is calculated.  These steps are repeated $B$ times to calculate $B$ sets of posterior probabilities.  An aggregate posterior probability that site $h$ is under positive selection is calculated by, for instance, averaging posterior probabilities over bootstrap replicates, $\sum_{b=1}^BPr_h(\omega>1|x_h,\hat{\theta}^{*b})/B$.}
  \label{fig:bootstrap}
\end{figure}

\section{Results}
\subsection{Non-standard ML Estimation Behaviour}
Parameter estimation by ML has attractive statistical properties, including consistency, efficiency, and asymptotic normality, when certain regularity conditions hold \citep{kalbfleisch1985probability,bickel2015mathematical}.  For settings where regularity conditions hold, I verified that I could obtain well-behaved estimates of the parameters of the $\omega$ distribution under two commonly used codon models: M2a \citep{nielsen1998likelihood,yang2005bayes} and M8 \citep{yang2000codon}.  I simulated 100 datasets representing a \textit{regular} estimation problem with an $\omega$ distribution having at least $10\%$ weight on each site class ($45\%$ $\omega=0$, $45\%$ $\omega=0.5$, and $10\%$ $\omega=5$). As expected, \glspl{mle} obtained from these data under both M2a and M8 have unimodal and symmetric distributions (figure \ref{fig:mle_hists}\subref{sfig:sim_M2a_easy},\subref{sfig:sim_M8_easy}).  For the estimates in this \textit{regular} case, there are no indications of departures from the limiting properties predicted by ML theory.

<<mles, echo=F>>=
cat1.scheme4.5.taxa.m2a.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme4_5_taxa_m2a_mles.csv',header=F)
cat1.scheme4.5.taxa.m8.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme4_5_taxa_m8_mles.csv',header=F)
cat1.scheme1.5.taxa.m2a.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme1_5_taxa_m2a_mles.csv',header=F)
## remove rows with crazy large w2
cat1.scheme1.5.taxa.m2a.mles <- cat1.scheme1.5.taxa.m2a.mles[-which(cat1.scheme1.5.taxa.m2a.mles[,7] > 15),]
cat1.scheme1.5.taxa.m8.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme1_5_taxa_m8_mles.csv',header=F)
## remove rows with crazy large w+
cat1.scheme1.5.taxa.m8.mles <- cat1.scheme1.5.taxa.m8.mles[-which(cat1.scheme1.5.taxa.m8.mles[,23] > 17),]
## over bootstraps for a single simulated dataset
cat1.scheme4.5.taxa.m2a.bs.4.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme4_5_taxa_bs_4_m2a_mles_easy.csv',header=F)
cat1.scheme4.5.taxa.m8.bs.73.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme4_5_taxa_bs_73_m8_mles_easy.csv',header=F)
cat1.scheme1.5.taxa.m2a.bs.86.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme1_bs_86_m2a_mles_difficult.csv',header=F)
cat1.scheme1.5.taxa.m8.bs.70.mles <- read.csv('../sba.git/doc/paper/data/cat1_scheme1_bs_70_m8_mles_difficult.csv',header=F)
@

\begin{figure}[H]
  \centering
   \begin{subfigure}[t]{0.49\textwidth}\centering Simulated\end{subfigure}
   \begin{subfigure}[t]{0.49\textwidth}\centering Bootstrap\end{subfigure}
   \begin{subfigure}[t]{.01\textwidth}
     \vspace{15 mm}
     \rotatebox{90}{\textit{Regular}}
   \end{subfigure}
   \begin{subfigure}[t]{0.22\textwidth}
     \centering
     <<echo=F>>=
     par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
     hist(cat1.scheme4.5.taxa.m2a.mles[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
     axis(1,cex.axis=3,padj=1)
     par(mar=c(4.5,1.5,2.8,4))
     hist(cat1.scheme4.5.taxa.m2a.mles[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
     axis(1,cex.axis=3,padj=1)
     @
     \caption{M2a}
     \label{sfig:sim_M2a_easy}
   \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(cat1.scheme4.5.taxa.m8.mles[,12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme4.5.taxa.m8.mles[,23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:sim_M8_easy}
  \end{subfigure}
   \begin{subfigure}[t]{0.22\textwidth}
     \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(1-cat1.scheme4.5.taxa.m2a.bs.4.mles[,8]-cat1.scheme4.5.taxa.m2a.bs.4.mles[,9],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme4.5.taxa.m2a.bs.4.mles[,11],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M2a}
    \label{sfig:bs_M2a_easy}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(1-cat1.scheme4.5.taxa.m8.bs.73.mles[,8],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme4.5.taxa.m8.bs.73.mles[,11],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:bs_M8_easy}
  \end{subfigure}
  \par\bigskip
  \begin{subfigure}[t]{.01\textwidth}
    \vspace{12 mm}
    \rotatebox{90}{\textit{Irregular}}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(cat1.scheme1.5.taxa.m2a.mles[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme1.5.taxa.m2a.mles[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M2a}
    \label{sfig:sim_M2a_diff}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(cat1.scheme1.5.taxa.m8.mles[,12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme1.5.taxa.m8.mles[,23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:sim_M8_diff}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(1-cat1.scheme1.5.taxa.m2a.bs.86.mles[,8]-cat1.scheme1.5.taxa.m2a.bs.86.mles[,9],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme1.5.taxa.m2a.bs.86.mles[,11],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M2a}
    \label{sfig:bs_M2a_diff}
  \end{subfigure}
  \begin{subfigure}[t]{0.22\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(1-cat1.scheme1.5.taxa.m8.bs.70.mles[,8],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme1.5.taxa.m8.bs.70.mles[,11],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:bs_M8_diff}
  \end{subfigure}
  \caption[MLE distributions of the $p_{\omega>1}$ and $\omega_{>1}$ parameters under M2a and M8]{MLE distributions of the $p_{\omega>1}$ and $\omega_{>1}$ parameters under M2a and M8.  Histograms are over 100 simulated (\subref{sfig:sim_M2a_easy},\subref{sfig:sim_M8_easy},\subref{sfig:sim_M2a_diff},\subref{sfig:sim_M8_diff}) and bootstrap (\subref{sfig:bs_M2a_easy},\subref{sfig:bs_M8_easy},\subref{sfig:bs_M2a_diff},\subref{sfig:bs_M8_diff}) datasets with the bootstrap datasets generated by sampling from one simulated dataset.  Data were simulated under \textit{regular} (\subref{sfig:sim_M2a_easy} - \subref{sfig:bs_M8_easy}) and \textit{irregular} (\subref{sfig:sim_M2a_diff} - \subref{sfig:bs_M8_diff}) conditions.\\ \\\textit{regular} simulation conditions: 5 taxa, $45\% \omega=0$, $45\% \omega=0.5$, and $10\% \omega=5$\\ \textit{irregular} simulation conditions: 5 taxa, $100\% \omega=1$}
  \label{fig:mle_hists}
\end{figure}

% Because the $\omega$ and $p$ parameters have boundaries on the values they can take on, all these distributions can be impacted by boundary conditions leading to a truncation in the distribution.
The regularity condition requiring true parameter values to be in the interior of the parameter space is sometimes violated when using codon models.  For such parameter settings, instabilities or departures from the expected limiting properties of ML estimation can arise including non-Gaussian and over-dispersed distributions of estimates.  To investigate instabilities under models M2a and M8, I simulated 100 datasets representing an \emph{irregular} estimation problem with sparse information, i.e., $100\%$ of the sites at the threshold for positive selection, $\omega=1$.  In figure \ref{fig:mle_hists}\subref{sfig:sim_M2a_diff},\subref{sfig:sim_M8_diff}, in contrast to the results presented in figure \ref{fig:mle_hists}\subref{sfig:sim_M2a_easy},\subref{sfig:sim_M8_easy}, there are instabilities in the \glspl{mle} for the parameters representing the proportion of sites under positive selection, $p_{\omega>1}$.  The $p_{\omega>1}$ parameter distributions under both models have mass concentrated on both the lower and upper boundaries of the parameter space, and the distributions of the corresponding $\omega_{>1}$ parameters are concentrated on the lower boundary.  Application of the \gls{lr} test to filter datasets that convey no evidence of positive selection did not prevent instabilities.  The null hypothesis of no positive selection was rejected for 10 datasets under M2a and 9 under M8, however, the \gls{mle} distributions after applying this pre-screening step remained unstable (supplementary figure \ref{fig:hists-cat1-scheme1-filter}).

Some of the model M2a \gls{mle} instabilities shown in figure \ref{fig:mle_hists}\subref{sfig:sim_M2a_diff},\subref{sfig:sim_M8_diff} are due to the discrete $\omega$ distribution.  True discrete distributions of interest can lie on the boundary of the parameter space, which is a regularity condition violation that gives rise to \gls{mle} instabilities.  For instance, consider data generated from an $\omega$ distribution with no mass on $\omega>1$.  Estimates of the $\omega$ distribution will tend to approximate the true distribution and one way this can occur under M2a is when $\hat\omega_{>1} \approx 1$.  When this happens, the likelihood will remain approximately constant over all choices of $p_{\omega=1}$ and $p_{\omega>1}$, giving a sum, $p_{\omega>1}+p_{\omega=1}$, that is approximately the same as that of the \gls{mle}.  Consequently, estimates of $p_{\omega=1} + p_{\omega>1}$ are stable, but estimates of $p_{\omega=1}$ and $p_{\omega>1}$ are not, because many different choices give the same sum.  Likewise, when a $p_{\omega>1}$ parameter is estimated near 0, the corresponding $\omega_{>1}$ can take on almost any value without changing the likelihood.  For example, two M2a and six M8 biologically unrealistic estimates of the $\omega_{>1}$ parameter (e.g. $\omega_{>1}=999$) occurred when the corresponding $p_{\omega>1}$ parameters were estimated to be 0.  These estimates were excluded from the $\omega_{>1}$ histograms.  For the data representing an \textit{irregular} estimation problem with all sites simulated with $\omega=1$, two other problematic M2a parameterizations that fit the data equally well occurred often.  First, all the weight was put on the $w_1$ category and second, all the weight was put on the $w_{>1}$ category when it was estimated very close to 1. Although there is virtually no difference in the likelihood scores between the two parameterizations, the \gls{neb} posterior probabilities for positive selection were 0 and 1 respectively.  These different \gls{mle} instabilities arose with two general types of simulation settings: 1) when fewer site classes were simulated than exist in the fitted model, and 2) when different site classes were simulated with similar levels of selection pressure.

When working with real data, often only a single sample is available and alternative techniques must be used to approximate distributions of parameter estimates.  One such technique is the bootstrap.  I used the new bootstrap-based approach with sequence alignments to investigate properties of the \gls{mle} distributions and to detect settings where inference tends to be problematic (see Methods).  While sampling with replacement from a single sample leads to a bootstrap parameter distribution that is a jagged estimate of a smooth distribution, I found the bootstrap, in many cases, can effectively estimate the distributions of \glspl{mle}.  Figure \ref{fig:mle_hists}\subref{sfig:bs_M2a_easy},\subref{sfig:bs_M8_easy} shows the distribution of the $\omega$ \glspl{mle} associated with positive selection generated over 100 bootstrap samples of a \textit{regular} dataset.  Note the resemblance of the bootstrap distributions in figure \ref{fig:mle_hists}\subref{sfig:bs_M2a_easy},\subref{sfig:bs_M8_easy} to the analogous distributions over simulated datasets in figure \ref{fig:mle_hists}\subref{sfig:sim_M2a_easy},\subref{sfig:sim_M8_easy}.  A comparison of figures \ref{fig:mle_hists}\subref{sfig:sim_M2a_diff},\subref{sfig:sim_M8_diff} and \ref{fig:mle_hists}\subref{sfig:bs_M2a_diff},\subref{sfig:bs_M8_diff} illustrates that when the distribution over multiple samples is problematic, so too is the distribution over bootstrap samples.  Among the 100 bootstrap \gls{mle} distributions obtained from the datasets simulated under \textit{irregular} model conditions, I identified 91 of the M2a and 95 of the M8 $p_{\omega>1}$ parameter distributions as unstable using the criterion that at least 5\% mass lies both below 0.2 and above 0.8.  These distributions indicate that the mixture distribution for $\omega$ ``flip-flops'' between few and many sites in a positive selection class.  Recall that under the generating model for these data, no sites are under positive selection.  Plots of the other parameters of the $\omega$ distributions can be found in supplementary figure \ref{fig:mle_p0_p1_w0_hists}.
Scenarios when the bootstrap distribution is not a good estimate of the true distribution of parameter estimates has been described in other settings, e.g., \citet[p. 81]{efron1994introduction}.
So, while the bootstrap alone can be helpful for identifying problems, it is not always a robust solution for deriving a correction for parameter uncertainty.

\subsection{Kernel Smoothing Improves the Bootstrap-based Method for Approximating MLE Distributions}
To avoid results that are a consequence of randomness due to bootstrapping, it is beneficial to choose the number of bootstrap samples, $B$, large enough so that the finite-sample bootstrap distribution approximates the infinite-sample bootstrap distribution well.  However, when regularity conditions are violated there is no guarantee that even the infinite-bootstrap distribution provides an adequate assessment of the variability of an \gls{mle}.  I tested this assertion under codon models where the distributions of the $p_{\omega>1}$ parameters were unstable over simulated and bootstrap datasets.  For the data representing \textit{irregular} model conditions described above, I generated 10,000 bootstrap datasets for each of the first 10 simulated datasets. The instabilities that characterize these 10 bootstrap distributions were largely unchanged by increasing $B$ (supplementary figure \ref{fig:hists-cat1-scheme1-more-bs}).  Similar difficulties arise in a variety of bootstrap applications.  As a simple example of the phenomenon, suppose interest is in $\theta$ from a binomial distribution with small $n$ and small $\theta$.  It is possible to sample almost all zeros, in which case the variance of the bootstrap distribution of $\theta$ estimates will be too small.  Such boundary issues related to small samples can similarly be problematic for $\omega$ distributions when estimated weights are close to 0.

I used kernel smoothing along with bootstrapping to characterize the uncertainty in \glspl{mle} under \emph{difficult} estimation conditions.
Kernel smoothing is typically used to approximate the infinite-sample distribution more effectively when using a smaller number of bootstrap samples.  However, the standard application of this technique \citep[p. 79]{davison1997bootstrap} was not sufficient when the \glspl{mle} were unstable.  For such cases, \textit{over smoothing} (i.e., using a larger than typically considered optimal bandwidth) was necessary to obtain conservative estimates of the \gls{mle} distributions, with larger variance, that suppressed the influence of the instabilities (supplementary figure \ref{fig:before-after-smoothing}).  By over-smoothing the $p$ parameters of codon models M2a and M8 with a uniform kernel I compensated for 1) low information content in the data, 2) fewer bootstrap samples, and 3) instabilities in the parameter estimates.  For this reason I included over-smoothing of the $p$ parameters in all applications of \gls{sba}.

\subsection{Simulation Results}
I used simulation to compare the performance of \gls{sba} with \gls{beb} and \gls{neb}.  The design of our studies was motivated by the more challenging schemes of \citet{wong2004accuracy} and \citet{yang2005bayes}, however our design extends theirs to investigate performance under progressively more model misspecification.  The design is divided into three scenarios covering three levels of model misspecification.  The \textit{Correct Model Scenario} is comprised of four simulation studies (studies $1$-$4$) where the nuisance parameters of the generating model ($\kappa=1$, $\pi_i=1/61$) were freely estimated by the fitted model.  The $\omega$ distributions used to generate the datasets are listed in the third column of table \ref{tab:sim}.

\begin{table}[H]
  \begin{threeparttable}
    \caption{Simulation design and false positive rates under NEB, BEB, and SBA each with models M2a and M8.}
    \centering
    \begin{tabular}[h!]{*{3}l*{6}c}
      \toprule
      Study              & Misspec.           & $\omega$ distribution  & \multicolumn{2}{c}{NEB}       & \multicolumn{2}{c}{BEB}         & \multicolumn{2}{c}{SBA}       \\
      \cmidrule(lr){1-1}   \cmidrule(lr){2-2}   \cmidrule(lr){3-3}       \cmidrule(lr){4-5}              \cmidrule(lr){6-7}                \cmidrule(lr){8-9}
                         &                    &                        & M2a           & M8            & M2a           & M8              & M2a           & M8            \\
      \cmidrule(lr){4-4} \cmidrule(lr){5-5}    \cmidrule(lr){6-6} \cmidrule(lr){7-7}  \cmidrule(lr){8-8} \cmidrule(lr){9-9}
      1                  & None               & 100\% 1                & \textbf{0.34} & \textbf{0.35} & 0.00          & 0.00            & 0.00          & 0.00          \\
      2                  & None               & 50\% 0.5, 50\% 1       & 0.00          & 0.00          & 0.00          & 0.00            & 0.00          & 0.00          \\
      3                  & None               & 50\% 1 50\% 1.5        & \textbf{0.35} & \textbf{0.37} & 0.00          & \textbf{0.05}   & 0.00          & 0.02          \\
      4                  & None               & 45\% 0, 45\% 1, 10\% 5 & 0.00          & 0.00          & 0.00          & 0.01            & 0.00          & 0.00          \\
      \\
      5                  & Mild               & 100\% 1                & \textbf{0.20} & \textbf{0.37} & 0.00          & \textbf{0.24}   & 0.00          & \textbf{0.13} \\
      6                  & Mild               & 50\% 0.5, 50\% 1       & 0.00          & \textbf{0.13} & 0.00          & \textbf{0.11}   & 0.00          & 0.02          \\
      7                  & Mild               & 50\% 1, 50\% 1.5       & \textbf{0.30} & \textbf{0.30} & 0.00          & \textbf{0.39}   & 0.00          & \textbf{0.12} \\
      8                  & Mild               & 45\% 0, 45\% 1, 10\% 5 & 0.00          & 0.04          & 0.00          & \textbf{0.12}   & 0.00          & 0.00          \\
      \\
      9                  & Heavy              & 100\% 1                & \textbf{0.71} & \textbf{0.71} & \textbf{0.55} & \textbf{0.62}   & \textbf{0.13} & \textbf{0.52} \\
      10                 & Heavy              & 50\% 0.5, 50\% 1       & \textbf{0.53} & \textbf{0.50} & 0.00          & 0.00            & 0.00          & 0.01          \\
      \bottomrule
    \end{tabular}
    \label{tab:sim}
    \begin{tablenotes}
      \small
    \item This was for 10,000 simulated datasets with 5-taxa and a total tree length of 3.
    \end{tablenotes}
  \end{threeparttable}
\end{table}

This scenario design matches selected schemes in \citet{yang2005bayes}.  The \textit{Mild Misspecification Scenario} uses the same $\omega$ distribution as the first scenario as the basis of four additional studies (studies $5$-$8$), but includes mild misspecification of the nuisance parameters (see Methods).  Lastly, the \textit{Heavy Misspecification Scenario}, includes two studies (studies $9$-$10$) with heavy misspecification for the fitted model, which represents a more plausible scenario for the analysis of real sequences.  In one study (study 9) the data were simulated using the highly biased codon frequencies from the Drosophila \textit{GstD1} gene \citep{bielawski2005maximum}.  In the second study (study 10), the generating model is based on a 50/50 mixture of two heterogeneous classes of sites.  One class was generated using equal codon frequencies, $\kappa=1$, and $\omega=0.5$, while the other used the Drosophila \textit{GstD1} gene codon frequencies, $\kappa=8$, and $\omega=1$.  For all 10 simulation studies I simulated 100 alignments, each having 500 codons, using the same 5-taxon tree from \citet{wong2004accuracy}.  The studies in the \textit{Correct Model Scenario} were repeated under model M2a with the 30-taxon tree from the same paper.

Table \ref{tab:sim} lists the false positive rates (proportion of sites inferred positively selected among those that are not) using a posterior probability cutoff of $0.95$ for \gls{neb}, \gls{beb}, and \gls{sba} under models M2A and M8.  Study 1 (no misspecification of the nuisance parameters and all sites simulated using $\omega=1$) is an interesting case as \gls{neb} exhibits false positives, while \gls{beb} and \gls{sba} do not.  This is expected; \gls{neb} is known to yield unreliable posterior probability calculations in small datasets \citep[e.g., ][]{anisimova2002accuracy,yang2005bayes}.  Because the conditions of study 1 yield unstable parameter estimates (figure \ref{fig:mle_hists}\subref{sfig:sim_M2a_diff}-\subref{sfig:bs_M8_diff}), the false positives under \gls{neb} reflect more than mere sampling errors.  \gls{mle} instabilities cause large $p_{\omega>1}$ to occur too often and these values lead to high posterior probabilities for positive selection under M2a and M8. The posterior probability calculations under \gls{sba} and \gls{beb} are reliable because those approaches do not assume the \glspl{mle} have been estimated without error.  \citet{yang2005bayes} suggests that with more data, the problems with \gls{neb} controlling false positives can be mitigated.  However, the \gls{mle} instabilities persisted in study 1 using a tree topology with 30 taxa (supplementary figure \ref{fig:sim_M2a_diff_30_taxa}), indicating that large sample sizes do not always ensure accurate predictions.

Relative to simulations with a single $\omega=1$ (study 1), when the $\omega$ distribution was 50\% $\omega=0.5$ and 50\% $\omega=1$ (study 2), the overall signal for positive selection was diminished and all false positive rates were $0$. Conversely, when the $\omega$ distribution was 50\% $\omega=1$ and 50\% $\omega=1.5$ (study 3) there was a slight increase in the \gls{neb} false positive rates relative to study 1. Under M2a the false positive rates were $0$ using \gls{beb} and \gls{sba}, but under M8 they increased to $0.05$ using \gls{beb} and to $0.02$ using \gls{sba}.  For study 4, because the simulated $\omega$ values for the three sites classes were far enough apart, the false positive rates were well controlled.

%\subsubsection*{The Impact of Model Misspecification}
The introduction of mild model misspecification of the nuisance parameters did not result in higher false positive rates under M2a, but did under M8.  For studies $5$-$8$, the \gls{beb} false positive rates (using a $0.95$ posterior probability threshold) under M8 increased in all four cases relative to the corresponding studies ($1$-$4$) in the \textit{Correct Model Scenario}.  The same \gls{sba} false positive rates only increased in two cases and by smaller amounts than with \gls{beb}.  When heavy model misspecification was introduced in the third scenario, \gls{neb} failed to adequately control false positives with rates between 50 and 71\% under both M2a and M8.  \gls{beb} and \gls{sba} also did not control the false positive rates in study 9, but did in study 10.

The results in table \ref{tab:sim} are over all sites in all simulated datasets.  After applying \gls{lr} tests at the 0.05 level to filter datasets that convey no evidence of positive selection, none of the false positive rates under \gls{beb} or \gls{sba} changed.  Supplementary table \ref{tab:simlrt} gives the false positive rates under \gls{neb} after the adjustment.  With the exception of two cases, the effect is minimal.  Interestingly, under the null hypothesis, the false positive rates of the \gls{lr} tests were larger than expected, particularly with model misspecification.

When testing for positive selection, we aim for large true positive rates, the proportion of sites truly under positive selection that are correctly identified, sometimes referred to as power.  A difficulty in comparing methods for detecting positive selection is the choice of threshold.  Lower thresholds tend to increase the true positive rate, but tend to also increase the false positive rate.  To ensure that comparisons of power for different methods correspond to the same false positive rate I used \gls{roc} curves, a convenient way to visualize the balance between accuracy and power for classification problems.  Each point on a curve represents a threshold for the posterior probability of positive selection.  Figure \ref{fig:roc} shows \gls{roc} curves for each of the simulations that included positive selection in the generating model (studies 3, 4, 7, and 8).  Curves are also included for the classification of sites using the generating parameters, i.e., the \glspl{mle} are fixed to the simulated values.  These curves represent an expected upper limit in performance of site classification (supplementary section \ref{sec:roc_opt}).  The lower limit for classification, when each site is randomly identified to be under positive selection, is represented by a $y=x$ line.

\begin{figure}[H]
    \centering
  \begin{subfigure}[t]{.48\textwidth}
    \centering
    \hspace{10 mm} \small{$50\%$ $\omega=1$, $50\%$ $\omega=1.5$}
  \end{subfigure}
  \begin{subfigure}[t]{.49\textwidth}
    \centering
    \hspace{10 mm} \small{$45\%$ $\omega=0$, $45\%$ $\omega=1$, $10\%$ $\omega=5$}
  \end{subfigure}
  \\[-2.6ex]
  \begin{subfigure}[t]{.03\textwidth}
    \vspace{8 mm}
    \rotatebox{90}{\textit{\small{Mild Misspec.}} \hspace{4 mm} \textit{\small{Correct Model}}}
  \end{subfigure}
  \begin{subfigure}[t]{0.465\textwidth}
    \centering
    <<echo=F,warning=F>>=

    c1s3.ft.fp     <- scan("../sba.git/doc/paper/data/c1s3_opt_fp.csv",sep=',')
    c1s3.ft.tp     <- scan("../sba.git/doc/paper/data/c1s3_opt_tp.csv",sep=',')

    c1s3m2a.beb.fp <- scan("../sba.git/doc/paper/data/c1s3_m2a_beb_fp.csv",sep=',')
    c1s3m2a.neb.fp <- scan("../sba.git/doc/paper/data/c1s3_m2a_neb_fp.csv",sep=',')
    c1s3m2a.sba.fp <- scan("../sba.git/doc/paper/data/c1s3_m2a_sba_fp.csv",sep=',')
    c1s3m2a.beb.tp <- scan("../sba.git/doc/paper/data/c1s3_m2a_beb_tp.csv",sep=',')
    c1s3m2a.neb.tp <- scan("../sba.git/doc/paper/data/c1s3_m2a_neb_tp.csv",sep=',')
    c1s3m2a.sba.tp <- scan("../sba.git/doc/paper/data/c1s3_m2a_sba_tp.csv",sep=',')

    c1s3m8.beb.fp  <- scan("../sba.git/doc/paper/data/c1s3_m8_beb_fp.csv",sep=',')
    c1s3m8.neb.fp  <- scan("../sba.git/doc/paper/data/c1s3_m8_neb_fp.csv",sep=',')
    c1s3m8.sba.fp  <- scan("../sba.git/doc/paper/data/c1s3_m8_sba_fp.csv",sep=',')
    c1s3m8.beb.tp  <- scan("../sba.git/doc/paper/data/c1s3_m8_beb_tp.csv",sep=',')
    c1s3m8.neb.tp  <- scan("../sba.git/doc/paper/data/c1s3_m8_neb_tp.csv",sep=',')
    c1s3m8.sba.tp  <- scan("../sba.git/doc/paper/data/c1s3_m8_sba_tp.csv",sep=',')

    c2s7.ft.fp     <- scan("../sba.git/doc/paper/data/c2s7_opt_fp.csv",sep=',')
    c2s7.ft.tp     <- scan("../sba.git/doc/paper/data/c2s7_opt_tp.csv",sep=',')

    c2s7m2a.beb.fp <- scan("../sba.git/doc/paper/data/c2s7_m2a_beb_fp.csv",sep=',')
    c2s7m2a.neb.fp <- scan("../sba.git/doc/paper/data/c2s7_m2a_neb_fp.csv",sep=',')
    c2s7m2a.sba.fp <- scan("../sba.git/doc/paper/data/c2s7_m2a_sba_fp.csv",sep=',')
    c2s7m2a.beb.tp <- scan("../sba.git/doc/paper/data/c2s7_m2a_beb_tp.csv",sep=',')
    c2s7m2a.neb.tp <- scan("../sba.git/doc/paper/data/c2s7_m2a_neb_tp.csv",sep=',')
    c2s7m2a.sba.tp <- scan("../sba.git/doc/paper/data/c2s7_m2a_sba_tp.csv",sep=',')

    c2s7m8.beb.fp  <- scan("../sba.git/doc/paper/data/c2s7_m8_beb_fp.csv",sep=',')
    c2s7m8.neb.fp  <- scan("../sba.git/doc/paper/data/c2s7_m8_neb_fp.csv",sep=',')
    c2s7m8.sba.fp  <- scan("../sba.git/doc/paper/data/c2s7_m8_sba_fp.csv",sep=',')
    c2s7m8.beb.tp  <- scan("../sba.git/doc/paper/data/c2s7_m8_beb_tp.csv",sep=',')
    c2s7m8.neb.tp  <- scan("../sba.git/doc/paper/data/c2s7_m8_neb_tp.csv",sep=',')
    c2s7m8.sba.tp  <- scan("../sba.git/doc/paper/data/c2s7_m8_sba_tp.csv",sep=',')

    thin <- floor(seq(1,length(c1s3.ft.fp),length=(length(c1s3.ft.fp)-1)/100))

    fp <- c(c1s3.ft.fp[thin],c1s3m2a.neb.fp[thin],c1s3m2a.sba.fp[thin],c1s3m2a.beb.fp[thin])
    fp <- c(fp,c1s3.ft.fp[thin],c1s3m8.neb.fp[thin],c1s3m8.sba.fp[thin],c1s3m8.beb.fp[thin])
    fp <- c(fp,c2s7.ft.fp[thin],c2s7m2a.neb.fp[thin],c2s7m2a.sba.fp[thin],c2s7m2a.beb.fp[thin])
    fp <- c(fp,c2s7.ft.fp[thin],c2s7m8.neb.fp[thin],c2s7m8.sba.fp[thin],c2s7m8.beb.fp[thin])

    tp <- c(c1s3.ft.tp[thin],c1s3m2a.neb.tp[thin],c1s3m2a.sba.tp[thin],c1s3m2a.beb.tp[thin])
    tp <- c(tp,c1s3.ft.tp[thin],c1s3m8.neb.tp[thin],c1s3m8.sba.tp[thin],c1s3m8.beb.tp[thin])
    tp <- c(tp,c2s7.ft.tp[thin],c2s7m2a.neb.tp[thin],c2s7m2a.sba.tp[thin],c2s7m2a.beb.tp[thin])
    tp <- c(tp,c2s7.ft.tp[thin],c2s7m8.neb.tp[thin],c2s7m8.sba.tp[thin],c2s7m8.beb.tp[thin])

    roc.data <- data.frame(fp,tp,
                               Method=rep(c("OPT","NEB","SBA","BEB"),
                                              each=length(thin),times=4),
                               Study=rep(c('Study 3 - M2a',
                                               'Study 3 - M8',
                                               'Study 7 - M2a',
                                               'Study 7 - M8'),
                                             each=length(thin)*4),
                               Scenario=rep(c('CM','MM'),
                                                each=length(thin)*8))

    roc.plot <- ggplot(roc.data,aes(fp,tp)) +
        coord_cartesian(xlim=c(-0.005,.405), ylim=c(-0.005,.405)) +
        labs(x="False Positive Rate",y="True Positive Rate") +
        geom_line(aes(color=Method,linetype=Method),size=1.2) +
        geom_abline(slope=1,intercept=0,color='gray60') +
        scale_color_manual(values=c("gray30","grey40","grey50","black")) +
        scale_linetype_manual(values=c("dotted","dotdash","dashed","solid")) +
        scale_x_continuous(breaks=c(0.1,0.2,0.3)) +
        facet_wrap(~Study,ncol=2)

    roc.plot +
            theme(panel.margin=unit(0,"lines"),
                  panel.background=element_blank(),
                  strip.background=element_blank(),
                  panel.grid.major=element_line(color="gray95"),
                  legend.position="none",
                  axis.line=element_line(colour="black"),
                  text=element_text(size=20),
                  panel.border = element_rect(colour = "black", fill=NA, size=1))
@
    \label{sfig:roc_hard}
  \end{subfigure}
  \begin{subfigure}[t]{0.48\textwidth}
    \centering
    <<echo=F,warning=F>>=

    c1s4.ft.fp     <- scan("../sba.git/doc/paper/data/c1s4_opt_fp.csv",sep=',')
    c1s4.ft.tp     <- scan("../sba.git/doc/paper/data/c1s4_opt_tp.csv",sep=',')

    c1s4m2a.beb.fp <- scan("../sba.git/doc/paper/data/c1s4_m2a_beb_fp.csv",sep=',')
    c1s4m2a.neb.fp <- scan("../sba.git/doc/paper/data/c1s4_m2a_neb_fp.csv",sep=',')
    c1s4m2a.sba.fp <- scan("../sba.git/doc/paper/data/c1s4_m2a_sba_fp.csv",sep=',')
    c1s4m2a.beb.tp <- scan("../sba.git/doc/paper/data/c1s4_m2a_beb_tp.csv",sep=',')
    c1s4m2a.neb.tp <- scan("../sba.git/doc/paper/data/c1s4_m2a_neb_tp.csv",sep=',')
    c1s4m2a.sba.tp <- scan("../sba.git/doc/paper/data/c1s4_m2a_sba_tp.csv",sep=',')

    c1s4m8.beb.fp <-  scan("../sba.git/doc/paper/data/c1s4_m8_beb_fp.csv",sep=',')
    c1s4m8.neb.fp <-  scan("../sba.git/doc/paper/data/c1s4_m8_neb_fp.csv",sep=',')
    c1s4m8.sba.fp <-  scan("../sba.git/doc/paper/data/c1s4_m8_sba_fp.csv",sep=',')
    c1s4m8.beb.tp <-  scan("../sba.git/doc/paper/data/c1s4_m8_beb_tp.csv",sep=',')
    c1s4m8.neb.tp <-  scan("../sba.git/doc/paper/data/c1s4_m8_neb_tp.csv",sep=',')
    c1s4m8.sba.tp <-  scan("../sba.git/doc/paper/data/c1s4_m8_sba_tp.csv",sep=',')

    c2s8.ft.fp  <-    scan("../sba.git/doc/paper/data/c2s8_opt_fp.csv",sep=',')
    c2s8.ft.tp  <-    scan("../sba.git/doc/paper/data/c2s8_opt_tp.csv",sep=',')

    c2s8m2a.beb.fp <- scan("../sba.git/doc/paper/data/c2s8_m2a_beb_fp.csv",sep=',')
    c2s8m2a.neb.fp <- scan("../sba.git/doc/paper/data/c2s8_m2a_neb_fp.csv",sep=',')
    c2s8m2a.sba.fp <- scan("../sba.git/doc/paper/data/c2s8_m2a_sba_fp.csv",sep=',')
    c2s8m2a.beb.tp <- scan("../sba.git/doc/paper/data/c2s8_m2a_beb_tp.csv",sep=',')
    c2s8m2a.neb.tp <- scan("../sba.git/doc/paper/data/c2s8_m2a_neb_tp.csv",sep=',')
    c2s8m2a.sba.tp <- scan("../sba.git/doc/paper/data/c2s8_m2a_sba_tp.csv",sep=',')

    c2s8m8.beb.fp <-  scan("../sba.git/doc/paper/data/c2s8_m8_beb_fp.csv",sep=',')
    c2s8m8.neb.fp <-  scan("../sba.git/doc/paper/data/c2s8_m8_neb_fp.csv",sep=',')
    c2s8m8.sba.fp <-  scan("../sba.git/doc/paper/data/c2s8_m8_sba_fp.csv",sep=',')
    c2s8m8.beb.tp <-  scan("../sba.git/doc/paper/data/c2s8_m8_beb_tp.csv",sep=',')
    c2s8m8.neb.tp <-  scan("../sba.git/doc/paper/data/c2s8_m8_neb_tp.csv",sep=',')
    c2s8m8.sba.tp <-  scan("../sba.git/doc/paper/data/c2s8_m8_sba_tp.csv",sep=',')

    thin <- floor(seq(1,length(c1s4.ft.fp),length=(length(c1s4.ft.fp)-1)/100))

    fp <- c(c1s4.ft.fp[thin],c1s4m2a.neb.fp[thin],c1s4m2a.sba.fp[thin],c1s4m2a.beb.fp[thin])
    fp <- c(fp,c1s4.ft.fp[thin],c1s4m8.neb.fp[thin],c1s4m8.sba.fp[thin],c1s4m8.beb.fp[thin])
    fp <- c(fp,c2s8.ft.fp[thin],c2s8m2a.neb.fp[thin],c2s8m2a.sba.fp[thin],c2s8m2a.beb.fp[thin])
    fp <- c(fp,c2s8.ft.fp[thin],c2s8m8.neb.fp[thin],c2s8m8.sba.fp[thin],c2s8m8.beb.fp[thin])

    tp <- c(c1s4.ft.tp[thin],c1s4m2a.neb.tp[thin],c1s4m2a.sba.tp[thin],c1s4m2a.beb.tp[thin])
    tp <- c(tp,c1s4.ft.tp[thin],c1s4m8.neb.tp[thin],c1s4m8.sba.tp[thin],c1s4m8.beb.tp[thin])
    tp <- c(tp,c2s8.ft.tp[thin],c2s8m2a.neb.tp[thin],c2s8m2a.sba.tp[thin],c2s8m2a.beb.tp[thin])
    tp <- c(tp,c2s8.ft.tp[thin],c2s8m8.neb.tp[thin],c2s8m8.sba.tp[thin],c2s8m8.beb.tp[thin])


    roc.data <- data.frame(fp,tp,
                               Method=rep(c("OPT","NEB","SBA","BEB"),
                                              each=length(thin),times=4),
                               Study=rep(c('Study 4 - M2a',
                                               'Study 4 - M8',
                                               'Study 8 - M2a',
                                               'Study 8 - M8'),
                                             each=length(thin)*4),
                               Scenario=rep(c('CM','MM'),
                                                each=length(thin)*8))

    roc.plot <- ggplot(roc.data,aes(fp,tp)) +
        coord_cartesian(xlim=c(-0.005,.1),ylim=c(-0.005,.805)) +
        labs(x="False Positive Rate",y="True Positive Rate") +
        geom_line(aes(color=Method,linetype=Method),size=1.2) +
        geom_abline(slope=1,intercept=0,color='gray60') +
        scale_shape_manual(values=c(15,18,17,16)) +
        scale_color_manual(values=c("gray30","grey40","grey50","black")) +
        scale_linetype_manual(values=c("dotted","dotdash","dashed","solid")) +
        scale_x_continuous(breaks=c(0.02,0.04,0.06,0.08)) +
        guides(colour = guide_legend(override.aes = list(size=1.4))) +
        facet_wrap(~Study,ncol=2)

    roc.plot +
        theme(panel.background=element_blank(),
              strip.background=element_blank(),
              panel.grid.major=element_line(colour="gray95"),
              panel.margin=unit(0,"lines"),
              axis.line=element_line(colour="black"),
              legend.title=element_blank(),
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.83,.80),
              legend.key.width=unit(6,"line"),
              text=element_text(size=20),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
    %\caption{Difficult with Model Misspecification}
    \label{sfig:roc_easy}
  \end{subfigure}
  \caption[ROC curves for the detection of sites under positive selection for BEB, NEB, and SBA analyses of data generated under two different simulation scenarios: without model misspecification (\textit{Correct Model}, studies 3 and 4) and with mild model misspecification (\textit{Mild Misspecification}, studies 7 and 8)]{ROC curves for the detection of sites under positive selection for BEB, NEB, and SBA analyses of data generated under two different simulation scenarios: without model misspecification (\textit{Correct Model}, studies 3 and 4) and with mild model misspecification (\textit{Mild Misspecification}, studies 7 and 8).  The data were simulated using a 5-taxon tree topology.  In studies 3 and 7, $50\%$ of the sites were simulated under neutral evolution ($\omega=1$) and $50\%$ of the sites under positive selection ($\omega=1.5$).  In studies 4 and 8, $45\%$ of the sites were simulated under purifying selection ($\omega=0$), $45\%$ under neutral evolution ($\omega=1$) and $10\%$ under positive selection ($\omega=5$).  Each plot includes a line for the lower bound (y=x) and an expected upper bound (OPT) when classification is made using the generating model parameters.  Curves for NEB do not always cover the whole range of false positive rates, because NEB sometimes estimates the $\omega$ distribution with all mass on $\omega > 1$.  In these cases, even with a posterior probability cut-off of 1, NEB still incorrectly classifies sites to be under positive selection.}
  \label{fig:roc}
\end{figure}

The introduction of mild misspecification made the task of detecting sites under positive selection more difficult in study 8.  This is evident from the shifting of the \gls{roc} curves down and to the right (lower rates of true positives for a given false positive rate) in study 8 relative to the corresponding simulations without the misspecification of the nuisance parameters in study 4.  The same effect was not observed between the \gls{roc} curves of studies 3 and 7.

In all cases, the \gls{sba} curves were at least as close as the \gls{beb} curves to the expected upper limit.  In studies 3 and 7 (50\% $\omega=1$, 50\% $\omega=1.5$), under M2a, where the estimates of the $p_{\omega>1}$ and $\omega_{>1}$ parameters were unstable (supplementary figure \ref{fig:roc_lrt}), the gaps between the curves for \gls{beb} and \gls{sba} were the largest, even when the number of taxa was increased from 5 to 30 (figure \ref{fig:roc_30_taxa}).  This indicates that \gls{sba}, for a given false positive rate, had more power to detect sites under positive selection than \gls{beb}.
<<ROC_30_data,echo=F>>=
c1s3.30.ft.fp  <-    scan("../sba.git/doc/paper/data/c1s3_30t_opt_fp.csv",sep=',')
c1s3.30.ft.tp  <-    scan("../sba.git/doc/paper/data/c1s3_30t_opt_tp.csv",sep=',')

c1s3m2a.30.beb.fp <- scan("../sba.git/doc/paper/data/c1s3_30t_m2a_beb_fp.csv",sep=',')
c1s3m2a.30.neb.fp <- scan("../sba.git/doc/paper/data/c1s3_30t_m2a_neb_fp.csv",sep=',')
c1s3m2a.30.sba.fp <- scan("../sba.git/doc/paper/data/c1s3_30t_m2a_sba_fp.csv",sep=',')

c1s3m2a.30.beb.tp <- scan("../sba.git/doc/paper/data/c1s3_30t_m2a_beb_tp.csv",sep=',')
c1s3m2a.30.neb.tp <- scan("../sba.git/doc/paper/data/c1s3_30t_m2a_neb_tp.csv",sep=',')
c1s3m2a.30.sba.tp <- scan("../sba.git/doc/paper/data/c1s3_30t_m2a_sba_tp.csv",sep=',')

thin <- floor(seq(1,length(c1s3.30.ft.fp),length=(length(c1s3.30.ft.fp)-1)/100))

fp <- c(c1s3.30.ft.fp[thin],c1s3m2a.30.neb.fp[thin],c1s3m2a.30.sba.fp[thin],c1s3m2a.30.beb.fp[thin])
tp <- c(c1s3.30.ft.tp[thin],c1s3m2a.30.neb.tp[thin],c1s3m2a.30.sba.tp[thin],c1s3m2a.30.beb.tp[thin])

roc.data <- data.frame(fp,tp,
                       Method=rep(c("OPT","NEB","SBA","BEB"),each=length(thin)))

title <- expression(paste("Study 3 (50% ",omega,"=1.0 50% ",omega,"=15) - 30 taxa - M2a"))

roc.plot <- ggplot(roc.data,aes(fp,tp)) +
    ggtitle(title) +
    geom_line(aes(color=Method,linetype=Method),size=1.2) +
    coord_cartesian(xlim=c(-0.005,.301), ylim=c(-0.005,.301)) +
    labs(x="False Positive Rate",y="True Positive Rate") +
    geom_abline(slope=1,intercept=0,color='gray60') +
    scale_shape_manual(values=c(15,18,17,16)) +
    ##scale_color_manual(values=c("#F8766D","#C77CFF","#00BFC4","#7CAE00")) +
    scale_color_manual(values=c("gray30","grey40","grey50","black")) +
    scale_linetype_manual(values=c("dotted","dotdash","dashed","solid"))
@
\begin{figure}[H]
    \centering
    <<echo=F>>=
    roc.plot +
        theme(plot.title=element_text(size=18),
              panel.background=element_blank(),
              panel.spacing=unit(2,"lines"),
              panel.grid.major=element_line(color="gray95"),
              legend.title=element_blank(),
              legend.position=c(0.88,0.65),
              legend.key=element_rect(fill = "transparent"),
              legend.key.width = unit(4,"line"),
              axis.line=element_line(colour="black"),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
    @
    \caption[ROC curves for the detection of sites under positive selection for BEB, NEB, and SBA analyses of data generated under \textit{Correct Model}, study 3 ($50\%$ $\omega=1$, $50\%$ $\omega=1.5$)]{ROC curves for the detection of sites under positive selection for BEB, NEB, and SBA analyses of data generated under \textit{Correct Model}, study 3 ($50\%$ $\omega=1$, $50\%$ $\omega=1.5$).  The data were simulated using a 30-taxon tree topology. The plot includes a curve for the lower bound (y=x) and an expected upper bound (OPT) when classification is made using the generating model parameters.  The curves for NEB do not always cover the whole range of false positive rates, because NEB sometimes estimates the $\omega$ distribution with all mass on $\omega > 1$.  In these cases, even with a posterior probability cut-off of 1, NEB still incorrectly classifies sites to be under positive selection.}
    \label{fig:roc_30_taxa}
\end{figure}

In studies 4 and 8 (45\% $\omega=0$, 45\% $\omega=1$, 10\% $\omega=5$), where the parameters of the $\omega$ distribution were well estimated, all approaches (\gls{neb}, \gls{beb}, and \gls{sba}) performed well and the \gls{roc} curves were all close to the expected upper limit.  Taken together, the results suggest that \gls{sba} balances accuracy and power at least as well as \gls{beb} and may be preferable to \gls{beb} when parameter estimates are unstable.

\subsection{Real Data Analysis}
I began the analysis of the 16 real datasets (described in Methods and summarized in table \ref{tab:realDataResultsOverview}) by using the bootstrap distributions of the \glspl{mle} to investigate their properties.  I examined the unsmoothed distributions of the parameters of the $\omega$ distribution.  These distributions indicate that the \glspl{mle} for a given model can have very different properties in different real datasets (supplementary figures \ref{fig:real_genes_m2a_p0_w0_mles}, \ref{fig:real_genes_m2a_p1_mles}, \ref{fig:real_genes_m2a_mles}, \ref{fig:real_genes_m8_mles}).  Although the real data represent different degrees of \textit{regular} and \textit{irregular} model properties, I was able to identify groups of genes that represent both extremes.  The \textit{regular} cases had no clear evidence of \gls{mle} instabilities and low bootstrap variance (e.g., lysin; figure \ref{fig:hists_lysin_CDH3_mles}\subref{sfig:lysin_m2a_mles},\subref{sfig:lysin_m8_mles}).  I determined that the $\omega$ distributions had been well estimated for 6 genes (\textit{pol}, \textit{vif}, lysin, \textit{nuoL3}, \textit{RafL}, and \textit{TrbL-VirB6\_3}).  In contrast, I uncovered evidence of \gls{mle} instabilities in other genes (e.g., \textit{CDH3}; figure \ref{fig:hists_lysin_CDH3_mles}\subref{sfig:CDH3_m2a_mles},\subref{sfig:CDH3_m8_mles}).  I determined that the $\omega$ distributions had been poorly estimated for 5 genes (\textit{CDH3}, \textit{mivN}, \textit{pgpA}, \textit{tax}, and \textit{TrbL-VirB6\_2}) under at least one model.  Because no single summary statistic (number of taxa, sequence length, tree length) was generally predictive of \textit{irregular} model properties, I recommend visual inspection of the bootstrap distributions for all real data analyses (supplementary figures \ref{fig:real_genes_m2a_mles}, \ref{fig:real_genes_m8_mles}).

\begin{sidewaystable}
  \begin{threeparttable}
    \caption{Genes analyzed under models M2a and M8 using NEB, BEB, and SBA approaches for site classification.}
    \centering
    \begin{tabular}[]{*{10}l}
      \toprule
      &       &       & \multicolumn{2}{c}{-lnL}          &                   &           &                \\
      \cmidrule(lr){4-5}
      \multicolumn{1}{c}{Gene} & \multicolumn{1}{c}{$N_t$} & \multicolumn{1}{c}{$N_c$} & \multicolumn{1}{c}{M1a/M2a} & \multicolumn{1}{c}{M7/M8} & \multicolumn{1}{c}{p-value} & \multicolumn{1}{c}{TTL} & \multicolumn{1}{c}{$N_s$} \\
      \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \cmidrule(lr){8-8}
      $\beta$-globin         & 17    & 144   & 3716.14/3712.55 & 3697.22/3686.13 & 0.0275/1.53e-5    & 8.40/8.57 & 0(0)/3(4) \\
      \textit{ccmF}          & 5     & 635   & 6121.78/6113.57 & 6127.62/6116.48 & 2.72e-4/1.46e-5   & 5.60/3.03 & 3(2)/3(5) \\
      \textit{CDH3}          & 11    & 176   & 5629.97/5623.37 & 5630.66/5623.88 & 1.35e-3 /1.14e-3  & 0.56/0.56 & 1(1)/1(1) \\
      \textit{ENAM}          & 11    & 1142  & 7514.30/7509.28 & 7609.16/7605.74 & 6.61e-3/0.0327    & 0.46/0.56 & 1(1)/2(1) \\
      \textit{env}           & 13    & 91    & 1114.64/1106.45 & 1115.40/1106.39 & 2.76e-4/1.23e-4   & 2.04/2.04 & 2(2)/2(4) \\
      lysin                  & 25    & 134   & 4472.65/4410.28 & 4472.16/4410.57 & 2.86e-14/0.00     & 8.81/8.82 & 22(22)/23(23) \\
      \textit{mivN}          & 5     & 504   & 3383.45/3832.93 & 3834.69/3831.44 & 0.595/0.0388      & 1.62/1.60 & 0(0)/1(1) \\
      \textit{nuoL3}         & 5     & 499   & 5006.16/4978.97 & 5011.37/4977.19 & 1.56e-12/1.44e-15 & 4.58/4.49 & 9(8)/10(10) \\
      \textit{perM}          & 5     & 351   & 2619.88/2619.43 & 2621.64/2617.94 & 0.638/0.0247      & 1.78/1.80 & 0(0)/2(0) \\
      \textit{pgpA}          & 5     & 198   & 1541.27/1539.29 & 1542.65/1538.91 & 0.138/0.0238      & 2.93/2.23 & 1(0)/1(1) \\
      \textit{pol}           & 23    & 947   & 9394.05/9363.96 & 9405.74/9365.88 & 8.52e-14/0.00     & 1.31/1.30 & 6(6)/10(13) \\
      \textit{RfaL}          & 5     & 403   & 3964.89/3955.34 & 3970.38/3955.44 & 7.16e-05/3.23e-7  & 3.46/3.46 & 2(1)/4(3) \\
      \textit{tax}           & 20    & 181   & 895.50/892.02   & 895.50/892.02   & 0.0309/0.0309     & 0.13/0.13 & 181(0)/181(21) \\
      \textit{TrbL-VirB6\_2} & 5     & 657   & 5492.55/5492.52 & 5301.23/5286.43 & 0.976/3.74e-7     & 2.12/2.10 & 0(0)/1(0) \\
      \textit{TrbL-VirB6\_3} & 5     & 938   & 8305.65/8288.36 & 8307.06/8269.09 & 3.09e-8/0.00      & 3.06/3.02 & 3(2)/18(11) \\
      \textit{vif}           & 29    & 192   & 3393.83/3367.86 & 3400.45/3370.66 & 2.29e-06/1.16e-13 & 2.90/2.91 & 10(8)/10(10) \\
      \bottomrule
    \end{tabular}
    \label{tab:realDataResultsOverview2}
    \begin{tablenotes}
      \small
    \item $N_t$: number of taxa, $N_c$: sequence length in number of codons, -lnL: -log likelihood for each nested model pair, p-value of the \gls{lr} test for the presence of positive selection, TTL: total tree length estimated under M2a/M8, $N_s$: number of sites classified to under positive selection using a posterior probability threshold of 0.95 under M2a/M8 for NEB(\gls{beb}).
    \end{tablenotes}
  \end{threeparttable}
\end{sidewaystable}

<<lysin,echo=F>>=
lysin.m2a.params <- read.csv("../sba.git/doc/real_data_results/mles/lysin_m2a_mles.csv",header=F)
lysin.m2a.omegas <- c(as.matrix(lysin.m2a.params[,5:7]))
lysin.m2a.omegas.order <- order(lysin.m2a.omegas)
lysin.m2a.omegas <- lysin.m2a.omegas[lysin.m2a.omegas.order]
lysin.m2a.ps.cumsum <- cumsum(c(as.matrix(lysin.m2a.params[,2:4]))[lysin.m2a.omegas.order])

lysin.m8.params <- read.csv("../sba.git/doc/real_data_results/mles/lysin_m8_mles.csv",header=F)
@

<<CDH3,echo=F>>=
CDH3.m2a.params <- read.csv("../sba.git/doc/real_data_results/mles/CDH3_m2a_mles.csv",header=F)
CDH3.m2a.omegas <- c(as.matrix(CDH3.m2a.params[,5:7]))
CDH3.m2a.omegas.order <- order(CDH3.m2a.omegas)
CDH3.m2a.omegas <- CDH3.m2a.omegas[CDH3.m2a.omegas.order]
CDH3.m2a.ps.cumsum <- cumsum(c(as.matrix(CDH3.m2a.params[,2:4]))[CDH3.m2a.omegas.order])

CDH3.m8.params <- read.csv("../sba.git/doc/real_data_results/mles/CDH3_m8_mles.csv",header=F)
@

\clearpage

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(lysin.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(lysin.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M2a}
    \label{sfig:lysin_m2a_mles}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(lysin.m8.params[,12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    axis(1,cex.axis=3,padj=1)
    hist(lysin.m8.params[,23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:lysin_m8_mles}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(CDH3.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(CDH3.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M2a}
    \label{sfig:CDH3_m2a_mles}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,4))
    hist(CDH3.m8.params[,12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(CDH3.m8.params[,23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:CDH3_m8_mles}
  \end{subfigure}
  \caption[MLE distributions over bootstrap datasets for the lysin and \textit{CDH3} genes]{MLE distributions over bootstrap datasets for the lysin and \textit{CDH3} genes.  The distributions of the $p_{\omega>1}$ and $\omega_{>1}$ parameters associated with positive selection were estimated under models M2a and M8 for each of 100 bootstrap datasets.}
  \label{fig:hists_lysin_CDH3_mles}
\end{figure}

Next I investigated the degree to which the real data results obtained under \gls{beb}, \gls{neb}, and \gls{sba} were consistent with each other.  This is challenging, because the posterior probability thresholds for site classification are not calibrated to give comparable false positive rates.  Our solution was to measure the rank correlations of the site-specific posterior probability scores for positive selection between methods (\gls{beb}, \gls{neb}, and \gls{sba}).  As there are a large number of pairwise comparisons, I took the mean relationship between methods for both the genes representing \textit{regular} and \textit{irregular} model estimation (table \ref{tab:method_cors}).
<<sp_cors,echo=F>>=
## order for good genes: HIVpol, HIVvif, Lysin, nuoL3, RafL, TrbL-VirB3
sp.cor.good.m2a.neb.beb <- c(0.91,1.00,1.00,0.98,1.00,1.00)
sp.cor.good.m2a.neb.sba <- c(0.76,0.97,0.99,0.98,0.96,0.97)
sp.cor.good.m2a.beb.sba <- c(0.85,0.98,0.99,0.99,0.97,0.97)

sp.cor.good.m8.neb.beb <- c(0.97,0.99,1.00,1.00,0.99,1.00)
sp.cor.good.m8.neb.sba <- c(0.88,0.97,0.99,0.98,0.98,0.96)
sp.cor.good.m8.beb.sba <- c(0.95,0.99,1.00,0.99,0.99,0.94)

## order for bad genes: CDH3, MivN, pgpA, tax, TrbL-VirB2
## can't add tax, because all Prs are 1 for NEB
sp.cor.bad.m2a.neb.beb <- c(0.4,0.77,0.72,0.72)
sp.cor.bad.m2a.neb.sba <- c(0.4,0.78,0.72,0.72)
sp.cor.bad.m2a.beb.sba <- c(1.0,0.96,0.98,0.98)

sp.cor.bad.m8.neb.beb <- c(0.4,0.97,1.00,1.00)
sp.cor.bad.m8.neb.sba <- c(0.4,0.91,0.95,0.98)
sp.cor.bad.m8.beb.sba <- c(1.0,0.97,0.95,0.98)

## order for good genes: HIVpol, HIVvif, Lysin, nuoL3, RafL, TrbL-VirB3
sp.cor.good.neb <- c(0.91,1.00,1.00,1.00,0.99,0.97)
sp.cor.good.beb <- c(0.98,1.00,0.99,0.99,0.99,0.97)
sp.cor.good.sba <- c(1.00,1.00,1.00,1.00,1.00,1.00)

## order for bad genes: CDH3, MivN, pgpA, tax, TrbL-VirB2
sp.cor.bad.neb <- c(1.0,0.78,0.72,0.73)
sp.cor.bad.beb <- c(1.0,1.00,0.98,1.00)
sp.cor.bad.sba <- c(1.0,0.98,1.00,0.99)
@
\begin{table}[H]
  \centering
  \begin{threeparttable}
    \caption{Spearman rank correlations between site posterior probabilities for different forms of classification.}
    \centering
    \begin{tabular}[h!]{l*{4}c}
      \toprule
      & \multicolumn{2}{c}{{\bf \textit{Regular}}} & \multicolumn{2}{c}{{\bf \textit{Irregular}}} \\
      \cmidrule(lr){2-3} \cmidrule(lr){4-5}
      & mean      & SD        & mean      & SD \\
      \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
      M2a                    &           &           &           &        \\
      \hspace{1em} NEB/BEB   & 0.98      & 0.04      & 0.65      & 0.17   \\
      \hspace{1em} NEB/SBA   & 0.94      & 0.09      & 0.66      & 0.17   \\
      \hspace{1em} BEB/SBA   & 0.96      & 0.05      & 0.98      & 0.02   \\
      M8                     &           &           &           &        \\
      \hspace{1em} NEB/BEB   & 0.99      & 0.01      & 0.84      & 0.30   \\
      \hspace{1em} NEB/SBA   & 0.96      & 0.04      & 0.81      & 0.27   \\
      \hspace{1em} BEB/SBA   & 0.98      & 0.03      & 0.98      & 0.02   \\
      \bottomrule
    \end{tabular}
    \label{tab:method_cors}
    \begin{tablenotes}
      \small
    \item The mean and standard deviation (SD) of the correlations are for real genes displaying \textit{regular} and \textit{irregular} estimation properties.
    \end{tablenotes}
  \end{threeparttable}
\end{table}
I found that when \glspl{mle} are well estimated (\textit{regular} genes), there is stronger agreement among all three methods in the ranking of sites according to the signal for positive selection.  In contrast, when the $\omega$ distributions are poorly estimated (genes representing \textit{irregular} estimation), \gls{beb} and \gls{sba} are generally consistent in their rankings, but differ from \gls{neb}.  These results suggest that \gls{neb}'s inability to accommodate \gls{mle} uncertainty in such datasets has the largest effect on the posteriors.  However, the problem of calibration remains.  Our simulation studies revealed that using a common posterior probability threshold for classification does not guarantee a similar trade-off between accuracy and power for different methods.  Indeed, I see evidence of this in the real data.  Comparing the counts of positively selected sites identified in the genes using thresholds of 0.50 and 0.95 reveals differences between \gls{beb} and \gls{sba} (table \ref{tab:num_sites_good_bad_genes}), despite large rank correlations.
\begin{table}[H]
  \centering
  \begin{threeparttable}
    \caption{Number of sites identified to be under positive selection for the real data.}
    \centering
    \begin{tabular}[!ht]{*{7}l}
      \toprule
      & \multicolumn{3}{c}{M2a}       & \multicolumn{3}{c}{M8}     \\
      \cmidrule(lr){2-4}              \cmidrule(lr){5-7}           \\
      Gene                   & NEB      & BEB   & SBA        & NEB     & BEB    & SBA     \\
      \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} \\
      \textit{CDH3}          & 1/1      & 12/1  & 46/0       & 1/1     & 22/1   & 117/5   \\
      \textit{mivN}          & 1/0      & 7/0   & 1/0        & 4/1     & 12/1   & 28/0    \\
      \textit{pgpA}          & 1/0      & 4/0   & 4/0        & 5/1     & 5/1    & 17/0    \\
      \textit{tax}           & 181/181  & 181/0 & 181/0      & 181/181 & 181/21 & 181/21  \\
      \textit{TrbL-VirB6\_2} & 0/0      & 16/0  & 0/0        & 11/1    & 18/0   & 59/0    \\
      \hline
      \rule{0pt}{3ex}\textit{pol}& 12/6     & 19/6  & 94/4       & 22/10   & 33/13  & 83/16   \\
      lysin                  & 33/22    & 32/22 & 42/5       & 37/23   & 37/23  & 41/11   \\
      \textit{nuoL3}         & 18/9     & 18/8  & 85/18      & 19/10   & 20/10  & 83/20   \\
      \textit{RfaL}          & 20/2     & 20/1  & 70/1       & 33/4    & 41/3   & 74/3    \\
      \textit{TrbL-VirB6\_3} & 28/3     & 27/2  & 73/9       & 45/18   & 44/11  & 134/48  \\
      \textit{vif}           & 13/10    & 13/8  & 31/6       & 15/10   & 19/10  & 37/10   \\
      \hline
      \rule{0pt}{3ex}$\beta$-globin & 4/0& 5/0   & 11/0       & 8/4     & 8/4    & 17/4    \\
      \textit{ccmF}          & 7/1      & 11/1  & 112/0      & 15/3    & 79/5   & 114/5   \\
      \textit{ENAM}          & 9/1      & 21/1  & 184/0      & 44/2    & 31/1   & 78/1    \\
      \textit{env}           & 14/3     & 16/3  & 21/3       & 16/3    & 22/5   & 24/3    \\
      \textit{perM}          & 4/0      & 6/0   & 0/0        & 6/2     & 6/0    & 36/3    \\
      \bottomrule
    \end{tabular}
    \label{tab:num_sites_good_bad_genes}
    \begin{tablenotes}
      \small
    \item  The posterior probability thresholds are 0.5/0.95.  The top genes represent \textit{irregular} estimation, the middle \textit{regular}, and the bottom genes are not categorized.
    \end{tablenotes}
  \end{threeparttable}
\end{table}
Under M2a, there was a stark difference between the \textit{irregular} genes and all other genes.  \gls{roc} curves for simulations studies are better suited for comparing methods, because they give direct comparisons of power at the same false positive rate.

I also used rank correlation to investigate the robustness of the methods (\gls{beb}, \gls{neb}, and \gls{sba}) to the chosen model (M2a versus M8).  I did this by computing the rank correlation, between models, of the site posterior probabilities obtained by the same method (table \ref{tab:model_cors}).
\begin{table}[H]
  \centering
  \begin{threeparttable}
    \caption{Spearman rank correlations between site posterior probabilities for models M2a and M8.}
    \centering
    \begin{tabular}[h!]{l*{4}c}
      \toprule
      & \multicolumn{2}{c}{{\bf \textit{Regular}}} & \multicolumn{2}{c}{{\bf \textit{Irregular}}} \\
      \cmidrule(lr){2-3} \cmidrule(lr){4-5}
      & mean      & SD        & mean      & SD \\
      \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
      \hspace{1em} NEB       & 0.98      & 0.04      & 0.81      & 0.13   \\
      \hspace{1em} BEB       & 0.99      & 0.01      & 1.00      & 0.01   \\
      \hspace{1em} SBA       & 1.00      & 0.00      & 0.99      & 0.00   \\
      \bottomrule
    \end{tabular}
    \label{tab:model_cors}
    \begin{tablenotes}
      \small
    \item The mean and standard deviation (SD) of the correlations are for real genes displaying \textit{regular} and \textit{irregular} estimation properties.
    \end{tablenotes}
    \end{threeparttable}
\end{table}
For the \textit{regular} genes, all three methods had high correlations with low variably.  For the genes representing \textit{irregular} estimation, the correlation was lower and the variability larger for \gls{neb} as compared to \gls{beb} and \gls{sba}.  The similarity across models that I observed for \gls{sba} may be a consequence of using nonparametric bootstrapping, which should show robustness to model misspecification.  It seems that \gls{beb}'s application of uniform priors to the $\omega$ distribution achieved a similar effect.

Up to this point, bootstrapping has been used to obtain surrogates for posteriors. An alternative use of bootstrapping is to construct confidence intervals for posteriors to quantify the uncertainty at any given site about what the true posterior of positive selection is. For the real data, these confidence intervals differed substantially between M2a and M8, highlighting differences between the two modelling frameworks. For sites having a posterior of at least 0.9 under one or more methods, the M8 confidence intervals for those sites were never wider than the corresponding M2a intervals (table \ref{tab:interval_widths_ps_sites}).
\begin{table}[H]
  \centering
  \begin{threeparttable}
    \caption{Average SBA posterior probability interval widths for sites with at least one method having a posterior probability over 0.9.}
    \centering
    \begin{tabular}[!ht]{*{4}l}
      \toprule
      Gene                       & \multicolumn{1}{c}{M2a}       & \multicolumn{1}{c}{M8} & \multicolumn{1}{c}{Difference} \\
      \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4}
      \textit{CDH3}              & 0.95     & 0.46  & 0.49    \\
      \textit{mivN}              & 1.00     & 1.00  & 0.00    \\
      \textit{pgpA}              & 1.00     & 1.00  & 0.00    \\
      \textit{tax}               & 0.87     & 0.31  & 0.56    \\
      \textit{TrbL-VirB6\_2}     & 1.00     & 1.00  & 0.00    \\
      \hline
      \rule{0pt}{3ex}\textit{pol}    & 0.78     & 0.78  & 0.00    \\
      lysin                      & 0.70     & 0.49  & 0.20    \\
      \textit{nuoL3}             & 0.26     & 0.21  & 0.05    \\
      \textit{RfaL}              & 0.68     & 0.48  & 0.19    \\
      \textit{TrbL-VirB6\_3}     & 0.66     & 0.10  & 0.57    \\
      \textit{vif}               & 0.36     & 0.14  & 0.21    \\
      \hline
      \rule{0pt}{3ex}$\beta$-globin  & 1.00     & 0.00  & 1.00    \\
      \textit{ccmF}              & 1.00     & 0.49  & 0.51    \\
      \textit{ENAM}              & 0.53     & 0.43  & 0.10    \\
      \textit{env}               & 0.51     & 0.27  & 0.24    \\
      \textit{perM}              & 0.91     & 0.14  & 0.77    \\
      \bottomrule
    \end{tabular}
    \label{tab:interval_widths_ps_sites}
    \begin{tablenotes}
      \small
    \item The top genes represent \textit{irregular} estimation properties, the middle \textit{regular}, and the bottom genes are not categorized.
    \end{tablenotes}
  \end{threeparttable}
\end{table}
This result reflects broad differences between the \gls{mle} distributions obtained under these two models; \gls{mle} distributions under M8 tend to be tighter, and more likely located away from a boundary (supplementary figures \ref{fig:real_genes_m2a_mles}, \ref{fig:real_genes_m8_mles}).  I believe this represents empirical support for the commonly held notion that M8 is more powerful than M2a \citep{wong2004accuracy}.  However, this relationship should not be assumed to hold when the \glspl{mle} are poorly estimated.  Confidence interval widths were at the maximum (1.0) for both M8 and M2a in three of the five genes representing \textit{irregular} estimation.  These findings highlight the importance of (1) inspecting bootstrap distributions to gain insights into the challenges posed by the data in hand, and (2) using \gls{sba} to accommodate \gls{mle} uncertainties (especially when they are poorly estimated).

Lastly, I interpret the results for the \emph{tax} gene of the human T-cell lymphotropic virus. This gene warrants special attention because it has a highly unusual site-pattern distribution, extreme \glspl{mle}, and has been employed as a boundary case in several studies of the \gls{neb} and \gls{beb} classifiers \citep{suzuki2004false,yang2005bayes}. The dataset has 20 taxa and 181 sites, 158 (87\%) of which are invariant across all 20 lineages.  At each of the 23 variable sites, there is just one codon that differs from all the others with 21 of the 23 codon changes coding for a different amino acid.  This atypical site-pattern distribution corresponds to a relatively large number of nonsysnonymous substitutions over very short branch lengths (mean branch length: $0.0064$ under both M2a and M8). A very high probability of positive selection (i.e., large values for both the $p_{\omega>1}$ and $\omega_{>1}$ parameters) is required to account for the nonsynonymous substitutions when the branch lengths are so short. In fact, both models M2a and M8 estimate 100\% of the sites to be in the $\omega>1$ class. This result belies that fact that considerable instability is associated with those parameter estimates, as revealed by bootstrapping (supplementary figures \ref{fig:real_genes_m2a_mles}, \ref{fig:real_genes_m8_mles}). Since \gls{neb} ignores parameter value uncertainty, it must assign a conditional posterior probability of $\omega>1$ ($Pr=1.0$) for all sites, including those that are invariant. In contrast, the site posteriors for \gls{beb} and \gls{sba} were similar and depended on the site patterns (supplementary table \ref{tab:tax}).  As expected, the \gls{sba} signal for positive selection was strongest at the 21 sites with nonsynonymous changes (M2a: $0.87 < Pr < 0.89$; M8: $0.99 < Pr < 0.99$), as compared to all other sites (M2a: $0.55 < Pr < 0.60$; M8: $0.76 < Pr < 0.80$).  The \gls{sba} confidence intervals under M8 revealed that the estimates of $Pr$ for the 21 sites with a nonsynonymous change were more reliable (average width: 0.028) than for the invariant sites (average width: 0.418).  I suggest this result is appropriate for these data. Almost all the signal in this dataset is contained in those 21 sites, and it is difficult to reconcile this amount of nonsynonymous change over such short branches without strong positive selection.  Moreover, when branch lengths are very short, an invariant site can only be viewed as carrying no signal about whether the $\omega$ value would be small or large over longer evolutionary periods. This leads to very wide 95\% \gls{sba} $Pr$ confidence intervals for these sites.

\section{Discussion}
I have presented an approach, based on an unconventional use of the nonparametric bootstrap, for evaluating \gls{mle} instabilities and improving site-specific inference of positive selection.  For any given site in an alignment, conclusions about positive selection are based on the aggregation and distributions of many estimates of $\omega$ and many posterior probabilities.  An important step in our approach involves smoothing the bootstrap distributions of the parameter estimates using techniques borrowed from kernel density estimation.  This step is critical for overcoming instabilities in parameter estimation.  Kernel smoothing also has the benefit of reducing computational costs relative to procedures that use full bootstrap sampling to obtain comparable numbers of \glspl{mle}.

Application of \gls{beb}, \gls{neb}, and \gls{sba} using models M2a and M8 to 100 simulated datasets in each of 10 different simulation scenarios showed that, under difficult simulation conditions when regularity conditions have not been met, \gls{neb} often poorly controls false positive classification of sites, even when the number of taxa is large.  This is in contrast to past recommendations, which suggested \gls{neb} does well at controlling false positive rates when analyzing large datasets (many taxa and long sequences) \citep[e.g.,][]{yang2005bayes}.  By accounting for variability of estimation, both \gls{beb} and \gls{sba} achieve better control of the false positive rates.  However, \gls{sba} provided consistently better control under M8 when there was mild model misspecification (studies 5-8 under in table 1), and this was unaffected by pre-screening via the \gls{lr} test.  I note that all real data are expected to be affected, to some degree, by model misspecification.

By accounting for variability of estimation, both \gls{beb} and \gls{sba} achieve good power relative to \gls{neb} as is evidenced by their tending to be closer to the expected upper limit of performance in the \gls{roc} curves.  Some of the simulation results suggest that M2a is a better-performing model. For instance, M2a gave 1) \gls{roc} curves closer to the expected upper bound in some cases (figure \ref{fig:roc}) and 2) lower false positive rates (table \ref{tab:sim}). This may, however, be a consequence of the simulations conditions being more suitable for M2a than M8.  For example, in studies 3 and 7, half the sites were simulated with $\omega=1$, and M2a has a site class with $\omega=1$ fixed.  On the other hand, considering sites with larger posteriors in the real data analysis, the 95\% posterior confidence intervals were usually narrower (and never wider) for M8 than M2a.  This supports previous results that suggest M8 has more power to detect sites under positive selection \citep{wong2004accuracy}.  The $\beta$-globin gene serves as a good example.  Of the five sites in this gene where either \gls{neb} or \gls{beb} gave a posterior of at least 0.9, the \gls{sba} confidence interval widths were all 1 for M2a, but averaged 0.129 for M8.  Moreover, the $\omega_{>1}$ parameter distributions tended to be wider for M2a than M8, particularly for the genes that displayed properties suggesting regularity conditions were met.  This is probably because the beta distribution used by M8 to model $\omega<1$ has more flexibility in real data conditions compared to an M2a model with the same number of parameters.

An appealing attribute of \gls{beb}, relative to \gls{sba}, is its limited use of computational resources.  Each \gls{sba} bootstrap analysis may use similar computational resources as \gls{beb} does for the one original dataset.  However, \gls{sba}'s greater computational requirements is a trade-off for a more rigorous assessment of the parameter estimation.  For example, \gls{sba} adjusts for the uncertainty in all model parameters, including branch lengths, while \gls{beb} does not.  A new \gls{beb} implementation that integrated over branch lengths would require costlier techniques because numerical integration does not scale well with higher dimension.  Moreover, because \gls{sba} estimates each set of bootstrap parameters independently, they can be estimated in parallel.  On a computing cluster with as many cores as bootstrap samples generated, the wall-clock times for \gls{beb} and \gls{sba} are comparable.

There are a limited number of \gls{beb} implementations for different models.  By contrast it is comparatively trivial to apply \gls{sba} to new models once the basic capacity for bootstrapping and parameter smoothing are in place.  This could facilitate the application of \gls{sba} to a wider variety of inference problems in molecular evolution than has occurred with \gls{beb}.  \gls{sba} for the popular branch-site codon model A \citep{yang2002bcodon,zhang2005evalimprovedbs} was implemented as a demonstration of the feasibility of \gls{sba} implementations for new models.  A new, preliminary implementation, which was completed within a few hours, can be found at https://github.com/Jehops/codeml\_sba.  An overview of the analysis of the \textit{NR1D1} gene \citep{baker2016functional} under \gls{sba} can be found in the supplementary section \ref{sec:nr1d1}.

There are useful by-products of the \gls{sba} approach for classifying sites. The histograms of the distributions of the \glspl{mle} over bootstrap samples provide insight into the degree of irregularity of the estimation.  For several of the datasets, most notably the \emph{tax} gene dataset, these histograms provided a clear indication that the \glspl{mle} were unstable.  In such cases, site classifications should be accepted with caution.  Even when regularity conditions have been met, the confidence intervals of the posteriors provide an additional tool for assessing the certainty about the strength of the signal for positive selection at an individual site.  I suggest that future analyses of real data should include both visual inspection of bootstrap distributions and reporting of \gls{sba}-derived confidence intervals of the posterior probabilities associated with positive selection.

Bootstrapping has been shown to provide effective adjustments to EB methods in other settings.  For example, \citet{laird1987empirical} studied the application of bootstrapping with EB methods for random effects models where both the observations and random effects distributions were Gaussian.  They argued that confidence intervals produced from bootstrap posteriors were frequently narrower than they should be and that bootstrap averaging helped to ameliorate problems.  They speculated that bootstrapping would produce good EB inferences for a broad class of EB problems.  In a prediction setting, a procedure that aggregates predictors generated from bootstrap replicates was proposed by \citet{breiman1996bagging}, which was shown to move some unstable predictors closer to optimality.  The bagging procedure used in that paper is equivalent to using the median posterior to classify sites under \gls{sba}.  Our experiments (data not shown) indicated that the average is a better measure of the middle of the distribution of site posterior probabilities.

While using the data in hand to account for errors in \gls{mle} estimation is helpful for detecting sites under positive selection, refinements of the \gls{sba} approach are warranted.  Like other approaches, I have avoided the difficult process of calibrating for type I errors in real data.
Choosing an optimal bandwidth parameter for smoothing a distribution is also a difficult process.  Under-smoothing will leave spurious bumps and irregularities in the distribution and over smoothing will remove useful information and increase bias.  There are different theoretical suggestions for the size of the bandwidth parameter, but these can be challenging to apply as they may depend on the unknown density \citep[p. 176]{venables2013modern}.
\gls{sba} uses bootstrap distributions to highlight problems when \glspl{mle} fall on or close to their boundaries.  I am hopeful that a penalized likelihood approach, which can push such estimates to the interior of the parameter space, will be helpful.  Bootstrapping does well to accommodate the variance in a parameter estimate, however, when estimates are very small, the variance, even under bootstrapping, may be underestimated.  This may be a problem I encountered with the branch lengths of the \emph{tax} gene.  Some preliminary experiments show that perturbing the very small branch length estimates of the \emph{tax} gene can cause large differences in the \glspl{mle} of the parameters of the $\omega$ distribution.  This suggests that applying kernel smoothing to parameters other than those defining the $\omega$ distribution may be helpful.

\gls{sba} can be applied to a wide variety of problems in molecular evolution where uncertainties or instabilities in \glspl{mle} impact inference based on empirical Bayes.  Examples where the method can be directly applied, with little or no modification, include: classification of sites into general rate categories \citep[e.g.,][]{mayrose2004comparison}, identification of positively selected sites in non-coding \gls{dna} \citep[e.g.,][]{haygood2007promoter}, identification codon sites subject to episodic change in selection pressure \citep[e.g.,][]{yang2002bcodon}, detection of Type-I functional divergence in protein sequences \citep[e.g.,][]{gaston2011phylogenetic}, detection of amino acid sites having shifts in the pattern of exchangeabilities \citep[e.g.,][]{le2012modeling}, and detection of amino acid sites evolving under a covarion-like evolutionary process \citep[e.g.,][]{penn2008evolutionary}.  With some modification, \gls{sba} could be applied to the task of ancestral state reconstruction.  As the field moves towards increasingly more complex models, there will be increasing demand for methods such as \gls{sba} that can account for parameter-estimate uncertainties.

\subsection{Bootstrap Methods to Adjust for Uncertainty}
To construct confidence intervals for a parameter, $\theta$, and correct bias, \citet{efron1979bootstrap} devised the bootstrap.
A bootstrap sample, $\bm{x^*}$, is obtained by drawing the values, $x_1^*,\dots,x_n^*,$ with replacement from a random sample, $\bm{x}$.
For each of $b=1 \dots B$ bootstrap samples, the bootstrap estimate can then be calculated, $\hat{\theta}^{*b}$, to obtain the bootstrap distribution of $\hat{\theta}$.  Bootstrap distributions are commonly used with phylogenetic data to test the topology of a proposed tree.
I applied the bootstrap to site patterns in a sequence alignment to adjust for the uncertainty in parameter estimates in EB classification.
The procedure is illustrated in figure \ref{fig:bootstrap}:
\begin{enumerate}
  \item From an alignment of protein coding \gls{dna} sequences, $\bm{x}$, with $n$ codon sites, randomly sample site patterns with replacement to obtain a bootstrap sample, $\bm{x}^{*b}$, with $n$ sites.
  \item Estimate the \glspl{mle}, $\hat{\theta}^{*b}$, for bootstrap sample $\bm{x}^{*b}$.
  \item Use $\hat{\theta}^{*b}$ and $\bm{x}$ to calculate posterior probabilities, $Pr_h(\omega>1|x_h,\hat{\theta}^{*b})$, that each site, $h$, is under positive selection.
  \item Repeat steps 1 through 3 $B$ times to calculate $B$ sets of posterior probabilities for each codon site.
  \item Calculate an aggregate posterior probability that each site is under positive selection by, e.g., averaging posterior probabilities over bootstrap replicates, $\sum_{b=1}^BPr_h(\omega>1|x_h,\hat{\theta}^{*b})/B$.
% or by taking the median, $med_iPr_h^i(\omega>1|\bm{X},\theta^i)$.
\end{enumerate}
A preliminary implementation of the \gls{sba} method supporting codon models M2a, M8, and branch-site model A, built upon the codeml application from the PAML package \citep{yang2007paml}, can be found at \\ https://github.com/Jehops/codeml\_sba.

\subsection{Kernel Smoothing to Approximate the Bootstrap Distribution}
Kernel smoothing \citep{akaike1954approximation,parzen1962estimation,rosenblatt1956remarks,wand1994kernel} is class of nonparametric techniques that can improve estimation of a distribution.  The kernel density estimator for a continuous density $f$, $\hat{f}(x;h) = (nh)^{-1}\sum_{i=1}^nK([x-X_i]/h)$, includes a kernel density (probability) function, $K$, to locally average or smooth observations and the amount of smoothing is controlled by a bandwidth parameter, $h$.  For small $h$, each of the $h^{-1}K([x-X_i]/h)$ contributions are large only for $x$ close to some $X_i$ giving rise to a bumpy
distribution, whereas for $h$ large the $h^{-1}K([x-X_i]/h)$ contributions overlap giving a much smoother distribution \citep{silverman1987bootstrap}. I used kernel density estimation to create smoothed bootstrap distributions for the $p$ parameters of the $\omega$ distributions under models M2a and M8 using a uniform kernel.

Kernel density estimation requires a bandwidth parameter as input. One method for determining $h$ is using leave-one-out cross validation \citep[p. 184]{venables2013modern},
\[\hat{f}_{(-k)}(x;h) = (n-1)^{-1}h^{-1}\sum_{i\ne k}K([x - X_i]/h).\]  In this approach, $h$ is chosen to maximize the sum of the logged density
estimates $\sum_k \log \hat f_{(-k)}(x_k;h)$, where $\hat f_{(-k)}(x;h)$ is the kernel density estimate constructed from all of the $x_i$ except $x_k$.  However, our experiments using leave-one-out likelihood to choose an optimal bandwidth parameter for the $p$ parameters of M2a and M8 merely resulted in smoothed estimates of the biased bootstrap distributions.  To obtain conservative estimates of the $p$ parameters that suppressed the influence of instabilities I chose to over smooth by using a bandwidth parameter of $h=0.4$ for  all applications of \gls{sba}.

Adding kernel smoothing to the bootstrap algorithm increases the number of parameter estimates used in step 5 of the unsmoothed algorithm by sampling from a smoothed bootstrap distribution. The adjustment is in step 2 of the algorithm. The \gls{ml} parameters estimated from bootstrap sample $b$, $\hat\theta^{*b}$, are replaced by $\theta^{sb}$ sampled from the smoothed bootstrap distribution.  The rest of the algorithm proceeds as in the unsmoothed version, but using $\theta^{sb}$ in place of $\hat\theta^{*b}$.

For model M8, the step 2 adjustment is as follows.  For each $\hat\theta^{*b}$, $p_{\omega < 1}^{sb}$ samples are repeatedly drawn from a univariate uniform distribution centered at $\hat p_{\omega < 1}^{*b}$ with width $2h$.  If necessary, the minimum and maximum points of the distribution are truncated to 0 and 1.  Let $\theta^{sb}$ denote $\hat\theta^{*b}$ with $p_{\omega<1}^{sb}$ replacing $\hat p_{\omega<1}^{*b}$ ($p_{\omega>1}^{sb} = 1-p_{\omega<1}^{sb}$).  The same procedure is used under model M2a, however, with three weight parameters, the sampling is done on a bivariate uniform distribution with the following additional restrictions: i) $p_{\omega<1}^{sb} + p_{\omega=1}^{sb} \le 1$, ii) $(\hat{p}_{\omega<1}^{*b} - h) \le p_{\omega<1}^{sb} \le (\hat{p}_{\omega<1}^{*b} + h)$, and iii) $(\hat{p}_{\omega=1}^{*b} - h) \le p_{\omega=1}^{sb} \le (\hat{p}_{\omega=1}^{*b} + h)$.  As with M8, if necessary, the minimum and maximum points of the distribution are truncated at 0 and 1, and $p_{\omega>1}^{sb} = 1 - p_{\omega<1}^{sb} - p_{\omega=1}^{sb}$.
\subsection{Simulation Studies}
Datasets were simulated using \emph{EvolverNSSites} from the PAML 4.8a package \citep{yang2007paml} and \emph{Indelible} \citep{fletcher2009indelible} following some of the settings described in \citet{wong2004accuracy}.  To compare the relative performance of \gls{beb}, \gls{neb}, and \gls{sba} for predicting sites under positive selection, 10 different simulation studies, divided into three scenarios, were used.  Table \ref{tab:sim} gives an overview of the $\omega$ distributions used to simulate the data.  The \textit{Correct Model Scenario} included four simulation studies where the nuisance parameters, $\kappa=1$ and $\pi_i=1/61$, matched the fitted model.  The \textit{Mild Misspecification} and \textit{Heavy Misspecification} scenarios included four simulation studies with mild misspecification and two studies with heavy misspecification of the fitted model, respectively.  The data in the \textit{Mild Misspecification Scenario} was simulated using $\kappa=8$ and empirical codon frequencies derived from application of the general time-reversible model \citep[p. 33]{Yang2006Computational} to the \textit{TrbL-VirB6-3} plasmid conjugative transfer protein of \textit{Rickettsia}.  In the fitted model, $\kappa$ was estimated, while the misspecification was introduced by using F3x4 (expected codon frequencies calculated using the nucleotide frequencies at the three codon positions).  For the \textit{Heavy Misspecification Scenario}, study 9 used the heavily biased codon frequencies from the Drosophila \textit{GstD1} gene and $\kappa=8$ to simulate the data.  In study 10, there were two heterogeneous classes of sites.  Half the sites were simulated using equal codon frequencies, $\kappa=1$, and $\omega=0.5$, while the other half with the Drosophila gsTD gene codon frequencies, $\kappa=8$, and $\omega=1$.  For both studies in this scenario, analysis was carried out using a single set of codon frequencies (set equal to $1/61$) and a single $\kappa$ parameter estimated for all sites in the data set.  For all studies in the three scenarios, 100 alignments, each having 500 codons, were simulated with the same 5-taxon tree from \citet{wong2004accuracy}.  The studies in the \textit{Correct Model Scenario} were repeated under model M2a with the 30-taxon tree from the same paper.

\subsection{Real Data Analysis}
Table \ref{tab:realDataResultsOverview} describes the real data sequences analyzed under models M2a and M8 using \gls{neb}, \gls{beb}, and \gls{sba}.  Of the the 16 genes, eight code for transmembrane proteins in \textit{Rickettsia} (\textit{ccmF}, \textit{mivN}, \textit{perM}, \textit{pgpA}, \textit{RfaL}, \textit{TrbL-VirB6\_2}, and \textit{TrbL-VirB6\_3}) and were previously analyzed in \citet{bao2008likelihood}.  Three genes from the HIV-1 virus (\textit{env} \textit{pol}, and \textit{vif}) and a $\beta$-globin gene were described and analyzed in \citet{yang2000codon}, two primate genes (\textit{CDH3} encoding cadherin and \textit{ENAM} encoding enamelin), a lysin gene from \citet{yang2000maximum}, and the \emph{tax} gene from the human T-cell lymphotrophic virus (HTLV) that was analyzed by \citet{suzuki2004false}.  All data is available at https://github.com/jehops/sba\_real\_data.

\chapter{Unrecognized Statistical Difficulties with Tests of Positive Selection under the Branch-Site Family of Codon Models}
\label{chap:bs}
\section{Introduction}
Early models of evolution had limited power to detect positive selection that acted upon proteins and amino acids.  A challenge is that positive selection, relative to purifying selection and neutral evolution, is a rare occurrence and often acts only upon a small proportion of sites \citep{petersen2007genes,studer2008pervasive}.  A single $\omega$, averaged over all sites, would rarely be estimated large enough to reject the null hypothesis of no positive selection, even for proteins that were subjected to positive selection.  By treating $\omega$ at a site as the realized value of a random variable, and thus allowing $\omega$ to vary over sites, the power to detect positive selection increases \citep{wong2004accuracy}.  The site models, however, do not allow $\omega$ to vary over time, thus positive selection is detected at only if the average $\omega$ over lineages is sufficiently large.

% Messier and Stewart inferred the ancestral sequences, then calculated dN and dS at a particular branch, then used a t-test with infinite df (z test).  When dN is larger than dS then declared positive selection.
% Zhang, Kumar, and Nei used simulations to determine that a t-distribution was not appropriate
% ancestral reconstruction has random errors and systematic biases (bad)
Another challenge to detect positive selection is that it often acts episodically \citep{kosiol2008patterns,studer2009evidence}.  Early methods to detect episodic positive selection, such as those of \cite{messier1997episodic} and \cite{zhang1997small}, inferred ancestral sequences to compute and compare the rates of nonsynonymous and synonymous substitutions at particular lineages of a phylogeny.  \cite{yang1998likelihood} avoided the problems with ancestral sequence reconstruction \citep{collins1994compositional} in the branch models.  The branch models use likelihood methods that condition upon all unknown ancestral states and allowed $\omega$ to vary over lineages to capture episodic positive selection.  However, the branch models assume no variation in $\omega$ among sites, so positive selection is detected along a lineage of the tree only if the average $\omega$ over sites is sufficiently large.

To increase the power to detect positive selection at a subset of sites along prespecified lineages of a phylogenetic tree, referred to as the foreground of the tree, branch-site codon models were developed \citep{yang2002bcodon,10.1093/molbev/msg149,zhang2005evalimprovedbs,bielawski2004maximum}.  The first branch-site model \citep{yang2002bcodon} used an \gls{lr} test that was shown, through simulation, to be susceptible to high false positive rates \citep{zhang2004freqfalsedetect}.  Changes were later made \citep{zhang2005evalimprovedbs} and new simulations showed that the updated \gls{lr} test did not suffer from high false positive rates when selection was relaxed in the foreground.  \cite{suzuki2008falsebs} and then \citep{nozawa2009reliabbs} reported that false positive rates were still excessively high, however most of these claims were dismissed due to faulty statistical interpretation \citep{yang2009defense, yang2010propertiesbs,zhai2012looking}.  \cite{yang2010propertiesbs} provided new simulation results, which suggest that the asymptotic theory is sound and the large-sample null distribution is reliable.  Work has also shown that the branch-site \gls{lr} test is normally conservative \citep{Branch-SiteTestRobust}, not misled by positive selection in background branches \citep{zhang2005evalimprovedbs, Branch-SiteTestRobust, EffectErrorsBranch-Site}, and robust to insertions and deletions, as long as the sequence alignment is correct \citep{EffectErrorsBranch-Site}.

Branch-site model A \citep{zhang2005evalimprovedbs} has provided a basis for other codon models of episodic selection.  \cite{guindon2004modeling} developed what they referred to as a stochastic branch-site model, which does not require the phylogenetic tree to be divided \emph{a priori} into background and foreground branches.  They employ two Markov processes, the usual process for codon states, and another for the unobservable $\omega$ class, which can change along any branch of the tree.  \cite{10.1093/molbev/mst198} showed through simulation that both branch-site models are conservative under null conditions, but branch-site model A is more powerful when the foreground is correctly chosen.  On the other hand, when too few or too many foreground branches are chosen, the power of branch-site model A decreases.  If prior information about the foreground is unavailable, each branch can be individually tested under branch-site model A using corrections for multiple tests \citep{anisimova2007multiple}.  \cite{10.1093/molbev/mst198} determined that the stochastic branch-site model is better suited for such exploratory experiments.

\cite{kosakovsky2011random}, \cite{murrell2012detecting}, and \citep{murrell2015gene} developed a class of models that relax the constraints of the selection arrangements in branch-sites model A by allowing each branch-site combination to have different $\omega$ values.  \cite{kosakovsky2011random} showed that branch-site model A can give excessive type I or type II errors when the data strongly deviate from the constrained $\omega$ distributions for background and foreground branches.  They reported that their random-effects method consistently matched or outperformed branch-site A tests in terms of power and error rates.

\cite{smith2015less} developed an adaptive feature to the random effects method, which determines the optimal number of rate categories for each branch using small-sample Akaike Information Criterion.  This is a welcome advancement as others have reported problems with codon models when the number of mixture classes is too large \citep[e.g.][]{mingrone2019}.  \cite{davydov2019comparativeanalysis} argued that branch-site model A erroneously detects positive selection in real genes at a rate greater than 70\%.  They developed a modified branch-site model, which includes a separate site parameter for the synonymous rate, thus accounting for nucleotide sequence selection and mutation rate.  Like \cite{baele2013bayesian} and \cite{gil2013codonphyml}, the synonymous rate at each site is the realized value of a discretized unit gamma distribution, but unlike those models, they allow $\omega$ to vary over both branches and sites.

Among the branch-site models in this family, branch-site model A employs the most restrictive constraints on its parameter values.  The constraints allow the model to serve as an explicit test of positive selection for \emph{a priori} hypotheses about specific branches.  Despite the sophistication and claimed improvements of newer methods for detecting episodic positive selection, branch-site model A remains widely used, because such tests are believed to have the highest power when \emph{a priori} information about the foreground branches are tested.

Branch-site model A has recently been used to describe, e.g., the molecular mechanisms of immunity, longevity, and cancer-resistance in bats \citep{scheben2020CancerBatBranchSite} and the North American beaver \citep{zhang2020beavers}, tumor suppressor in cetaceans \citep{martinez2020positiveCetaceans}, and the antagonistic insect-plant interaction between swallowtail butterflies and birthworts \citep{allio2020genomeButterflies}.  With such wide use, it is important to fully understand its tendencies and limitations and when it or other models are appropriate.  Here I perform new simulations studies to reassess the properties of branch-site model A.  By assessing the statistical properties of a base form of a branch-site model, I aim to provide insight for the model and related models that implement extensions \citep[e.g.,][]{davydov2019comparativeanalysis}.

\section{Theory and Methods}
The parameters of the $\omega$ distribution under the alternative model of branch-site model A described in \cite{yang2002bcodon} and \cite{zhang2005evalimprovedbs} are shown in table \ref{tab:bs_omega_dist}.  For the null model, \cite{yang2002bcodon} constrained $\omega_0=0$ and permitted only sites classes $0$ and $1$, which is equivalent to sites model M1 \citep{yang2000codon}.  Using data simulated under the null hypothesis, \cite{zhang2004freqfalsedetect} found positive selection in the foreground was erroneously detected in 19\%—54\% of cases.  \cite{zhang2005evalimprovedbs} used an updated null model, which was shown through simulation to resolve the problem of inflated false positive rates.  This updated null model includes all four $\omega$ site classes of the alternative model, but constrained $\omega_2=1$.  In addition, \cite{zhang2005evalimprovedbs} relaxed the constraint on $\omega_0$ by allowing it to take on values between $0$ and $1$.

\begin{table}[h!]
  \centering
  \begin{threeparttable}
    \caption{The $\omega$ distribution under the alternative model for the branch-site model described in \cite{yang2002bcodon} and \cite{zhang2005evalimprovedbs}}.
    \begin{tabular}[!ht]{*{4}l}
      \toprule
      Site Class     & Proportion                  & Background     & Foreground     \\
      \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4}
      0              & $p_0$                       & $\omega_0$     & $\omega_0$     \\
      1              & $p_1$                       & $\omega_1=1$   & $\omega_1=1$   \\
      2a             & $(1-p_0 - p1)p_0/(p_0+p_1)$ & $\omega_0$     & $\omega_2>1$     \\
      2b             & $(1-p_0 - p1)p_1/(p_0+p_1)$ & $\omega_1=1$   & $\omega_2>1$   \\
      \bottomrule
    \end{tabular}
    \label{tab:bs_omega_dist}
    \begin{tablenotes}
      \small \item Model A of \cite{yang2002bcodon} constrains $\omega_0=0$ and uses sites model M1a (no sites classes 2a or 2b) as the null model.  In \cite{zhang2005evalimprovedbs}, model A constrains $0<\omega_0<1$ and uses a null model with all four sites classes and constrains $\omega_2=1$.
    \end{tablenotes}
  \end{threeparttable}
\end{table}

Here I conduct simulation studies to explore how the asymptotic null distribution of the improved branch-site \gls{lr} test \citep{zhang2005evalimprovedbs} may vary from the recommended $\chi^2$ distributions. Codon sequence data were simulated using the \emph{Indelible} simulation software \citep{fletcher2009indelible}.  For each of three simulation scenarios covering $23$ distinct simulations studies, $1000$ sequence alignments $500$, $5000$, or $10000$ codons long were generated using a symmetric, 8-taxon tree (figure \ref{fig:bstree}) with the total of all branch lengths summing to $3$ or $6$.  All sequence alignments were simulated with a transition to transversion rate ratio, $\kappa=2$ and equal codon frequencies.  Table \ref{tab:BSALRSims} provides an overview of the simulation conditions, including the $\omega$ distributions used to simulate the data.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[t]{.5\textwidth}
    <<bstree,echo=F,cache=F>>=
    bstree <- read.tree(text="(((A:0.214286,B:0.214286):0.214286,(C:0.214286,D:0.214286):0.214286):0.214286,((E:0.214286,F:0.214286):0.214286,(G:0.214286,H:0.214286):0.214286):0.214286);")
    edge.widths <- rep(2,68)
    edge.widths[c(2,5)] <- 6
    plot(bstree,no.margin=T,cex=2,type="phylogram",lab4ut="axial",show.node.label=F,label.offset=0.01)
    @
  \end{subfigure}
  \caption[]{Phylogenetic tree topology used in branch-site model simulations.}
  \label{fig:bstree}
\end{figure}

\begin{table}[h!]
  \centering
  \begin{threeparttable}
    \caption{Design of branch-site model A simulation studies for assessing estimated LR distribution under null hypothesis conditions.}
    \begin{tabular}[]{*{5}l}
      \toprule
      \multicolumn{1}{c}{Study} & \multicolumn{1}{c}{$N_c$} & \multicolumn{1}{c}{$N_f$} & \multicolumn{1}{c}{TTL} & \multicolumn{1}{c}{$\omega$ distribution} \\
      \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
      \multicolumn{5}{c}{Single Foreground Branch} \\
      \hline
      1a  & 500   & 1(t)   & 3 & $p_0{=}0.7$ $p_1{=}0.2$ $\omega_0{=}0.3$ \\
      1b  & 5000  & 1(t)   & 3 & \\
      1c  & 5000  & 1(t)*  & 3 & \\
      1d  & 5000  & 1(t)** & 3 & \\
      2a  & 500   & 1(t)   & 3 & $p_0{=}0.75$ $p_1{=}0.25$ $\omega_0{=}0.3$ \\
      2b  & 5000  & 1(t)   & 3 & \\
      3a  & 500   & 1(t)   & 3 & $p_0{=}0.25 $ $p_1{=}0.75$ $\omega_0{=}0.3$ \\
      3b  & 5000  & 1(t)   & 3 & \\
      4a  & 5000  & 1(t)*  & 6 & $p_0{=}0.5$ $p_1{=}0.5$ $\omega_0{=}0$ \\
      4b  & 5000  & 1(t)** & 6 & \\
      4c  & 5000  & 1(i)*  & 6 & \\
      4d  & 5000  & 1(i)** & 6 & \\
      \hline
      \multicolumn{5}{c}{Half Tree Foreground} \\
      \hline
      5   & 5000  & h     & 6 & $p_0{=}0.375$ $p_1{=}0.375$ $\omega_0{=}0$ \\
      6   & 5000  & h     & 6 & $p_0{=}0.475$ $p_1{=}0.475$ $\omega_0{=}0$ \\
      7a  & 5000  & h     & 3 & $p_0{=}0.5$ $p_1{=}0.5$ $\omega_0{=}0$ \\
      7b  & 5000  & h     & 6 & \\
      7c  & 10000 & h     & 3 & \\
      \hline
      \multicolumn{5}{c}{Misspecification of $\omega$ Distribution} \\
      \hline
      8a  & 500   & 1(t)   & 3 & $[p_0,p_1,p_2]{=}[0.4,0.4,0.2]$ $[\omega_0,\omega_1,\omega_2]{=}[0.1,0.5,0.9]$  \\
      8b  & 5000  & 1(t)   & 3 & \\
      8c  & 500   & h      & 3 & \\
      8d  & 5000  & h      & 3 & \\
      9a  & 500   & h      & 3 & $[p_0,p_1,p_2]{=}[0.4,0.2,0.4]$ $[\omega_0,\omega_1,\omega_2]{=}[0.1,0.5,1]$ \\
      9b  & 5000  & h      & 3 & \\
      \bottomrule
    \end{tabular}
    \label{tab:BSALRSims}
    \begin{tablenotes}
      \small
    \item $N_c$: sequence length in number of codons, $N_f$: number foreground branches (t: terminal branch, i: internal branch, h: half tree, *: foreground is $1/10$ length or other branches, **: foreground branch is $10$ times length of other branches), TTL: total tree length.
    \end{tablenotes}
  \end{threeparttable}
\end{table}

The \textit{Single Foreground Branch Scenario} was comprised of $12$ studies with each sequence alignment simulated using a single terminal foreground branch leading to taxon A or an internal foreground branch at the base of the tree in figure \ref{fig:bstree}.  The sequence lengths, total branch lengths, and foreground branch length varied between studies in this scenario.  The \textit{Half Tree Foreground Scenario} was comprised of $5$ simulations studies, each with half of the 8-taxon tree in the foreground.  Four of the studies in this scenario used $5000$-codon sequences and one used $10000$.  The \textit{Misspecification of $\omega$ Distribution Scenario} was comprised of $6$ studies with sequences generated using codon model M3 (k=3) \citep{yang2000codon}, i.e., the number of simulated $\omega$ site classes does not match the number of $\omega$ site classes in the fitted model.

Table \ref{tab:BSAConfSims} describes simulation studies conducted under the \textit{Confounding Foreground Scenario}.  Confounding here means that any foreground branches specified under simulation do not match the foreground branches of the fitted model.  For each study in this scenario, data were simulated under both the null ($\omega_2=1$ in foreground branch) and the alternative ($\omega_2 \ge 1$ in the foreground) models.  This was done to compare \gls{lr} statistic \glspl{cdf} with and without confounding foreground branches.  The goal of the the studies in this scenario is to determine whether confounding foreground branches may cause false detection of positive selection under the fitted model.

\begin{table}[h!]
  \centering
  \begin{threeparttable}
    \caption{Design of branch-site model A simulation studies for assessing estimated LR distribution under confounding foreground branch conditions.}
    \begin{tabular}[]{*{5}l}
      \toprule
      \multicolumn{1}{c}{Study} & \multicolumn{1}{c}{$N_c$} & \multicolumn{1}{c}{$N_f$} & \multicolumn{1}{c}{TTL} & \multicolumn{1}{c}{$\omega$ distribution} \\
      \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
      10  & 5000  & 1(i)   & 3 & $p_0{=}0.5$ $p_1{=}0.4$ $\omega_0{=}0.75$ $\omega_2{=}2.0$ \\
      11  & 5000  & 1(i)   & 6 & $p_0{=}0.475$ $p_1{=}0.475$ $\omega_0{=}0$ $\omega_2{=}3.0$ \\
      12  & 5000  & h      & 6 & $p_0{=}0.475$ $p_1{=}0.475$ $\omega_0{=}0$ $\omega_2{=}3.0$ \\
      \bottomrule
    \end{tabular}
    \label{tab:BSAConfSims}
    \begin{tablenotes}
      \small
    \item $N_c$: sequence length in number of codons, $N_f$: number foreground branches (t: terminal branch, i: internal branch, h: half tree, *: foreground is $1/10$ length or other branches, **: foreground branch is $10$ times length of other branches), TTL: total tree length.
    \end{tablenotes}
  \end{threeparttable}
\end{table}

\section{Results and Discussion}
\subsection{LR Tests Tend to be Conservative when Information Content is Low}
Figure \ref{fig:BSSingleBranchLRs} shows the \gls{lr} statistic \glspl{cdf} for branch-site model A when a single branch is specified in the foreground.  Studies 1a - 1d differ from the other studies in this scenario in that $p_0+p_1<1$.  Consequently, alternative hypotheses parameters that give the true generating model have positive weight on the 2a and 2b classes in Table \ref{tab:BSAConfSims}. Because they have positive weight, $\omega_2$ must equal $1$ to give the true generating distribution. Thus the parameters are identifiable under the alternative model, which implies that the non-standard likelihood theory of \cite{self1987asymptotic} will apply with large samples.  That theory suggests that the large sample \gls{lr} statistic distribution is well approximated by a $\chi^2_0/2 + \chi^2_1/2$ distribution.  However, the results of studies 1a and 1c indicate that the sparseness of information contained in a single foreground branch may not be sufficiently influential to draw all or most weight away from $p_0$ or $p_1$.  Histograms of $1-p_0-p_1$ for studies 1a - 1d in figure \ref{fig:bshistsc1} show that too often all or nearly all weight is placed on sites classes $\omega_0$ and $\omega_1$ in studies 1a and 1c.  With increased information content in the foreground branch, whether by longer sequences lengths as in study 1b, or a longer foreground branch as in study 1d, the $\chi^2_0/2 + \chi^2_1/2$ distribution does well to approximate the large-sample \gls{lr} statistic distribution.

\begin{figure}[h!]
  \captionsetup[subfigure]{labelformat=empty}
  \begin{subfigure}[t]{.01\textwidth}
    \vspace{62 mm}
    \rotatebox{90}{$P(X \le x)$}
  \end{subfigure}
  \hspace{3mm}
  \begin{subfigure}[t]{.96\textwidth}
    <<sfbslnl,echo=F,warning=F>>=
    rm(list=ls())
    sim1a.lnl <- read.csv("./bs_data/sim1a_lnLs.csv")
    sim1b.lnl <- read.csv("./bs_data/sim1b_lnLs.csv")
    sim1c.lnl <- read.csv("./bs_data/sim1c_lnLs.csv")
    sim1d.lnl <- read.csv("./bs_data/sim1d_lnLs.csv")

    sim2a.lnl <- read.csv("./bs_data/sim2a_lnLs.csv")
    sim2b.lnl <- read.csv("./bs_data/sim2b_lnLs.csv")

    sim3a.lnl <- read.csv("./bs_data/sim3a_lnLs.csv")
    sim3b.lnl <- read.csv("./bs_data/sim3b_lnLs.csv")

    sim4a.lnl <- read.csv("./bs_data/sim4a_lnLs.csv")
    sim4b.lnl <- read.csv("./bs_data/sim4b_lnLs.csv")
    sim4c.lnl <- read.csv("./bs_data/sim4c_lnLs.csv")
    sim4d.lnl <- read.csv("./bs_data/sim4d_lnLs.csv")

    lrs.sim1a <- sort(2*(sim1a.lnl$bsA-sim1a.lnl$null2))
    lrs.sim1b <- sort(2*(sim1b.lnl$bsA-sim1b.lnl$null2))
    lrs.sim1c <- sort(2*(sim1c.lnl$bsA-sim1c.lnl$null2))
    lrs.sim1d <- sort(2*(sim1d.lnl$bsA-sim1d.lnl$null2))

    lrs.sim2a <- sort(2*(sim2a.lnl$bsA-sim2a.lnl$null2))
    lrs.sim2b <- sort(2*(sim2b.lnl$bsA-sim2b.lnl$null2))

    lrs.sim3a <- sort(2*(sim3a.lnl$bsA-sim3a.lnl$null2))
    lrs.sim3b <- sort(2*(sim3b.lnl$bsA-sim3b.lnl$null2))

    lrs.sim4a <- sort(2*(sim4a.lnl$bsA-sim4a.lnl$null2))
    lrs.sim4b <- sort(2*(sim4b.lnl$bsA-sim4b.lnl$null2))
    lrs.sim4c <- sort(2*(sim4c.lnl$bsA-sim4c.lnl$null2))
    lrs.sim4d <- sort(2*(sim4d.lnl$bsA-sim4d.lnl$null2))

    N <- length(lrs.sim1a)
    x <- seq(0,6.3,length.out=N)

    par(mfrow=c(2,2))
    par(mai=c(0,.3,0,0))
    plot(x,1/2+pchisq(x,1)/2,type='l',ylab='',xaxt='n',ann=F,main="Study 1")
    title("Studies 1a-1d",adj=0.9,line=-5)
    title(expression(paste(p[0],"=0.7 ",p[1],"=0.2 ",omega[0],"=0.3")),adj=0.9,line=-8)
    points(x,pchisq(x,1),col='grey',type='l')
    points(sort(lrs.sim1a),c(1:N)/N,col='#0072B2',lty=2,lwd=2.0,type='l') # blue
    points(sort(lrs.sim1b),c(1:N)/N,col='#d66d00',lty=3,lwd=2.5,type='l') # orange
    points(sort(lrs.sim1c),c(1:N)/N,col='#000000',lty=4,lwd=2.0,type='l') # black
    points(sort(lrs.sim1d),c(1:N)/N,col='#67933a',lty=6,lwd=2.0,type='l') # green

    par(mai=c(0,0,0,0))
    plot(x,1/2+pchisq(x,1)/2,type='l',ylab='',xaxt='n',yaxt='n',main='',ann=F)
    title("Studies 2a-2b",adj=0.9,line=-5)
    title(expression(paste(p[0],"=0.75 ",p[1],"=0.25 ",omega[0],"=0.3")),adj=0.9,line=-8)
    points(x,pchisq(x,1),col='grey',type='l')
    points(sort(lrs.sim2a),c(1:N)/N,col='#0072B2',lty=2,lwd=2.0,type='l') # blue
    points(sort(lrs.sim2b),c(1:N)/N,col='#d66d00',lty=3,lwd=2.5,type='l') # orange

    par(mai=c(.3,.3,0,0))
    plot(x,1/2+pchisq(x,1)/2,type='l',yaxt='n',ylab='',main='',ann=F)
    title("Studies 3a-3b",adj=0.9,line=-5)
    title(expression(paste(p[0],"=0.25 ",p[1],"=0.75 ",omega[0],"=0.3")),adj=0.9,line=-8)
    axis(2, at=c(0.5,0.6,0.7,0.8,0.9),labels=c('0.5','0.6','0.7','0.8','0.9'))
    points(x,pchisq(x,1),col='grey',type='l')
    points(sort(lrs.sim3a),c(1:N)/N,col='#0072B2',lty=2,lwd=2.0,type='l') # blue
    points(sort(lrs.sim3b),c(1:N)/N,col='#d66d00',lty=3,lwd=2.5,type='l') # orange

    par(mai=c(0.3,0,0,0))
    plot(x,1/2+pchisq(x,1)/2,type='l',ylab='',yaxt='n',main='',ann=F)
    title("Studies 4a-4d",adj=0.9,line=-5)
    title(expression(paste(p[0],"=",p[1],"=0.5 ",omega[0],"=0")),adj=0.9,line=-8)
    points(x,pchisq(x,1),col='grey',type='l')
    points(sort(lrs.sim4a),c(1:N)/N,col='#0072B2',lty=2,lwd=2.0,type='l') # blue
    points(sort(lrs.sim4b),c(1:N)/N,col='#d66d00',lty=3,lwd=2.5,type='l') # orange
    points(sort(lrs.sim4c),c(1:N)/N,col='#000000',lty=4,lwd=2.0,type='l') # black
    points(sort(lrs.sim4d),c(1:N)/N,col='#67933a',lty=6,lwd=2.0,type='l') # green
    legend(2.8,.77,c('a','b','c','d',expression(chi[0]^2/2 + chi[1]^2/2),expression(chi[1]^2)),
           cex=1.3,
           col=c('#0072B2','#d66d00','#000000','#67933a','#000000','grey'),
           lty=c(2,3,4,6,1,1),
           lwd=c(2,2.5,2,2,1,1),
           seg.len=4,
           box.col='white')
    @
    \caption{LRS}
  \end{subfigure}
  \caption[CDFs of LR statistics under branch-site model A for data simulated in the \emph{Single Foreground Branch Scenario}]{CDFs of LR statistics under branch-site model A for data simulated in the \emph{Single Foreground Branch Scenario}.  For each simulation study, 1,000 sequence alignments were generated using a balanced, 8-taxon tree topology with branch lengths summing to $3$ or $6$.  The simulated parameter values of the $\omega$ distribution are shown in each panel.  Studies 1c, 4a, and 4c have foreground branches that are $1/10$ the length of the other branches in the tree.  Studies 1d, 4b, and 4d have foreground branches that are $10$ times the length of the other branches in the tree.  Sequences are 500 codons long in studies 1a, 2a, and 3a and 5000 codons long in all the studies in this scenario.  Refer to table \ref{tab:BSALRSims} for detailed simulation conditions.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ and $\chi^2_1$ are also included.}
  \label{fig:BSSingleBranchLRs}
\end{figure}

\begin{figure}[h!]
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
    <<sim1aparams,echo=F,warning=F>>=
    par1a <- read.csv("./bs_data/params_1a.csv")
    hist(1-par1a$p0-par1a$p1,xlim=c(0,1),main='Study 1a',cex.main=3,xlab='',axes=F,ylab='',col='black',density=200,breaks=50)
    axis(1,at=seq(0,1,.1),cex.axis=3,padj=1)
    @
  \end{subfigure}
  \begin{subfigure}[t]{0.4\textwidth}
    <<sim1bparams,echo=F,warning=F>>=
    par1b <- read.csv("./bs_data/params_1b.csv")
    hist(1-par1b$p0-par1b$p1,xlim=c(0,1),main='Study 1b',cex.main=3,xlab='',axes=F,ylab='',col='black',density=200,breaks=50)
    axis(1,at=seq(0,1,.1),cex.axis=3,padj=1)
    @
  \end{subfigure}
  \\
  \begin{subfigure}[t]{0.4\textwidth}
    <<sim1cparams,echo=F,warning=F>>=
    par1c <- read.csv("./bs_data/params_1c.csv")
    hist(1-par1c$p0-par1c$p1,xlim=c(0,1),main='Study 1c',cex.main=3,xlab='',axes=F,ylab='',col='black',density=200,breaks=50)
    axis(1,at=seq(0,1,.1),cex.axis=3,padj=1)
    @
  \end{subfigure}
  \begin{subfigure}[t]{0.4\textwidth}
    <<sim1dparams,echo=F,warning=F>>=
    par1d <- read.csv("./bs_data/params_1d.csv")
    hist(1-par1d$p0-par1d$p1,xlim=c(0,1),main='Study 1d',cex.main=3,xlab='',axes=F,ylab='',col='black',density=200,breaks=50)
    axis(1,at=seq(0,1,.1),cex.axis=3,padj=1)
    @
  \end{subfigure}
  \begin{subfigure}[t]{1\textwidth}
    \centering
    \large
    $1-p_0-p_1$
  \end{subfigure}
  \caption[Histograms of the weight MLEs as $1-p_0-p_1$ for studies 1a - 1d of the \emph{Single Foreground Branch Scenario}]{Histograms of the weight MLEs as $1-p_0-p_1$ for studies 1a - 1d of the \emph{Single Foreground Branch Scenario}.}
  \label{fig:bshistsc1}
\end{figure}

For the remaining studies in the \emph{Single Foreground Branch Scenario}, with sequence data simulated with all weight on the $\omega<1$ site classes ($p_0+p_1=1$), the non-standard likelihood theory of \cite{self1987asymptotic} is not expected to apply.  The regularity condition of \cite{self1987asymptotic} that is violated is identifiability.  Because $p_0+p_1=1$, alternative hypothesis parameters with $p_{2a}=p_{2b}=0$ and arbitrary $w_2$ give the true generating model.  Unlike the results of \cite{mingrone2019}, which showed violations of the conditions of the non-standard likelihood theory gave anti-conservative \gls{lr} statistic distributions in site models, studies 2-4 resulted in conservative \gls{lr} statistic distributions.  The conditions are however similar to those described in \cite{mingrone2019} where it was argued that if $\omega_2$ were fixed under the alternative model, only $p_2=0$ would give the null model.  These conditions match case $5$ of \cite{self1987asymptotic}, which gives a $\chi_0^2/2 + \chi_1^2/2$ \gls{lr} distribution.  The alternative maximized over $\omega_2$ would always give a larger likelihood than with $\omega_2$ fixed, so the theoretical large-sample expectation is anti-conservative behaviour.  A similar theoretical argument for large-sample anti-conservative behaviour is expected for branch-site model A, but the argument is complicated, because positive weight must remain on $\omega_2$.

\subsection{LR Tests are Anti-conservative when Information Content is High}
In the \emph{Half Tree Foreground Scenario}, the estimated \gls{lr} distributions are anti-conservative relative to a $\chi_0^2/2 + \chi_1^2/2$ and not well estimated by either a $\chi_0^2/2 + \chi_1^2/2$ or $\chi_1^2$ distribution, and more so when the total tree length is increased or the number of generated sites is increased to $10,000$ (figure \ref{fig:BSHalfTreeLRs}).  With more of the tree in the foreground, the anti-conservative \gls{lr} distribution behaviour observed in \cite{mingrone2019} is also observed under branch-site model A.  In studies $7b$ and $7c$, $7.1\%$ and $7.4\%$ of the \gls{lr} statistics were beyond $2.71$, the $5\%$ threshold of the $\chi_0^2/2 + \chi_1^2/2$ distribution.  I speculate that as the information content increases with more of the tree in the foreground, the alternative model does better to explain nonsynonymous changes evident in several locations.  That extra freedom leads to anti-conservativeness.  Note that anti-conservativeness increases as $p_0+p_1$ increases, in line with expectations based on the violation of \cite{self1987asymptotic} regularity conditions when $p_0+p_1=1$ (figure \ref{fig:bshistsc2}).

\begin{figure}[h!]
  \captionsetup[subfigure]{labelformat=empty}
  \begin{subfigure}[t]{.01\textwidth}
    \vspace{58 mm}
    \rotatebox{90}{$P(X \le x)$}
  \end{subfigure}
  \hspace{3mm}
  \begin{subfigure}[t]{.96\textwidth}
    <<htfslnl,echo=F,warning=F>>=
    rm(list=ls())
    sim5.lnl <- read.csv("./bs_data/sim5_lnLs.csv")
    sim6.lnl <- read.csv("./bs_data/sim6_lnLs.csv")
    sim7a.lnl <- read.csv("./bs_data/sim7a_lnLs.csv")
    sim7b.lnl <- read.csv("./bs_data/sim7b_lnLs.csv")
    sim7c.lnl <- read.csv("./bs_data/sim7c_lnLs.csv")

    lrs.sim5 <- sort(2*(sim5.lnl$bsA-sim5.lnl$null2))
    lrs.sim6 <- sort(2*(sim6.lnl$bsA-sim6.lnl$null2))
    lrs.sim7a <- sort(2*(sim7a.lnl$bsA-sim7a.lnl$null2))
    lrs.sim7b <- sort(2*(sim7b.lnl$bsA-sim7b.lnl$null2))
    lrs.sim7c <- sort(2*(sim7c.lnl$bsA-sim7c.lnl$null2))

    N <- length(lrs.sim6)
    x <- seq(0,6.3,length.out=N)

    par(mfrow=c(1,3))

    par(mai=c(.3,.3,0,0))
    plot(x,1/2+pchisq(x,1)/2,type='l',main='',xlab='',ylab='',cex.axis=1.5)
    title("Study 5",adj=0.05,line=-2,cex.main=1.5)
    title(expression(paste(p[0],"=",p[1],"=0.375")),adj=0.07,line=-4,cex.main=1.5)
    title(expression(paste(omega[0],"=0")),adj=0.05,line=-5.5,cex.main=1.5)
    points(x,pchisq(x,1),col='grey',type='l')
    points(sort(lrs.sim5),c(1:N)/N,col='#0072B2',lty=2,lwd=2.0,type='l') # blue

    par(mai=c(.3,0,0,0))
    plot(x,1/2+pchisq(x,1)/2,type='l',main='',xlab='',cex.axis=1.5,yaxt='n')
    title("Study 6",adj=0.1,line=-2,cex.main=1.5)
    title(expression(paste(p[0],"=",p[1],"=0.475")),adj=0.12,line=-4,cex.main=1.5)
    title(expression(paste(omega[0],"=0")),adj=0.09,line=-5.5,cex.main=1.5)
    points(x,pchisq(x,1),col='grey',type='l')
    points(sort(lrs.sim6),c(1:N)/N,col='#0072B2',lty=2,lwd=2.0,type='l') # blue

    par(mai=c(.3,0,0,0))
    plot(x,1/2+pchisq(x,1)/2,type='l',yaxt='n',xlab='',ylab='',cex.axis=1.5)
    title("Studies 7a-7c",adj=0.15,line=-2,cex.main=1.5)
    title(expression(paste(p[0],"=",p[1],"=0.5")),adj=0.12,line=-3.8,cex.main=1.5)
    title(expression(paste(omega[0],"=0")),adj=0.09,line=-5.5,cex.main=1.5)
    points(x,pchisq(x,1),col='grey',type='l')
    points(sort(lrs.sim7a),c(1:N)/N,col='#0072B2',lty=2,lwd=2.0,type='l') # blue
    points(sort(lrs.sim7b),c(1:N)/N,col='#d66d00',lty=3,lwd=2.5,type='l') # orange
    points(sort(lrs.sim7c),c(1:N)/N,col='#000000',lty=4,lwd=2.0,type='l') # black

    legend(1.4,.7,c('a','b','c',expression(chi[0]^2/2 + chi[1]^2/2),expression(chi[1]^2)),
           cex=1.7,
           col=c('#0072B2','#d66d00','#000000','#000000','grey'),
           lty=c(2,3,4,1,1),
           lwd=c(2,2.5,2,1,1),
           seg.len=2.8,
           box.col='white')
    @
    \caption{LRS}
  \end{subfigure}
  \caption[CDFs of LR statistics under branch-site model A for data simulated in the \emph{Half Tree Foreground Scenario}]{CDFs of LR statistics under branch-site model A for data simulated in the \emph{Half Tree Foreground Scenario}.  For each simulation study, 1,000 sequence alignments were generated using a balanced, 8-taxon tree topology with branch lengths summing to $3$ or $6$.  The simulated paramter values of the $\omega$ distribution are shown in each panel.  Refer to table \ref{tab:BSALRSims} for detailed simulation conditions.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ and $\chi^2_1$ are also included.}
  \label{fig:BSHalfTreeLRs}
\end{figure}

\begin{figure}[h!]
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
    <<sim5params,echo=F,warning=F>>=
    par5 <- read.csv("./bs_data/params_5.csv")
    hist(1-par5$p0-par5$p1,xlim=c(0,1),main='Study 5',cex.main=3,xlab='',axes=F,ylab='',col='black',density=200,breaks=50)
    axis(1,at=seq(0,1,.1),cex.axis=3,padj=1)
    @
  \end{subfigure}
  \begin{subfigure}[t]{0.4\textwidth}
    <<sim7bparams,echo=F,warning=F>>=
    par7b <- read.csv("./bs_data/params_7b.csv")
    hist(1-par7b$p0-par7b$p1,xlim=c(0,1),main='Study 7b',cex.main=3,xlab='',axes=F,ylab='',col='black',density=200,breaks=50)
    axis(1,at=seq(0,1,.1),cex.axis=3,padj=1)
    @
  \end{subfigure}
  \begin{subfigure}[t]{1\textwidth}
    \centering
    \large
    $1-p_0-p_1$
  \end{subfigure}
  \caption[Histograms of the weight MLEs as $1-p_0-p_1$ for studies 5 and 7b of the \emph{Single Foreground Branch Scenario}]{Histograms of the weight MLEs as $1-p_0-p_1$ for studies 5 and 7b of the \emph{Half Tree Foreground Scenario}.}
  \label{fig:bshistsc2}
\end{figure}

In many circumstances, anti-conservative behaviour is more concerning than lack of power, so recommendations have been to simply use thresholds from a $\chi_1^2$ distribution to make the test conservative.  However, if \gls{lr} statistics become large with larger $\omega_2$ values, tests base on a a $\chi_1^2$ distribution could also pose risks of anti-conservative behaviour.  For few foreground branches, this is not expected, but with more of the tree in the foreground, the \gls{lr} statistic could vary more with larger $\omega_2$ values.

\subsection{LR Tests are Extremely Conservative when the $\omega$ Distribution is Misspecified}

Data in the simulation studies of the \emph{Misspecification of $\omega$ Distribution Scenario} were simulated with two different $\omega$ distributions from an M3 k=3 model \citep{yang2000codon}.  Studies $8a-8d$ placed weights $[p_0$,$p_1$,$p_2]=[0.4,0.4,0.2]$ on $[\omega_0,\omega_1,\omega_2]=[0.1,0.5,0.9]$ and studies $9a$ and $9b$ puts weights $[p_0,p_1,p_2]=[0.4,0.2,0.4]$ on $[\omega_0,\omega_1,\omega_2]=[0.1,0.5,1.0]$.  Regardless whether a single branch or half of the tree is part of the foreground, \gls{lr} statistic distributions are highly conservative when the $\omega$ is misspecified (figure \ref{fig:BSMisspecLRs}).

\begin{figure}[h!]
  \captionsetup[subfigure]{labelformat=empty}
  \begin{subfigure}[t]{.01\textwidth}
    \vspace{58 mm}
    \rotatebox{90}{$P(X \le x)$}
  \end{subfigure}
  \hspace{3mm}
  \begin{subfigure}[t]{.96\textwidth}
    <<mslnl,echo=F,warning=F>>=
    rm(list=ls())
    sim8a.lnl <- read.csv("./bs_data/sim8a_lnLs.csv")
    sim8b.lnl <- read.csv("./bs_data/sim8b_lnLs.csv")
    sim8c.lnl <- read.csv("./bs_data/sim8c_lnLs.csv")
    sim8d.lnl <- read.csv("./bs_data/sim8d_lnLs.csv")
    sim9a.lnl <- read.csv("./bs_data/sim9a_lnLs.csv")
    sim9b.lnl <- read.csv("./bs_data/sim9b_lnLs.csv")

    lrs.sim8a <- sort(2*(sim8a.lnl$bsA-sim8a.lnl$null2))
    lrs.sim8b <- sort(2*(sim8b.lnl$bsA-sim8b.lnl$null2))
    lrs.sim8c <- sort(2*(sim8c.lnl$bsA-sim8c.lnl$null2))
    lrs.sim8d <- sort(2*(sim8d.lnl$bsA-sim8d.lnl$null2))
    lrs.sim9a <- sort(2*(sim9a.lnl$bsA-sim9a.lnl$null2))
    lrs.sim9b <- sort(2*(sim9b.lnl$bsA-sim9b.lnl$null2))

    N <- length(lrs.sim8a)
    x <- seq(0,6.3,length.out=N)

    par(mfrow=c(1,2))
    par(mai=c(.5,.45,0,0))

    plot(x,1/2+pchisq(x,1)/2,type='l',main='',xlab='',ylab=expression("P(X"<="x)"))
    title("Studies 8a-8d",adj=0.65,line=-10,cex.main=1)
    title(expression(paste(p[0],"=",p[1],"=0.4 ",p[2],"=0.2")),adj=0.7,line=-12,cex.main=0.85,cex.main=1)
    title(expression(paste(omega[0],"=0.1 ",omega[1],"=0.5 ",omega[2],"=0.9"),p[2],"=0.2"),adj=0.82,line=-13,cex.main=1)
    points(x,pchisq(x,1),col='grey',type='l')
    points(sort(lrs.sim8a),c(1:N)/N,col='#0072B2',lty=2,lwd=2.0,type='l') # blue
    points(sort(lrs.sim8b),c(1:N)/N,col='#d66d00',lty=3,lwd=2.5,type='l') # orange
    points(sort(lrs.sim8c),c(1:N)/N,col='#000000',lty=4,lwd=2.0,type='l') # black
    points(sort(lrs.sim8d),c(1:N)/N,col='#67933a',lty=6,lwd=2.0,type='l') # green

    par(mai=c(.5,0,0,0))

    plot(x,1/2+pchisq(x,1)/2,type='l',main='',xlab='',yaxt='n',cex.lab=1.5)
    title("Studies 9a-9b",adj=0.65,line=-10,cex.main=1)
    title(expression(paste(p[0],"=0.4 ",p[1],"=0.2 ",p[2],"=0.4")),adj=0.8,line=-12,cex.main=1)
    title(expression(paste(omega[0],"=0.1 ",omega[1],"=0.5 ",omega[2],"=0.9"),p[2],"=0.2"),adj=0.83,line=-13,cex.main=1)
    points(x,pchisq(x,1),col='grey',type='l')
    points(sort(lrs.sim9a),c(1:N)/N,col='#0072B2',lty=2,lwd=2.0,type='l') # blue
    points(sort(lrs.sim9b),c(1:N)/N,col='#d66d00',lty=3,lwd=2.5,type='l') # orange

    legend(2.8,.7,c('a','b','c','d',expression(chi[0]^2/2 + chi[1]^2/2),expression(chi[1]^2)),
           cex=1,
           col=c('#0072B2','#d66d00','#000000','#67933a','#000000','grey'),
           lty=c(2,3,4,6,1,1),
           lwd=c(2,2.5,2,2,1,1),
           seg.len=4,
           box.col='white')
    @
  \end{subfigure}
  \begin{subfigure}[t]{1\textwidth}
    \centering
    \hspace{3em}LRS
  \end{subfigure}
  \caption[CDFs of LR statistics under branch-site model A for data simulated in the \emph{Misspecification of $\omega$ Distribution Scenario}]{CDFs of LR statistics under branch-site model A for data simulated in the \emph{Misspecification of $\omega$ Distribution Scenario}.  For each simulation study, 1,000 sequence alignments were generated using a balanced, 8-taxon tree topology with branch lengths summing to $3$ or $6$.  The simulated paramter values of the $\omega$ distributions are shown in each panel.  Refer to table \ref{tab:BSALRSims} for detailed simulation conditions.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ and $\chi^2_1$ are also included.}
  \label{fig:BSMisspecLRs}
\end{figure}

Under the null model, site class 2a is less constrained relative to the other sites classes, because it can model sites using two $\omega$ values, $\omega_0<1$ in the background and $\omega=1$ in the foreground.  This additional flexibility and perhaps near identifiability results reported in \cite{mingrone2019} makes site class 2a best able to fit sites generated under $\omega=0.5$.  However, there is little additional flexibility under the alternative model since all the generating $\omega$ values are less than $1$.  Thus, differences in likelihood scores under the null and alternative models are often small, resulting in the highly conservative \gls{lr} distribution.  When half of the tree is in the foreground, only $8$ of the $1000$ \gls{lr} tests are rejected with using the $5\%$ $\chi_0^2/2 + \chi_1^2/2$ distribution threshold of $2.71$, and only $2$ tests are rejected using the $1\%$ threshold of $5.41$.

\subsection{Confounded Foreground Branches May Cause False Detection of Positive Selection}
Figure \ref{fig:BSConfoundLRs} shows estimated \gls{lr} statistic \glspl{cdf} under branch-site model A for simulation studies 10, 11, and 12 in the \emph{Confounding Foreground Scenario}.  In studies 10 and 11, the \gls{lr} statistics are conservative and distributions are comparable with or without confounding.  By contrast, in Study 12, strong anti-conservative behaviour is seen.  The more anti-conservative behaviour under the null by comparison with Studies 10 and 11, suggests that part of the reason is simply the additional information content in the alternative model when more of the tree is in the foreground.  It is surprising that the results are so anti-conservative with confounding as the nonsynonymous changes are expected primarily outside of the foreground specified in the fitted model.  Perhaps the ability of the alternative components of the model to explain at least one nonsynonymous change when many occur due to the confounding leads to much more explanatory value under the alternative.

\begin{figure}[h!]
  \centering
  \captionsetup[subfigure]{labelformat=empty}
  \begin{subfigure}[t]{.01\textwidth}
    \vspace{62 mm}
    \rotatebox{90}{$P(X \le x)$}
  \end{subfigure}
  \hspace{3mm}
  \begin{subfigure}[t]{.95\textwidth}
    <<simcsc,echo=F,warning=F>>=
    rm(list=ls())
    simc10.lnl <- read.csv("./bs_data/simc10_lnLs.csv")
    simc10n.lnl <- read.csv("./bs_data/simc10n_lnLs.csv")
    simc11.lnl <- read.csv("./bs_data/simc11_lnLs.csv")
    simc11n.lnl <- read.csv("./bs_data/simc11n_lnLs.csv")
    simc12.lnl <- read.csv("./bs_data/simc12_lnLs.csv")
    simc12n.lnl <- read.csv("./bs_data/simc12n_lnLs.csv")

    lrs.simc10 <- sort(2*(simc10.lnl$bsA-simc10.lnl$null2))
    lrs.simc10n <- sort(2*(simc10n.lnl$bsA-simc10n.lnl$null2))
    lrs.simc11 <- sort(2*(simc11.lnl$bsA-simc11.lnl$null2))
    lrs.simc11n <- sort(2*(simc11n.lnl$bsA-simc11n.lnl$null2))
    lrs.simc12 <- sort(2*(simc12.lnl$bsA-simc12.lnl$null2))
    lrs.simc12n <- sort(2*(simc12n.lnl$bsA-simc12n.lnl$null2))

    N <- length(lrs.simc10)
    x <- seq(0,6.3,length.out=N)

    par(mfrow=c(3,2),mai=c(0,.3,0.1,0))

    plot(x,1/2+pchisq(x,1)/2,type='l',ylab='',xaxt='n',ann=F,main="Study 1")
    title("Study 10",adj=0.9,line=-5)
    ##title(expression(paste(p[0],"=0.7 ",p[1],"=0.2 ",omega[0],"=0.3")),adj=0.9,line=-8)
    title(expression(paste(p[0],"=0.5 ",p[1],"=0.4 ",omega[0],"=0.75 ",omega[2],"=2.0")),adj=0.9,line=-8)
    points(x,pchisq(x,1),col='grey',type='l')
    points(sort(lrs.simc10),c(1:N)/N,col='#0072B2',lty=2,lwd=2.0,type='l') # blue
    points(sort(lrs.simc10n),c(1:N)/N,col='#d66d00',lty=3,lwd=2.5,type='l') # orange
    legend(2.5,.73,c('Null','Confound',expression(chi[0]^2/2 + chi[1]^2/2),expression(chi[1]^2)),
         cex=1.5,
         col=c('#d66d00','#0072B2','#000000','grey'),
         lty=c(3,2,1,1),
         lwd=c(2,2.5,1,1),
         seg.len=4,
         box.col='white')

    tree <- read.tree(text="(((A:0.214286,B:0.214286):0.214286,(C:0.214286,D:0.214286):0.214286):0.214286,((E:0.214286,F:0.214286):0.214286,(G:0.214286,H:0.214286):0.214286):0.214286);");
    edge.colors <- rep(1,68)
    ##edge.colors[c(12,14,33,34,36,38,53,54,55,59)] <- brewer.pal(n=10,name="Paired")
    edge.colors[c(2,5)] <- c("#0000FF","#8B0000")
    ##edge.widths <- rep(2,68)
    edge.widths <- rep(4,14)
    ##edge.widths[c(2,5)] <- 6

    par(mai=c(.3,0,0,0))
    plot(tree,cex=1.5,type="phylogram",edge.color=edge.colors,edge.width=edge.widths,lab4ut="axial",show.node.label=F,label.offset=0.01)
    ##edgelabels(tree$edge.length, bg="black", col="white", font=2)
    ##add.scale.bar()
    ##branches.tested <- c('Vampyroteuthis','Sepiida','Groenlandibelids','Idiosepius','Sepiolida','Loliginidae','Oegop_Bathy','Bathyteuthis','Oegopsida','Ommastrepidae')
    legend(0,8.7,c('Simulated','Fitted'),c("#8B0000","#0000FF"),box.lty=0,cex=1.4)

    par(mai=c(0,.3,0,0))
    plot(x,1/2+pchisq(x,1)/2,type='l',ylab='',xaxt='n',ann=F,main="Study 1")
    title("Study 11",adj=0.9,line=-5)
    title(expression(paste(p[0],"=0.475 ",p[1],"=0.475 ",omega[0],"=0 ",omega[2],"=3.0")),adj=0.95,line=-9)
    ##title(expression(paste(p[0],"=0.7 ",p[1],"=0.2 ",omega[0],"=0.3")),adj=0.9,line=-8)
    points(x,pchisq(x,1),col='grey',type='l')
    points(sort(lrs.simc11),c(1:N)/N,col='#0072B2',lty=2,lwd=2.0,type='l') # blue
    points(sort(lrs.simc11n),c(1:N)/N,col='#d66d00',lty=3,lwd=2.5,type='l') # orange

    tree <- read.tree(text="(((A:0.214286,B:0.214286):0.214286,(C:0.214286,D:0.214286):0.214286):0.214286,((E:0.214286,F:0.214286):0.214286,(G:0.214286,H:0.214286):0.214286):0.214286);");
    edge.colors <- rep(1,14)
    edge.colors[c(2,5)] <- c("#0000FF","#8B0000")
    edge.widths <- rep(4,14)

    par(mai=c(.3,0,0,0))
    plot(tree,cex=1.5,type="phylogram",edge.color=edge.colors,edge.width=edge.widths,lab4ut="axial",show.node.label=F,label.offset=0.01)
    ##edgelabels(tree$edge.length, bg="black", col="white", font=2)
    ##add.scale.bar()

    par(mai=c(0.3,0.3,0,0))
    plot(x,1/2+pchisq(x,1)/2,type='l',ylab='',,ann=F,main="Study 1")
    title("Study 12",adj=0.9,line=-6)
    title(expression(paste(p[0],"=0.475 ",p[1],"=0.475 ",omega[0],"=0 ",omega[2],"=3.0")),adj=0.95,line=-11)
    points(x,pchisq(x,1),col='grey',type='l')
    points(sort(lrs.simc12),c(1:N)/N,col='#0072B2',lty=2,lwd=2.0,type='l') # blue
    points(sort(lrs.simc12n),c(1:N)/N,col='#d66d00',lty=3,lwd=2.5,type='l') # orange

    tree <- read.tree(text="(((A:0.214286,B:0.214286):0.214286,(C:0.214286,D:0.214286):0.214286):0.214286,((E:0.214286,F:0.214286):0.214286,(G:0.214286,H:0.214286):0.214286):0.214286);");
    edge.colors <- rep(1,14)
    ##edge.colors[c(12,14,33,34,36,38,53,54,55,59)] <- brewer.pal(n=10,name="Paired")
    edge.colors[seq(2,7)] <- c("#0000FF")
    edge.colors[seq(9,14)] <- c("#8B0000")
    edge.widths <- rep(4,14)

    par(mai=c(.3,0,0,0))
    plot(tree,cex=1.5,type="phylogram",edge.color=edge.colors,edge.width=edge.widths,lab4ut="axial",show.node.label=F,label.offset=0.01)
    ##edgelabels(tree$edge.length, bg="black", col="white", font=2)
    ##add.scale.bar()
    ##branches.tested <- c('Vampyroteuthis','Sepiida','Groenlandibelids','Idiosepius','Sepiolida','Loliginidae','Oegop_Bathy','Bathyteuthis','Oegopsida','Ommastrepidae')
    @
    %\caption{LRS}
  \end{subfigure}
  \caption[CDFs of LR statistics under branch-site model A for data simulated in the \emph{Confounding Foreground Scenario}]{CDFs of LR statistics under branch-site model A for data simulated in the \emph{Confounding Foreground Scenario}.  For each simulation study, 1,000 sequence alignments were generated using a balanced, 8-taxon tree topology with branch lengths summing to $3$ or $6$.  Under the Null simulation conditions no foreground branches were specified and under the confounding simulatoin conditions, the foreground is shown in the right panel, which does not match the foreground under the fitted model.  The simulated paramter values of the $\omega$ distribution are shown for each study.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ and $\chi^2_1$ are also included.}
  \label{fig:BSConfoundLRs}
\end{figure}

\subsection{Summary}

The simulation results show that \gls{lr} distributions under the null hypothesis are sometimes poorly approximated by those predicted by theory.
% , i.e., $\chi_0^2+\chi_1^2/2$, or the more conservative distribution $\chi_1^2$.
The \gls{lr} distributions can vary heavily according to factors such as the branches considered for positive selection and the irregularity of certain parameter estimates.  In some cases, the test is so lacking in power, that a re-calibration of the null \gls{lr} distribution should be considered.  In other scenarios, when more of the tree is in the foreground, excessively high false positive rates were observed.


Other simulation results show that when positive selection has occurred in background locations of the phylogeny, detection of positive selection in the foreground can be strongly influenced and in some cases, excessive false detection of positive selection may occur.

\chapter{Concluding Remarks}
Some assumptions made in models of molecular evolution are extremely unrealistic.  Assuming equal selection pressure over all amino sites in a protein is one example.  As most proteins must maintain the capacity to fold into complex structures to complete some biological function, the majority of acid substitutions are selected against.  Estimating a single $\omega$ parameter averaged over all sites in the protein will usually lack sufficient power to detect the relatively few sites that may be subjected to positive selection.  By modelling each site as the realized value from an $\omega$ distribution, the increased power to detect a few sites under positive selection made the models practical for wider use.  However, increasing model complexity to account for more of the complex evolutionary processes that give rise to the diversity in homologous proteins can have both favourable and unfavourable consequences.

Mixture models of codon evolution, such as those described in Chapter \ref{chap:modl}, violate regularity conditions required for standard likelihood theory.  Indeed, likelihood theory does not support any particular large sample null distribution and the correct distribution is dependent on parameter values of the generating null distribution.  Another difficultly with mixture models of codon evolution that have been described in Chapter \ref{chap:modl} is near unidentifiability.  With few taxa and small branch lengths, two very different $\omega$ distributions can give nearly the same site pattern probabilities, making estimation and inference challenging.

The modified likelihood approach, described in Chapter \ref{chap:modl}, adds a penalty for small weight on $\omega > 1$ to likelihood calculations.  Simulation results show that this modification, in many cases, gives tractable \gls{lr} statistic distributions that are well approximated by a $\chi_0^2+\chi_1^2/2$
distribution and helps to adequately control false positive rates with minimal impact on power.  Potential future work related to the modified likelihood approach is (i) calculation of an optimal likelihood penalty using, e.g., a cross-validation procedure and (ii) application to other models of molecular evolution.

When the null hypothesis of no positive selection at the protein level is rejected by the \gls{lr} test, a determination whether positive selection has acted upon particular amino acid sites is warranted.  This site-wise analysis, carried out using Bayes rule, is dependent on the \gls{ml}-estimated model parameters.  Parameter values that are estimated with high error can lead to unacceptably high levels of false detection of positive selection at amino acid sites.  The \gls{beb} approach adjusts for uncertainty in some parameter estimates, but the approach has limitations.

In Chapter \ref{chap:sba}, \gls{sba}, a new alternative to \gls{beb} that accommodates uncertainty in all \glspl{mle}, is described.  For each amino acid site, many parameter values are generated from a smoothed bootstrap distribution and substituted into the posterior probability calculation to give a distribution of posterior probabilities which reflects parameter uncertainty.  By accounting for errors associated with all model parameters, \gls{sba}, relative to \gls{beb}, has advantages.  Simulations under model M8, using data simulated to reflect many real data conditions with mild model misspecification, showed that \gls{sba} provided consistently better control of false positive rates than \gls{beb}.  Other advantages of \gls{sba} are that (i) it provides visualization of the \gls{mle} distributions over bootstrap samples providing insight into the degree of irregularity of the estimation and (ii) it is comparatively simple to implement for new models.  Some potential refinements to the \gls{sba} approach are left for future work such as determining an optimal bandwidth parameter for smoothing.  Under-smoothing will not correct irregularities in the bootstrap parameter distributions and over smoothing will remove useful information and increase bias.

Branch-site models, such as those described in Chapter \ref{chap:bs} have been purported to have more power to detect instances of episodic positive selection that has acted upon some sites along particular lineages of the tree.  Through simulation studies, I have identified challenges with the model.  With low information content in the foreground of the tree, e.g., when there is a single, shorter branch in the foreground, or when the generating model does not match well to the fitted model, the test for positive selection can be very conservative.  On the other hand, when there is more information content in the foreground, the \gls{lr} test can be anti-conservative.  Application of the modified likelihood approach described in Chapter \ref{chap:modl} to branch-site model A would likely be beneficial for obtaining tractable \gls{lr} distributions when more of the tree is in foreground.  Perhaps of most concern are the results when the foreground of the generated model does not match with the foreground of the fitted model.  Despite past reports that the model is not misled by positive selection in background branches \citep{zhang2005evalimprovedbs, Branch-SiteTestRobust, EffectErrorsBranch-Site}, a simulation result from Chapter \ref{chap:bs} strongly contradicts this assertion.  While some potential pitfalls have been identified with branch-site model A, more work is warranted to better understand why, e.g., the model can be so strongly misled by positive selection in other parts of the tree.

% \chapter{Supplementary Materials}
\chapter*{Supplementary Materials}\stepcounter{chapter}\addcontentsline{toc}{chapter}{Supplementary Materials}
\captionsetup[figure]{list=no}
\captionsetup[table]{list=no}
\section{Appendix I: Proof of the Limiting Distribution of the Modified Likelihood}
\label{sec:modlProof}
\singlespacing
%\setcounter{equation}{0}
We prove below that the limiting distribution of the modified likelihood ratio statistic is $\chi_0^2/2 + \chi_1^2/2$.  We assume without proof that the codon model considered is identifiable: for a fixed tree, no two distinct sets of parameters give exactly the same distribution of site patterns.  Such results have not been established for codon models.  However, there are a number identifiability results for similar rates-across-sites \citep{allman2008identifiability} and covarion models \citep{allman2009identifiability} that suggest it is a plausible assumption.  We assume as well that third partial derivatives of the probability of a site pattern, over any set of parameters, is bounded in a neighbourhood of the true parameter values.  Finally we assume that the covariance matrix $V$ defined below is positive definite.

\subsection*{Taylor's Series}
Let $\beta=(\omega_+,\psi^T)^T$ and let $\beta^{0}=[1,(\psi^{0})^T]^T$ where $\psi^0$ denotes the true generating parameter under the null hypothesis.  It follows similarly as in \cite{chen2004testing} that $\hat\beta \rightarrow \beta^0$ where $\hat\beta$ is the modified ML estimator.  Since the convergence of the modified ML estimator $\hat p_+$ of $p_+$ is at present unclear, we approximate modified likelihood ratios through Taylor's series approximation of the log likelihoods, with respect to $\beta$ at $\beta^0$, holding $p_+$ fixed:
\begin{equation}
  \label{eq:a1}
\tilde l(p_+,\omega_+,\psi) -  l_H(\psi^0) = L(\beta^0;p_+) + Q(\beta^0;p_+) + C(\beta^*;p_+) + C\log(p_+)
\end{equation}
where $L$, $Q$ and $C$ denote the linear, quadratic and cubic terms and $\beta^*$ is some value between $\beta$ and $\beta^0$.

\subsection*{Linear Term}
The linear term is
\begin{equation}
  \nonumber
  L(\beta^0;p_+) = (\beta - \beta^0)^T \sum_h \pd{}{\beta} \log p(x_h;\beta^0,p_+)
\end{equation}
It is not difficult to show that the collection, $S(x_h)_\psi,$ of derivatives of $\log p(x_h;\beta^0,p_+)$ with respect to $\psi$ are independent of $p_+$.  The other derivative is
\begin{equation}
  \label{eq:a3}
  \pd{}{\omega_+} \log p(x_h;\beta^0,p_+) = p_+ (1-p_0) \pd{}{\omega} p(x|1;\zeta)/p(x_h;\beta^0,p_+) =: p_+ S(x_h)_{\omega_+}
\end{equation}
so that $S(x_h)^T=[S(x_h)_{\omega_+}, S(x_h)_\psi^T]$ is independent of $p_+$.  Standard likelihood theory gives that $E[S(X_h)]=0$, so by the Central Limit Theorem, $n^{-1/2}S_n=n^{-1/2}\sum S(X_h)$ is approximately normal with mean 0 and a covariance matrix we denote by $V$.  Let $\delta_n^T=\sqrt{n}[p_+(\omega_+-1), (\psi - \psi^0)^T].$  Then
\begin{equation}
  \label{eq:a4}
  L(\beta^0;p_+) = n^{-1/2}S_n^T \delta_n
\end{equation}

\subsection*{Quadratic Term}
The quadratic term in (\ref{eq:a1}) is
\begin{equation}
  \label{eq:a5}
  Q(\beta^0;p_+) = \frac{1}{2} (\beta - \beta^0)^T l^{(2)}(\beta^0)(\beta - \beta^0)
\end{equation}
where
\begin{equation}
  \label{eq:a6}
  l^{(2)}(\beta^0) = \sum_h Q^{(1)}(x_h;\beta^0,p_+) + \sum_h \pd{}{\beta} \log p(x_h;\beta^0,p_+) \pd{}{\beta} \log p(x_h;\beta^0,p_+)^T
\end{equation}
and $p_H(x_h) Q^{(1)}(x_h;\beta^0,p_+)_{ij}$ is the partial derivative of $p(x_h;\beta^0,p_+)$ with respect to $\beta_i$ and $\beta_j$.  It is not difficult to see that $p_H(x_h) Q^{(1)}(x_h;\beta^0,p_+)_{ij}$ is independent of $p_+$ unless $i=j=1$, in which case it equals $p_+$ times a partial derivative of the form $\partial^2 p(x|1;\zeta)/\partial \omega^2$.  Standard likelihood theory gives that $E[Q^{(1)}(X_h;\beta^0,p_+)]=0$.  Thus the Central Limit Theorem gives that $Q_n^{(1)}:=\sum Q^{(1)}(x_h;\beta^0,p_+)=O_P(n^{1/2})$ for any fixed $p_+$.  Since $Q_n^{(1)}$ depends linearly on $p_+$, $Q_n^{(1)}=O_p(n^{1/2})$ uniformly in $p_+$.  Substituting in (\ref{eq:a6}), then (\ref{eq:a5}) and using the relationships between derivatives of $\log p(x_h;\beta^0,p_+)$ and $S(x_h)$ established earlier,
\begin{equation}
  \label{eq:a7}
  Q(\beta^0;p_+) = \frac{1}{2}(\beta - \beta^0)^TQ_n^{(1)}(\beta - \beta^0) + \frac{1}{2n}\delta_n^T \sum_h S(x_h) S(x_h)^T \delta_n
\end{equation}
Let $Q_n^{(2)}=\sum_h S(x_h) S(x_h)^T-nV$.  Since $E[S(x_h) S(x_h)^T]=V$, the Central Limit Theorem gives that $Q_n^{(2)}=O_P(n^{1/2})$.  Since
\begin{equation}
  \label{eq:a18}
  Q(\beta^0;p_+) = \frac{1}{2}(\beta - \beta^0)^TQ_n^{(1)}(\beta - \beta^0) + \frac{1}{2n}\delta_n^T Q_n^{(2)} \delta_n - \frac{1}{2}\delta_n^T V \delta_n
\end{equation}
for $\delta_n=O_P(1)$ we have that
\begin{equation}
  \label{eq:a8}
  Q(\beta^0;p_+) = - \frac{1}{2}\delta_n^T V \delta_n + O_P(n^{-1/2})
\end{equation}

\subsection*{Cubic Term}
The cubic term in (\ref{eq:a1}) is
\begin{equation}
  \label{eq:a9}
  C(\beta^*;p_+) = \frac{1}{6} \sum_{ijk} l^{(3)}(\beta^*;p_+)_{ijk} (\beta - \beta^0)_i(\beta - \beta^0)_j (\beta - \beta^0)_k
\end{equation}
where $l^{(3)}(\beta^*;p_+)_{ijk}$ denotes the third partial derivative of the log likelihood with respect to $\beta_i$, $\beta_j$ and $\beta_k$.  Since we have assumed third partial derivatives of $\log p(x_h;\beta^0,p_+)$ are bounded in a neighbourhood of $\beta^0$ by, say, $M(x_h)$, $|l^{(3)}(\beta^*;p_+)|/n$ is bounded by $n^{-1}\sum_h M(X_h).$  It follows by the Law of Large Numbers that $l^{(3)}(\beta^*;p_+)=O_P(n)$ and consequently that for $\beta-\beta^0=O_P(n^{-1/2})$, $C(\beta^*;p_+)=O_p(n^{-1/2})$.  Combining (\ref{eq:a4}) and (\ref{eq:a8}) in (\ref{eq:a1}) gives that for $\delta_n=O_P(1)$,
\begin{equation}
  \label{eq:a10}
  \tilde l(p_+,\omega_+,\psi) - l_H(\psi^0) = n^{-1/2}S_n^T \delta_n - \frac{1}{2}\delta_n^T V \delta_n + C\log(p_+)+ O_P(n^{-1/2})
\end{equation}

\subsection*{Approximation with the modified MLE}
Since $\hat\delta$ has not been shown to be equal to $O_p(1)$, (\ref{eq:a10}) does not immediately apply.  However, since $b_n=\hat\delta/|\hat\delta|=O_p(1)$, the argument for (\ref{eq:a10}) gives that
\begin{equation}
  \label{eq:a11}
  \tilde l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\psi^0) =  |\hat\delta|^2 \{n^{-1/2}S_n^T b_n/ |\hat\delta| - \frac{1}{2}b_n^T V b_n + O_P(n^{-1/2})\} + C\log(\hat p_+)
\end{equation}
Since $V$ is positive definite, the right-hand side of (\ref{eq:a11}) becomes negative if $|\hat\delta|$ diverges.  However, since $\hat\beta$ is a maximizer, the difference in (\ref{eq:a11}) is always positive.  Thus it must be the case that $\hat\delta=O_p(1)$.  This implies that $\hat\psi-\psi^0=O_P(n^{-1/2})$ and that $\hat p_+ (\hat\omega_+ - 1) =O_P(n^{-1/2})$.  Similarly as in Lemma 1 of \cite{chen2004testing}, with probability, converging to 1, $\hat p_+\ge \epsilon$ for some $\epsilon > 0$, so that $\hat\omega_+ - 1=O_P(n^{-1/2})$.  Thus the approximation (\ref{eq:a10}) applies with $\beta=\hat\beta$:
\begin{eqnarray}
  \nonumber
  \tilde l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\psi^0) &=& n^{-1/2}S_n^T \hat\delta_n - \frac{1}{2}\hat\delta_n^T V \hat\delta_n + C\log(\hat p_+)+ O_P(n^{-1/2})\\
\label{eq:a12}
&\le& \max_{\delta,p_+} \{n^{-1/2}S_n^T \delta - \frac{1}{2} \delta^T V \delta + C\log(p_+)\} + O_P(n^{-1/2})
\end{eqnarray}
The inequality (\ref{eq:a12}) holds when maximization is restricted so that the maximizing $\delta$ and $p_+$ correspond to a valid $\beta$ and $p_+$: $\delta_{\omega_+} \ge 0$ and $p_+ \le 1$.  If we denote the corresponding $\beta$ and $p_+$ as $\tilde\beta$ and $\tilde p_+$, since the maximizing $\delta$ and $p_+$ are $O_P(1)$, the expression in (\ref{eq:a12}) is the same as $\tilde l(\tilde p_+,\tilde\omega_+,\tilde\psi) - l_H(\psi^0)$ up to the order indicated.  Since $\tilde  l(\hat p_+,\hat\omega_+,\hat\psi)- l_H(\psi^0)$ is larger than $\tilde l(\tilde p_+,\tilde\omega_+,\tilde\psi) - l_H(\psi^0)$, the reverse inequality holds in (\ref{eq:a12}) as well, implying that it is an equality.  The maximized value of $C\log(p_+)$ is $C\log(1)=0$.  Thus (\ref{eq:a12}) is
\begin{equation}
  \label{eq:a13}
  \tilde l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\psi^0) = \max_{\delta} \{n^{-1/2}S_n^T \delta - \frac{1}{2} \delta^T V \delta\}+ O_P(n^{-1/2})
\end{equation}

\subsection*{The log likelihood under the null}
No modification of the likelihood is considered under the null, so standard ML results gives that
\begin{equation}
  \label{eq:a20}
l_H(\psi)- l_H(\psi^0) = (n^{-1/2}S_{n\psi})^TV_{\psi\psi}^{-1}(n^{-1/2}S_{n\psi} )+  O_P(n^{-1/2})
\end{equation}
The difference between (\ref{eq:a12}) and (\ref{eq:a20}) gives that the modified likelihood ratio satisfies that
\begin{equation}
  \label{eq:a14}
 \tilde l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\psi) =  \max_{\delta} \{2n^{-1/2}S_n^T \delta - \delta^T V \delta\}
-  (n^{-1/2}S_{n\psi})^TV_{\psi\psi}^{-1}(n^{-1/2}S_{n\psi} )+  O_P(n^{-1/2})
\end{equation}

\subsection*{Maximization under the alternative}
Omitting details, after simplification, maximizing over $\delta$ with $\delta_{\omega_+}$ fixed gives
\begin{equation}
  \label{eq:a15}
 \max_{\delta} \{2n^{-1/2}S_n^T \delta - \delta^T V \delta\}
=   (n^{-1/2}S_{n\psi})^TV_{\psi\psi}^{-1}(n^{-1/2}S_{n\psi} ) + 2 \delta_{\omega_+} n^{-1/2} S_{n\omega_+}^c - \delta_{\omega_+}^2 V_{\omega_+}^c
\end{equation}
where
\begin{equation}
  \label{eq:a16}
  S_{n\omega_+}^c = S_{n\omega_+} - V_{\omega_+\psi} V_{\psi\psi}^{-1} S_{n\psi}, ~~~ V_{\omega_+}^c = V_{\omega_+\omega_+} - V_{\omega_+\psi} V_{\psi\psi}^{-1} V_{\psi\omega_+}
\end{equation}
If $S_{n\omega_+}^c < 0$ in (\ref{eq:a15}) the right hand side is decreasing in $\delta_{\omega_+}$ and so, subject to the restriction that $\delta_{\omega_+}\ge 0$, the maximizing $\delta_{\omega_+}=0$.  Otherwise the maximizer is $n^{-1/2} S_{n\omega_+}^c/\sqrt{V_{\omega_+}^c}$.  Substituting in (\ref{eq:a15}) and (\ref{eq:a14}) gives
\begin{equation}
  \label{eq:a17}
 \tilde l(\hat p_+,\hat\omega_+,\hat\psi) - l_H(\psi) = [n^{-1/2} S_{n\omega_+}^c/\sqrt{V_{\omega_+}^c}]_+^2 + O_P(n^{-1/2})
\end{equation}

\subsection*{Distribution of $S_{n\omega_+}^c$}
Because $n^{-1/2}S_{n\omega_+}^c$ is a linear transformation of $n^{-1/2}S_n$ which has an approximate normal distribution with mean 0 and covariance matrix $V$, it too has a normal distribution.  It has mean 0 and variance
\begin{eqnarray}
\nonumber
  {\rm Var}(n^{-1/2}S_{n\omega_+}^c) &=& {\rm Var}(n^{-1/2}S_{n\omega_+}) - 2 V_{\omega_+\psi} V_{\psi\psi}^{-1} {\rm Cov}(n^{-1/2}S_{n\psi},n^{-1/2}S_{n\omega_+}) \\
\nonumber
&&~~~~~~~~~
+ V_{\omega_+\psi} V_{\psi\psi}^{-1}  {\rm Var}(n^{-1/2}S_{n\psi})  V_{\psi\psi}^{-1} V_{\psi\omega_+}\\
\nonumber
&=& V_{\omega_+\omega_+} - 2 V_{\omega_+\psi} V_{\psi\psi}^{-1} V_{\psi\omega_+} + V_{\omega_+\psi} V_{\psi\psi}^{-1} V_{\psi\psi} V_{\psi\psi}^{-1} V_{\psi\omega_+} = V_{\omega_+}^c
\end{eqnarray}
Thus $n^{-1/2}S_{n\omega_+}^c/\sqrt{V_{\omega_+}^c}$ is approximately standard normal.  It follows from $Z\sim N(0,1)$ giving $Z^2\sim \chi_1^2$ and $Z^2$ being independent of the event that $Z>0$, that the limiting distribution of $W$ is $\chi_1^2/2 + \chi_0^2/2.$

\clearpage

\section{Modified Likelihood Supplementary Figures}

\begin{figure}[H]
    \centering
    <<CDF5taxaTL6,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.25_w0_0.25_5_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.25_w0_0.25_5_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.25_w0_0.25_5_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.25_w0_0.5_5_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.25_w0_0.5_5_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.25_w0_0.5_5_taxa_tl_6_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.5_w0_0.25_5_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.5_w0_0.25_5_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.5_w0_0.25_5_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.5_w0_0.5_5_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.5_w0_0.5_5_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.5_w0_0.5_5_taxa_tl_6_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.75_w0_0.25_5_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.75_w0_0.25_5_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.75_w0_0.25_5_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.75_w0_0.5_5_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.75_w0_0.5_5_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_6/data/p0_0.75_w0_0.5_5_taxa_tl_6_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_5_taxa_m2a_lnl-p0_0.25_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_5_taxa_c2_lnl-p0_0.25_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_5_taxa_m2a_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_5_taxa_c2_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_5_taxa_m2a_lnl-p0_0.5_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_5_taxa_c2_lnl-p0_0.5_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_5_taxa_m2a_lnl-p0_0.5_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_5_taxa_c2_lnl-p0_0.5_w0_0.5_5_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_5_taxa_m2a_lnl-p0_0.75_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_5_taxa_c2_lnl-p0_0.75_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_5_taxa_m2a_lnl-p0_0.75_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_5_taxa_c2_lnl-p0_0.75_w0_0.5_5_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_5_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_5_taxa_m2a,lrs_p0_0.25_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_5_taxa_m2a,lrs_p0_0.25_w0_0.5_5_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_5_taxa_m2a,lrs_p0_0.5_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_5_taxa_m2a,lrs_p0_0.5_w0_0.5_5_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_5_taxa_m2a,lrs_p0_0.75_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_5_taxa_m2a,lrs_p0_0.75_w0_0.5_5_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of likelihood (C=0) and modified likelihood (C=2) ratio statistics under the nested model pair M1a/M2a for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 6.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF5taxaTL6}
\end{figure}

\begin{figure}[H]
    \centering
    <<CDF5taxaTL9,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.25_w0_0.25_5_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.25_w0_0.25_5_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.25_w0_0.25_5_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.25_w0_0.5_5_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.25_w0_0.5_5_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.25_w0_0.5_5_taxa_tl_9_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.5_w0_0.25_5_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.5_w0_0.25_5_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.5_w0_0.25_5_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.5_w0_0.5_5_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.5_w0_0.5_5_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.5_w0_0.5_5_taxa_tl_9_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.75_w0_0.25_5_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.75_w0_0.25_5_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.75_w0_0.25_5_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.75_w0_0.5_5_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.75_w0_0.5_5_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_9/data/p0_0.75_w0_0.5_5_taxa_tl_9_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_5_taxa_m2a_lnl-p0_0.25_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_5_taxa_c2_lnl-p0_0.25_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_5_taxa_m2a_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_5_taxa_c2_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_5_taxa_m2a_lnl-p0_0.5_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_5_taxa_c2_lnl-p0_0.5_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_5_taxa_m2a_lnl-p0_0.5_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_5_taxa_c2_lnl-p0_0.5_w0_0.5_5_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_5_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_5_taxa_m2a_lnl-p0_0.75_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_5_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_5_taxa_c2_lnl-p0_0.75_w0_0.25_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_5_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_5_taxa_m2a_lnl-p0_0.75_w0_0.5_5_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_5_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_5_taxa_c2_lnl-p0_0.75_w0_0.5_5_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_5_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_5_taxa_m2a,lrs_p0_0.25_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_5_taxa_m2a,lrs_p0_0.25_w0_0.5_5_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_5_taxa_m2a,lrs_p0_0.5_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_5_taxa_m2a,lrs_p0_0.5_w0_0.5_5_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_5_taxa_m2a,lrs_p0_0.75_w0_0.25_5_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_5_taxa_m2a,lrs_p0_0.75_w0_0.5_5_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 9.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF5taxaTL9}
\end{figure}

\begin{figure}[H]
    \centering
    <<CDF10taxaTL3,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.25_w0_0.25_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.25_w0_0.25_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.25_w0_0.25_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.25_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.25_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.25_w0_0.5_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.5_w0_0.25_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.5_w0_0.25_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.5_w0_0.25_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.5_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.5_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.5_w0_0.5_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.75_w0_0.25_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.75_w0_0.25_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.75_w0_0.25_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.75_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.75_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_3/data/p0_0.75_w0_0.5_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_10_taxa_m2a_lnl-p0_0.25_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_10_taxa_c2_lnl-p0_0.25_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_10_taxa_m2a_lnl-p0_0.25_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_10_taxa_c2_lnl-p0_0.25_w0_0.5_10_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_10_taxa_m2a_lnl-p0_0.5_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_10_taxa_c2_lnl-p0_0.5_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_10_taxa_m2a_lnl-p0_0.5_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_10_taxa_c2_lnl-p0_0.5_w0_0.5_10_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_10_taxa_m2a_lnl-p0_0.75_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_10_taxa_c2_lnl-p0_0.75_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_10_taxa_m2a_lnl-p0_0.75_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_10_taxa_c2_lnl-p0_0.75_w0_0.5_10_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_10_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_10_taxa_m2a,lrs_p0_0.25_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_10_taxa_m2a,lrs_p0_0.25_w0_0.5_10_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_10_taxa_m2a,lrs_p0_0.5_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_10_taxa_m2a,lrs_p0_0.5_w0_0.5_10_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_10_taxa_m2a,lrs_p0_0.75_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_10_taxa_m2a,lrs_p0_0.75_w0_0.5_10_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a balanced, 10-taxon tree topology with branch lengths summing to 3.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF10taxaTL3}
\end{figure}

\begin{figure}[H]
    \centering
    <<CDF10taxaTL6,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.25_w0_0.25_10_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.25_w0_0.25_10_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.25_w0_0.25_10_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.25_w0_0.5_10_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.25_w0_0.5_10_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.25_w0_0.5_10_taxa_tl_6_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.5_w0_0.25_10_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.5_w0_0.25_10_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.5_w0_0.25_10_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.5_w0_0.5_10_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.5_w0_0.5_10_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.5_w0_0.5_10_taxa_tl_6_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.75_w0_0.25_10_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.75_w0_0.25_10_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.75_w0_0.25_10_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.75_w0_0.5_10_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.75_w0_0.5_10_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_6/data/p0_0.75_w0_0.5_10_taxa_tl_6_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_10_taxa_m2a_lnl-p0_0.25_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_10_taxa_c2_lnl-p0_0.25_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_10_taxa_m2a_lnl-p0_0.25_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_10_taxa_c2_lnl-p0_0.25_w0_0.5_10_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_10_taxa_m2a_lnl-p0_0.5_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_10_taxa_c2_lnl-p0_0.5_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_10_taxa_m2a_lnl-p0_0.5_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_10_taxa_c2_lnl-p0_0.5_w0_0.5_10_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_10_taxa_m2a_lnl-p0_0.75_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_10_taxa_c2_lnl-p0_0.75_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_10_taxa_m2a_lnl-p0_0.75_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_10_taxa_c2_lnl-p0_0.75_w0_0.5_10_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_10_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_10_taxa_m2a,lrs_p0_0.25_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_10_taxa_m2a,lrs_p0_0.25_w0_0.5_10_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_10_taxa_m2a,lrs_p0_0.5_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_10_taxa_m2a,lrs_p0_0.5_w0_0.5_10_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_10_taxa_m2a,lrs_p0_0.75_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_10_taxa_m2a,lrs_p0_0.75_w0_0.5_10_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a balanced, 10-taxon tree topology with branch lengths summing to 6.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF10taxaTL6}
\end{figure}

\begin{figure}[H]
    \centering
    <<CDF10taxaTL9,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.25_w0_0.25_10_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.25_w0_0.25_10_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.25_w0_0.25_10_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.25_w0_0.5_10_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.25_w0_0.5_10_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.25_w0_0.5_10_taxa_tl_9_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.5_w0_0.25_10_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.5_w0_0.25_10_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.5_w0_0.25_10_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.5_w0_0.5_10_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.5_w0_0.5_10_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.5_w0_0.5_10_taxa_tl_9_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.75_w0_0.25_10_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.75_w0_0.25_10_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.75_w0_0.25_10_taxa_tl_9_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.75_w0_0.5_10_taxa_tl_9_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.75_w0_0.5_10_taxa_tl_9_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_10_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/10_taxa_bl_9/data/p0_0.75_w0_0.5_10_taxa_tl_9_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_10_taxa_m2a_lnl-p0_0.25_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_10_taxa_c2_lnl-p0_0.25_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_10_taxa_m2a_lnl-p0_0.25_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_10_taxa_c2_lnl-p0_0.25_w0_0.5_10_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_10_taxa_m2a_lnl-p0_0.5_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_10_taxa_c2_lnl-p0_0.5_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_10_taxa_m2a_lnl-p0_0.5_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_10_taxa_c2_lnl-p0_0.5_w0_0.5_10_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_10_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_10_taxa_m2a_lnl-p0_0.75_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_10_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_10_taxa_c2_lnl-p0_0.75_w0_0.25_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_10_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_10_taxa_m2a_lnl-p0_0.75_w0_0.5_10_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_10_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_10_taxa_c2_lnl-p0_0.75_w0_0.5_10_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_10_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_10_taxa_m2a,lrs_p0_0.25_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_10_taxa_m2a,lrs_p0_0.25_w0_0.5_10_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_10_taxa_m2a,lrs_p0_0.5_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_10_taxa_m2a,lrs_p0_0.5_w0_0.5_10_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_10_taxa_m2a,lrs_p0_0.75_w0_0.25_10_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_10_taxa_m2a,lrs_p0_0.75_w0_0.5_10_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a balanced, 10-taxon tree topology with branch lengths summing to 9.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF10taxaTL9}
\end{figure}

\begin{figure}[H]
    \centering
    <<CDF32taxaTL3,echo=F,warning=F>>=
    p0_0.25_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.25_w0_0.25_32_taxa_tl_3_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.25_w0_0.25_32_taxa_tl_3_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.25_w0_0.25_32_taxa_tl_3_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.25_w0_0.5_32_taxa_tl_3_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.25_w0_0.5_32_taxa_tl_3_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.25_w0_0.5_32_taxa_tl_3_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.5_w0_0.25_32_taxa_tl_3_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.5_w0_0.25_32_taxa_tl_3_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.5_w0_0.25_32_taxa_tl_3_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.5_w0_0.5_32_taxa_tl_3_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.5_w0_0.5_32_taxa_tl_3_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.5_w0_0.5_32_taxa_tl_3_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.75_w0_0.25_32_taxa_tl_3_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.75_w0_0.25_32_taxa_tl_3_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.75_w0_0.25_32_taxa_tl_3_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.75_w0_0.5_32_taxa_tl_3_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.75_w0_0.5_32_taxa_tl_3_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_3/data/p0_0.75_w0_0.5_32_taxa_tl_3_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_32_taxa_m2a_lnl-p0_0.25_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_32_taxa_c2_lnl-p0_0.25_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_32_taxa_m2a_lnl-p0_0.25_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_32_taxa_c2_lnl-p0_0.25_w0_0.5_32_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_32_taxa_m2a_lnl-p0_0.5_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_32_taxa_c2_lnl-p0_0.5_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_32_taxa_m2a_lnl-p0_0.5_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_32_taxa_c2_lnl-p0_0.5_w0_0.5_32_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_32_taxa_m2a_lnl-p0_0.75_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_32_taxa_c2_lnl-p0_0.75_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_32_taxa_m2a_lnl-p0_0.75_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_32_taxa_c2_lnl-p0_0.75_w0_0.5_32_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_32_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_32_taxa_m2a,lrs_p0_0.25_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_32_taxa_m2a,lrs_p0_0.25_w0_0.5_32_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_32_taxa_m2a,lrs_p0_0.5_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_32_taxa_m2a,lrs_p0_0.5_w0_0.5_32_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_32_taxa_m2a,lrs_p0_0.75_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_32_taxa_m2a,lrs_p0_0.75_w0_0.5_32_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a balanced, 32-taxon tree topology with branch lengths summing to 3.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF32taxaTL3}
\end{figure}

\begin{figure}[H]
    \centering
    <<CDF32taxaTL6,echo=F,warning=F>>=
    rm(list=ls())
    p0_0.25_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.25_w0_0.25_32_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.25_w0_0.25_32_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.25_w0_0.25_32_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.25_w0_0.5_32_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.25_w0_0.5_32_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.25_w0_0.5_32_taxa_tl_6_c2_lnLs.csv",sep=',')

    p0_0.5_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.5_w0_0.25_32_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.5_w0_0.25_32_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.5_w0_0.25_32_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.5_w0_0.5_32_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.5_w0_0.5_32_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.5_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.5_w0_0.5_32_taxa_tl_6_c2_lnLs.csv",sep=',')

    p0_0.75_w0_0.25_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.75_w0_0.25_32_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.75_w0_0.25_32_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.25_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.75_w0_0.25_32_taxa_tl_6_c2_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.75_w0_0.5_32_taxa_tl_6_m1a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.75_w0_0.5_32_taxa_tl_6_m2a_lnLs.csv",sep=',')
    p0_0.75_w0_0.5_32_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/32_taxa_bl_6/data/p0_0.75_w0_0.5_32_taxa_tl_6_c2_lnLs.csv",sep=',')

    lrs_p0_0.25_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.25_w0_0.25_32_taxa_m2a_lnl-p0_0.25_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.25_w0_0.25_32_taxa_c2_lnl-p0_0.25_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.25_w0_0.5_32_taxa_m2a_lnl-p0_0.25_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.25_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.25_w0_0.5_32_taxa_c2_lnl-p0_0.25_w0_0.5_32_taxa_m1a_lnl))

    lrs_p0_0.5_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.5_w0_0.25_32_taxa_m2a_lnl-p0_0.5_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.5_w0_0.25_32_taxa_c2_lnl-p0_0.5_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.5_w0_0.5_32_taxa_m2a_lnl-p0_0.5_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.5_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.5_w0_0.5_32_taxa_c2_lnl-p0_0.5_w0_0.5_32_taxa_m1a_lnl))

    lrs_p0_0.75_w0_0.25_32_taxa_m2a <- sort(2*(p0_0.75_w0_0.25_32_taxa_m2a_lnl-p0_0.75_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.25_32_taxa_c2 <- sort(2*(p0_0.75_w0_0.25_32_taxa_c2_lnl-p0_0.75_w0_0.25_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_32_taxa_m2a <- sort(2*(p0_0.75_w0_0.5_32_taxa_m2a_lnl-p0_0.75_w0_0.5_32_taxa_m1a_lnl))
    lrs_p0_0.75_w0_0.5_32_taxa_c2 <- sort(2*(p0_0.75_w0_0.5_32_taxa_c2_lnl-p0_0.75_w0_0.5_32_taxa_m1a_lnl))

    N <- length(p0_0.25_w0_0.25_32_taxa_m1a_lnl)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrs_p0_0.25_w0_0.25_32_taxa_m2a,lrs_p0_0.25_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.25_w0_0.5_32_taxa_m2a,lrs_p0_0.25_w0_0.5_32_taxa_c2,
             x,lrs_p0_0.5_w0_0.25_32_taxa_m2a,lrs_p0_0.5_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.5_w0_0.5_32_taxa_m2a,lrs_p0_0.5_w0_0.5_32_taxa_c2,
             x,lrs_p0_0.75_w0_0.25_32_taxa_m2a,lrs_p0_0.75_w0_0.25_32_taxa_c2,
             x,lrs_p0_0.75_w0_0.5_32_taxa_m2a,lrs_p0_0.75_w0_0.5_32_taxa_c2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- rep(c(prob.t,rep(1:N/N,2)),6)

    cdf.data <- data.frame(lrs,cprob,
                           weight=rep(c(0.25,0.5,0.75),each=6*N),
                           omega=rep(c(0.25,0.5),each=3*N,times=3),
                           model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N,times=6))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="LRS",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3)) +
        facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.8),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR (C=0) and modified LR (C=2) statistics under M1a/M2a nested model pairs for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a balanced, 32-taxon tree topology with branch lengths summing to 6.  The value of $\omega_0$ and its weight, $p_0$, used to generate the data are shown as column and row labels.  CDFs for $\chi^2_0/2 + \chi^2_1/2$ are also included.}
  \label{fig:CDF32taxaTL6}
\end{figure}

\begin{figure}[H]
    \centering
    <<w0MLEsM1a, echo=F,warning=F>>=
    p0_0.25_w0_0.25_m1a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.25_m1a_mles.csv",header=F)
    colnames(p0_0.25_w0_0.25_m1a_mles) <- c('k','p0','p1','w0','w1')
    p0_0.25_w0_0.5_m1a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m1a_mles.csv",header=F)
    colnames(p0_0.25_w0_0.5_m1a_mles) <- c('k','p0','p1','w0','w1')
    p0_0.5_w0_0.25_m1a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.25_m1a_mles.csv",header=F)
    colnames(p0_0.5_w0_0.25_m1a_mles) <- c('k','p0','p1','w0','w1')
    p0_0.5_w0_0.5_m1a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.5_w0_0.5_m1a_mles.csv",header=F)
    colnames(p0_0.5_w0_0.5_m1a_mles) <- c('k','p0','p1','w0','w1')
    p0_0.75_w0_0.25_m1a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.25_m1a_mles.csv",header=F)
    colnames(p0_0.75_w0_0.25_m1a_mles) <- c('k','p0','p1','w0','w1')
    p0_0.75_w0_0.5_m1a_mles <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.75_w0_0.5_m1a_mles.csv",header=F)
    colnames(p0_0.75_w0_0.5_m1a_mles) <- c('k','p0','p1','w0','w1')

    w0 <- c(p0_0.25_w0_0.25_m1a_mles$w0,p0_0.25_w0_0.5_m1a_mles$w0,
                  p0_0.5_w0_0.25_m1a_mles$w0,p0_0.5_w0_0.5_m1a_mles$w0,
                  p0_0.75_w0_0.25_m1a_mles$w0,p0_0.75_w0_0.5_m1a_mles$w0)

    N <- nrow(p0_0.25_w0_0.25_m1a_mles)

    mle.data <- data.frame(w0,
                           weight=rep(c(0.25,0.5,0.75),each=2*N),
                           omega=rep(c(0.25,0.5),each=N,times=3))

    cdf.plot <- ggplot(mle.data,aes(w0,y=..ncount..)) +
        geom_histogram(binwidth=.005,fill=I('black')) +
        labs(x=expression(omega[0]),y='') +
facet_grid(weight~omega,labeller=label_bquote(cols=omega[0]*'='*.(omega),rows=p[0]*'='*.(weight)))

    cdf.plot +
        theme(panel.spacing=unit(2,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              text=element_text(size=16),
              axis.text.y=element_blank(),
              axis.ticks.y=element_blank())
@
\caption[]{MLEs of the $\omega_0$ parameter under model M1a for six simulation settings.  For each simulation setting, 10,000 sequence alignments were generated with two site classes, $\omega<1$ and $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 3.  The value of $\omega_0$ and its weight, $p_0$, used to simulate the data are shown as column and row labels.}
  \label{fig:w0MLEsM1a}
\end{figure}

\begin{figure}[H]
    \centering
    <<CDF5taxaPS, echo=F,warning=F>>=
    p0_0.25_w0_0.5_5_taxa_m0_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m0_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m1a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m1a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_m2a_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_m2a_lnLs.csv",sep=',')
    p0_0.25_w0_0.5_5_taxa_c2_lnl <- scan("~/scm/modl.git/sim/null/5_taxa_bl_3/data/p0_0.25_w0_0.5_c2_lnLs.csv",sep=',')

    lrt0 <- 2*(p0_0.25_w0_0.5_5_taxa_m1a_lnl-p0_0.25_w0_0.5_5_taxa_m0_lnl)
    lrt1 <- 2*(p0_0.25_w0_0.5_5_taxa_m2a_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl)
    lrt2 <- 2*(p0_0.25_w0_0.5_5_taxa_c2_lnl-p0_0.25_w0_0.5_5_taxa_m1a_lnl)
    rej <- which(lrt0>=qchisq(.9,1))

    lrt0 <- sort(2*(p0_0.25_w0_0.5_5_taxa_m1a_lnl[rej]-p0_0.25_w0_0.5_5_taxa_m0_lnl[rej]))
    lrt1 <- sort(2*(p0_0.25_w0_0.5_5_taxa_m2a_lnl[rej]-p0_0.25_w0_0.5_5_taxa_m1a_lnl[rej]))
    lrt2 <- sort(2*(p0_0.25_w0_0.5_5_taxa_c2_lnl[rej]-p0_0.25_w0_0.5_5_taxa_m1a_lnl[rej]))

    N <- length(lrt0)
    x <- seq(0,6.3,length.out=N)

    lrs <- c(x,lrt1,lrt2)

    prob.t <- 1/2+pchisq(x,1)/2
    cprob <- c(prob.t,1:N/N,1:N/N)

    cdf.data <- data.frame(lrs,cprob,model=rep(c('Theory','M2a (C=0)','M2a (C=2)'),each=N))

    cdf.plot <- ggplot(cdf.data,aes(lrs,cprob)) +
        coord_cartesian(xlim=c(0,6), ylim=c(0.5,1)) +
        labs(x="Likelihood Ratio Statistic",y=expression("P(X"<="x)")) +
        geom_line(aes(linetype=model),size=.5) +
        scale_linetype_manual(values=c("dashed","dotted","solid"),labels=c('M2a (C=0)','M2a (C=2)',expression(chi[0]^2/2 + chi[1]^2/2))) +
        scale_y_continuous(breaks=scales::pretty_breaks(n=3))

    cdf.plot +
        theme(panel.spacing=unit(0,"lines"),
              panel.background=element_blank(),
              strip.background=element_blank(),
              legend.title=element_blank(),
              legend.text.align=0,
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.80,.7),
              legend.key.width=unit(2.8,"line"),
              axis.line=element_line(colour="black"),
              text=element_text(size=16),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
  \caption[]{CDFs of LR statistics without (C=0) and with (C=2) likelihood modification after pre-screening the data with M0/M1a LR tests.  The modified LR statistics were calculated under the nested model pair M1a/M2a for 4987 simulated sequence alignments that were rejected under the M0/M1a null hypothesis of only one $\omega$ site class.  The alignments were simulated with 25\% of the sites evolving under $\omega=0.5$ and the remaining sites evolving under $\omega=1$ using a 5-taxon tree topology with branch lengths summing to 3.  A modified likelihood tuning parameters of $C=2$ was used.  A $\chi^2_0/2+\chi^2_1/2$ CDF is also included.}
  \label{fig:CDFPS}
\end{figure}

\begin{figure}[H]
  \centering
<<klp0.75w0.5, echo=F>>=
site.lik.truth <- read.csv("~/scm/modl.git/sim/null/5_taxa_bl_3_10000_codons/data/sim_p0_0.75_w0_0.5/site_lik_truth.csv",header=F)
colnames(site.lik.truth) <- c('cnt','lik')
n <- sum(site.lik.truth$cnt)
cnt <- site.lik.truth$cnt
y.truth <- site.lik.truth$lik
y.truth[y.truth==0] <- 1e-12
kl <- numeric(0)
vkl <- numeric(0)
for (p in seq(0,1,.1)) {
    for (w in seq(0,1,.1)) {
        site.lik <- read.csv(paste("~/scm/modl.git/sim/null/5_taxa_bl_3_10000_codons/data/sim_p0_0.75_w0_0.5/site_like_p_",p,"_w_",w,".csv",sep=''),header=F)
        y <- site.lik[,2]
        y[y==0] <- 1e-12
        y.bar <- sum(cnt*log(y.truth / y))/n
        kl <- c(kl,y.bar)
        vkl <- c(vkl,sum(cnt*(log(y.truth / y) - y.bar)^2)/n)
    }
}
p <- rep(seq(0,1,.1),each=11)
w <- rep(seq(0,1,.1),times=11)
lb <- kl - 2*sqrt(vkl/10000)
ub <- kl + 2*sqrt(vkl/10000)
kl.data <- data.frame(kl,lb,ub,w,p)
kl.plot <- ggplot(kl.data,aes(p,kl)) +
    labs(x=expression("p"[0]),y="Kullback-Leibler Divergence") +
    geom_point() +
    geom_hline(yintercept=0) + facet_wrap(~w,ncol=3,labeller=label_bquote(cols=omega[0]*'='*.(w))) +
    coord_cartesian(ylim = c(-0.001, 0.01)) +
    geom_errorbar(aes(ymin=lb,ymax=ub))

kl.plot + theme(panel.spacing=unit(0,"lines"),
                panel.background=element_blank(),
                strip.background=element_blank(),
                legend.title=element_blank(),
                legend.key=element_rect(fill="transparent"),
                text=element_text(size=12),
                panel.border = element_rect(colour = "black", fill=NA, size=1))
@
\caption[]{Approximations of the Kullback-Leibler divergences between the distributions of site likelihoods for the generating model and other mixing distributions.  The approximations were obtained as the mean lnL difference between 10,000 site patterns generated under model M1a using a 5-taxon tree with branch lengths summing to 3 and the mixing distribution $(p_0,\omega_0)=(0.75,0.5)$, and other mixing distributions with varying weights on values of $\omega$ ranging from $0$ to $1$.  Error bars for two standard errors $(s_{KL}/\sqrt{10000})$ above and below each Kullback-Leibler estimate are included.  Points missing from each plot are above the visible range.}
\label{fig:KLp0.75w0.5}
\end{figure}

\clearpage

% \setcounter{table}{0}
% \makeatletter
% \renewcommand{\thetable}{S\@arabic\c@table}
% \makeatother

% \setcounter{figure}{0}
% \makeatletter
% \renewcommand{\thefigure}{S\@arabic\c@figure}
% \makeatother

\singlespacing

% \vspace{3cm}
% \contentsline {section}{\numberline {}Contents}{}
% \vspace{.2cm}
% \contentsline {subsection}{\numberline {}Section                S1:  Optimality of ROC curve using true mixing distribution}{2}
% \contentsline {subsection}{\numberline {}Section                S2:  SBA Branch-Site Model A - Analysis of \textit{NR1D1}}{5}
% \contentsline {subsection}{\numberline {}Table   \hspace{1.8mm} S1:  False positive rates after LRT}{7}
% \contentsline {subsection}{\numberline {}Table   \hspace{1.8mm} S2:  Analysis of the \textit{tax} gene}{8}
% \contentsline {subsection}{\numberline  {}Figure \hspace{.2mm}  S1:  MLE histograms after LRT}{9}
% \contentsline {subsection}{\numberline  {}Figure \hspace{.2mm}  S2:  MLE bootstrap histograms}{10}
% \contentsline {subsection}{\numberline {}Figure  \hspace{.2mm}  S3:  MLE histograms over 10,000 bootstrap samples}{11}
% \contentsline {subsection}{\numberline {}Figure  \hspace{.2mm}  S4:  Simplex plots illustrating smoothing of model M2a $p$-parameters}{12}
% \contentsline {subsection}{\numberline {}Figure  \hspace{.2mm}  S5:  MLE histograms for 30-taxon tree}{13}
% \contentsline {subsection}{\numberline {}Figure  \hspace{.2mm}  S6:  MLE histograms with and without misspecification}{14}
% \contentsline {subsection}{\numberline {}Figure  \hspace{.2mm}  S7:  ROC curves after pre-screening with LRT}{15}
% \contentsline {subsection}{\numberline {}Figure  \hspace{.2mm}  S8:  MLE histograms for real data under model M2a}{16}
% \contentsline {subsection}{\numberline {}Figure  \hspace{.2mm}  S9:  MLE histograms for real data under model M2a}{17}
% \contentsline {subsection}{\numberline {}Figure  \hspace{.2mm}  S10: MLE histograms for real data under model M2a}{18}
% \contentsline {subsection}{\numberline {}Figure  \hspace{.2mm}  S11: MLE histograms for real data under model M8}{19}

% \clearpage

\section{Optimality of ROC curve using the true mixing distribution}
\label{sec:roc_opt}
\setcounter{equation}{0}
\begingroup
\fontsize{11pt}{11pt}\selectfont
We establish here that the ROC curve using posterior probabilities calculated with the true distribution of $\omega$ across sites is optimal in the sense that for any $x$-axis value, its $y$-axis value is always the largest attainable. The result is a consequence of the Neyman-Pearson Lemma \citep{neyman1933ix} which was used to establish optimality of likelihood ratio tests in the case of simple null and alternative hypotheses where all parameters of the model are known.

For a given site, let $\phi(X,W)$ denote a test for positive selection at that site. Specifically, $\phi(X,W)=1$ when the test finds in favour of positive selection and is 0 otherwise. It depends on $X,$ which represents the data at the site and possibly on random $W,$ independent of the data at the site; for all tests considered in the paper, $W$ is data from all other sites. For instance, the test, $\phi_1(X)$, using posterior probabilities calculated with the true distribution of $\omega$, uses only the data at the site and sets $\phi_1^{(k)}(X)=1$ if $P(\omega>1|X) > k$ for threshold $k$. In obtaining the ROC curve, each choice of $k$ gives a different false positive rate (the $x$-axis value). The true positive rate for a given $k$ is the corresponding $y$-value. Since
\[P(\omega > 1 | X) > k \Longleftrightarrow \frac{p(X|\omega > 1) P(\omega > 1)}{ p(X|\omega > 1) P(\omega > 1) + p(X|\omega \le 1 ) P(\omega \le 1) } > k, \]
a brief argument can be given to show that the test rejects when
\begin{equation}
  \label{eq:roc-opt0}
  p(X|\omega > 1) P(\omega > 1) / \{p(X|\omega \le 1 )P(\omega \le 1)\} > c.
\end{equation}
where to simplify notation we set $c=[1/k-1]^{-1}>0$. We denote this test as $\phi_1^{(c)}$.

The ROC curve for $\phi_1^{(c)}$ is optimal if, for any other test, $\phi_0^{(t)}$, the $y$-axis value (true positive rate) for $\phi_1^{(c)}$ at least as large as that of $\phi_0^{(t)}$ whenever the $x$-axis values (false positive rates) for the two tests are the same. Much like with codon frequencies, which can refer to observed or population frequencies, the ROC curve for a test sometimes refers to the random quantity that varies from data set to data set depending upon the number of true and false positives for that data set. According to this definition it is impossible to guarantee that $\phi_1^{(c)}$ always gives a uniformly better ROC curve. For instance, there is always some small probability of unusual data sets where the positively selected sites show only synonymous changes, in which case even poor tests may give better ROC curves. The population version of an ROC curve refers to the curve with the limiting proportions of false positives on the $x$-axis and true positives on the $y$-axis; values that can be approximated by averaging over many simulated data sets. It is for the population version of the ROC curve that the optimality property holds.

The limiting false positive rate of $\phi_1^{(c)}$, is the same as $\phi_0^{(t)}$, if $c$ and $t$ are chosen so that
\begin{equation}
  \label{eq:roc-opt1}
  P(\phi_1^{(c)}(X)=1,\omega \le 1)=P(\phi_0^{(t)}(X,W)=1,\omega \le 1).
\end{equation}
We wish to show that when (\ref{eq:roc-opt1}) holds, meaning that the x-axis values of the ROC curve are the same, the probability of a true positive for $\phi_1^{(c)}$ (y-axis value) is at least as large as that of $\phi_0^{(t)}$:
\begin{equation}
  \label{eq:roc-opt2}
  P(\phi_1^{(c)}(X)=1 , \omega > 1) - P(\phi_0^{(t)}(X,W)=1 , \omega > 1) \ge 0
\end{equation}
In using the Neyman-Pearson Lemma, it is convenient to express the true positive probability as an expectation of an indicator function. For any event, $A$, $P(A)=E[I\{A\}]$ where $I\{A\}$ is 1 or 0 depending upon whether $A$ is true or not. Since $\phi_1^{(c)}(X)I\{\omega>1\}=1$ or 0, according to whether the event $\phi_1^{(c)}(X)=1, \omega > 1$ holds or not, we get that (\ref{eq:roc-opt2}) can be expressed as
\begin{equation}
  \label{eq:roc-opt3}
  E[\phi_1^{(c)}(X)I\{\omega > 1\}] - E[ \phi_1^{(t)}(X,W)I\{\omega > 1\}] \ge 0
\end{equation}
As a final simplification, let $\phi_0^{(*t)}(X)=E[\phi_0^{(t)}(X,W)|X]$. Then
\begin{eqnarray}
  \nonumber
  E[\phi_0^{(t)}(X,W) I\{\omega > 1\}] &=& E[E[\phi_0^{(t)}(X,W) I\{\omega > 1\}|X]] \\
  \label{eq:roc-opt4}
                                       &=& E[I\{\omega > 1\} E[\phi_0^{(t)}(X,W)]|X]] = E[I\{\omega > 1\} \phi_0^{(*t)}(X)].
\end{eqnarray}
Substituting (\ref{eq:roc-opt4}) in (\ref{eq:roc-opt3}), we obtain that $\phi_1^{(c)}$ is optimal $\phi_0^{(t)}$ if
\begin{equation}
  \label{eq:roc-opt5}
  E[\phi_1^{(c)}(X)I\{\omega > 1\}] - E[\phi_0^{(*t)}(X)I\{\omega > 1\} ] \ge 0.
\end{equation}
whenever (\ref{eq:roc-opt1}) holds. With this simplification the result follows immediately from the proof of the Neyman-Pearson Lemma as we now show.
\begin{eqnarray}
  \nonumber
  E[\phi_1^{(c)}(X) I\{\omega > 1\}-\phi_0^{(*t)}(X) I\{\omega > 1\}] &=& \sum_{x, \omega'} \{\phi_1^{(c)}(x) - \phi_0^{(*t)}(x)\} I\{\omega' > 1\} P(X=x, \omega = \omega') \\
  \nonumber
                                                                      &=& \sum_{x} \{\phi_1^{(c)}(x) - \phi_0^{(*t)}(x)\} \sum_{\omega'>1}  P(X=x, \omega = \omega')\\
  \nonumber
                                                                      &=& \sum_{x} \{\phi_1^{(c)}(x) - \phi_0^{(*t)}(x)\} P(X=x,\omega > 1) \\
  \label{eq:roc04}
                                                                      &=& \sum_{x} \{\phi_1^{(c)}(x) - \phi_0^{(*t)}(x)\}p(x|\omega > 1) P(\omega > 1).
\end{eqnarray}

For any $x$ such that the test $\phi_0^{(c)}(x)$ rejects, by (\ref{eq:roc-opt0})
\begin{equation}
  \label{eq:roc-opt5a}
  p(x|\omega > 1) P(\omega > 1)> c p(x|\omega \le 1 ) P(\omega \le 1)
\end{equation}
Since $0\le \phi_0^{(*t)}(x)\le 1$ and since the test rejects when $\phi_1^{(c)}(x)=1$
\begin{equation}
  \label{eq:roc-opt6}
  \{\phi_1^{(c)}(x) - \phi_0^{(*t)}(x)\}=1-\phi_0^{(*t)}(x)\ge 0.
\end{equation}
Combining (\ref{eq:roc-opt5a})-(\ref{eq:roc-opt6}) gives that
\begin{equation}
  \label{eq:roc05}
  \{\phi_1^{(c)}(x) - \phi_0^{(*t)}(x)\}p(x|\omega > 1) p(\omega > 1)\ge  c     \{\phi_1^{(c)}(x) - \phi_0^{(*t)}(x)\}p(x|\omega \le 1 ) p(\omega \le 1)
\end{equation}
Consider now $x$ such that $\phi_0^{(c)}$ does not reject. Then
\begin{equation}
  \label{eq:roc-opt7}
  p(x|\omega > 1) P(\omega > 1)\le c p(x|\omega \le 1 ) P(\omega \le 1)
\end{equation}
and since $\phi_0^{(*t)}(x)\ge 0$ and $\phi_1^{(c)}(x)=0$,
\begin{equation}
  \label{eq:roc-opt8}
  \{\phi_1^{(c)}(x) - \phi_0^{(*t)}(x)\}=-\phi_0^{(*t)}(x)\le 0.
\end{equation}
Combining (\ref{eq:roc-opt7})-(\ref{eq:roc-opt8}) gives (\ref{eq:roc05}). In short, (\ref{eq:roc05}) is satisfied for all $x$. Substituting in (\ref{eq:roc04}),
\begin{eqnarray}
  \label{eq:roc-opt8a}
  E[\phi_1^{(c)}(X) I\{\omega > 1\}-\phi_0^{(*t)}(X) I\{\omega > 1\}] &\ge& c \sum_{x} \{\phi_1^{(c)}(x) - \phi_0^{(*t)}(x)\}p(x|\omega \le 1 ) p(\omega \le 1)
  % \\
  % \nonumber
  % &=& c \sum_{x} \{1 - \phi_0^{(*t)}(x) - [1 -\phi_1^{(c)}(x)] \} P(X=x, \omega \le 1)\\
  % \nonumber
  % &=& c \sum_{x} \{1 - \phi_0^{(*t)}(x) - [1 -\phi_1^{(c)}(x)] \} \sum_{\omega' \le 1} P(X=x, \omega =\omega')\\
  % \nonumber
  % &=& c \sum_{x, \omega'} \{1 - \phi_0^{(*t)}(x) - [1 -\phi_1^{(c)}(x)] \} I\{\omega'\le 1\} P(X=x, \omega =\omega')\\
  % \nonumber
  % &=& c \{E[\{1-\phi_0^{(*t)}(X)\} I\{\omega\le 1\}] - E[\{1-\phi_1^{(c)}(X)\} I\{\omega\le 1\}]\}
\end{eqnarray}

Now
\begin{eqnarray}
  \nonumber
  \sum_{x} \phi_1^{(c)}(x) p(x|\omega \le 1 ) p(\omega \le 1)
  &=& \sum_{x} \phi_1^{(c)}(x) P(X=x,\omega\le 1) \\
  \nonumber
  &=& \sum_{x,\omega\le 1} \phi_1^{(c)}(x) p(x,\omega)\\
  \label{eq:roc-opt9}
  &=& \sum_{x|\phi_1^{(c)}(x)=1, \omega\le 1}p(x,\omega)
      = P(\phi_1^{(c)}(X)=1,\omega \le 1)
\end{eqnarray}
Similarly,
\begin{equation}
  \label{eq:roc-opt10}
  \sum_{x} \phi_0^{(*t)}(x)p(x|\omega \le 1 ) p(\omega \le 1) = P(\phi_0^{(t)}(X)=1,\omega \le 1)
\end{equation}
Substituting (\ref{eq:roc-opt9})-(\ref{eq:roc-opt10}) in (\ref{eq:roc-opt8a}) and using (\ref{eq:roc-opt1}) gives the desired result that
\begin{eqnarray}
  \nonumber
  E[\phi_1^{(c)}(X) I\{\omega > 1\}-\phi_0^{(*t)}(X) I\{\omega > 1\}] &\ge& c \{P(\phi_1^{(c)}(X)=1,\omega \le 1) - P(\phi_0^{(t)}(X)=1,\omega \le 1)\} =0
\end{eqnarray}
\endgroup

\clearpage

\section{SBA Supplementary Figures and Tables}

\begin{table}[H]
  \caption[False positive rates after LRT.]{False positive rates for each simulation study after application of the likelihood ratio (LR) test.  The \textit{LR Test} column lists the proportion of significant tests.  False positive rates are only included when they differ from the corresponding rates without the LR Test.  All rates under BEB and SBA remained the same with and without the LR test.  Values in parentheses denote the change in the rate after applying the LR Test.  A posterior probability threshold of 0.95 was used for classifying sites to be under positive selection.}
  \centering
  \begin{tabular}[h!]{*{3}l*{4}c}
    \toprule
    Study              & Misspecification   & $\omega$ distribution  & \multicolumn{2}{c}{LR Test}               & \multicolumn{2}{c}{NEB}                       \\
    \cmidrule(lr){1-1} \cmidrule(lr){2-2}   \cmidrule(lr){3-3}       \cmidrule(lr){4-5}                      \cmidrule(lr){6-7}
                       &                    &                        & M2a  & M8                             & M2a                   & M8                    \\
                                                                       \cmidrule(lr){4-4} \cmidrule(lr){5-5}   \cmidrule(lr){6-6}      \cmidrule(lr){7-7}
    1                  & None               & 100\% 1                & 0.10 & 0.09                           & \textbf{0.05} (-0.29) & 0.04 (-0.31)          \\
    2                  & None               & 50\% 0.5, 50\% 1       & 0.08 & 0.17                           &                       &                       \\
    3                  & None               & 50\% 1 50\% 1.5        & 0.94 & 0.94                           & \textbf{0.33} (-0.02) & \textbf{0.35} (-0.02) \\
    4                  & None               & 45\% 0, 45\% 1, 10\% 5 & 1.00 & 1.00                           &                       &                       \\
    \\
    5                  & Mild               & 100\% 1                & 0.23 & 1.00                           &                       &                       \\
    6                  & Mild               & 50\% 0.5, 50\% 1       & 0.13 & 1.00                           &                       &                       \\
    7                  & Mild               & 50\% 1, 50\% 1.5       & 0.92 & 1.00                           & \textbf{0.26} (-0.04) &                       \\
    8                  & Mild               & 45\% 0, 45\% 1, 10\% 5 & 1.00 & 1.00                           &                       &                       \\
    \\
    9                  & Heavy              & 100\% 1                & 1.00 & 0.00                           &                       &                       \\
    10                 & Heavy              & 50\% 0.5, 50\% 1       & 0.71 & 0.74                           & \textbf{0.36} (-0.17) & \textbf{0.38} (-0.12) \\
    \bottomrule
  \end{tabular}
  \label{tab:simlrt}
\end{table}

\clearpage

\begin{table}[H]
  \caption[Analysis of the \emph{tax} Gene.]{Analysis of the \emph{tax} Gene.  Shown are the estimated total tree lengths (TL), maximum likelihood parameters (MLEs), -log likelihoods (-lnL), and the range of site posterior probabilities (Pr) under models M2a and M8 using BEB and SBA to classify sites.  The range of posterior probabilities are shown for three categories of sites: invariant, single synonymous substitution (SSS), and single nonsynonymous substitution (SNS).}
  \centering
  \begin{tabular}[!ht]{*{3}l}
    \toprule
                                                  & \multicolumn{1}{c}{M2a}             & \multicolumn{1}{c}{M8}              \\
    \cmidrule(lr){2-2} \cmidrule(lr){3-3}
    TL                                            & 0.128                               & 0.128                               \\
    MLEs                                          & $p_{\omega>1}=1.0$, $\omega_{>1}=4.87$ & $p_{\omega>1}=1.0$, $\omega_{>1}=4.87$ \\
    -lnL                                          & 892.0                               & 892.0                               \\
    \underline{Invariant Sites} (159)             &                                     &                                     \\
    \multicolumn{1}{r}{\hspace{2 mm}BEB Pr Range} & $0.552,0.607$                       & $0.689,0.732$                       \\
    \multicolumn{1}{r}{SBA Pr Range}              & $0.543,0.596$                       & $0.761,0.799$                       \\
    \underline{SSS Sites} (2)                     &                                     &                                     \\
    \multicolumn{1}{r}{BEB Pr Range}              & $0.589,0.590$                       & $0.718,0.719$                       \\
    \multicolumn{1}{r}{SBA Pr Range}              & $0.578,0.579$                       & $0.787,0.787$                       \\
    \underline{SNS Sites} (21)                    &                                     &                                     \\
    \multicolumn{1}{r}{BEB Pr Range}              & $0.911,0.927$                       & $0.961,0.968$                       \\
    \multicolumn{1}{r}{SBA Pr Range}              & $0.871,0.892$                       & $0.990,0.991$                       \\
    \bottomrule
  \end{tabular}
  \label{tab:tax}
\end{table}

\clearpage

<<mles-filtered, echo=F>>=
cat1.scheme4.5.taxa.m2a.mles <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme4_5_taxa_m2a_mles.csv',header=F)
cat1.scheme4.5.taxa.m8.mles <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme4_5_taxa_m8_mles.csv',header=F)
cat1.scheme1.5.taxa.m2a.mles <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_5_taxa_m2a_mles.csv',header=F)
## remove rows with crazy large w2
cat1.scheme1.5.taxa.m2a.mles <- cat1.scheme1.5.taxa.m2a.mles[-which(cat1.scheme1.5.taxa.m2a.mles[,7] > 15),]
cat1.scheme1.5.taxa.m8.mles <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_5_taxa_m8_mles.csv',header=F)
## remove rows with crazy large w+
cat1.scheme1.5.taxa.m8.mles <- cat1.scheme1.5.taxa.m8.mles[-which(cat1.scheme1.5.taxa.m8.mles[,23] > 17),]
## over bootstraps for a single simulated dataset
cat1.scheme4.5.taxa.m2a.bs.4.mles <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme4_5_taxa_bs_4_m2a_mles_easy.csv',header=F)
cat1.scheme4.5.taxa.m8.bs.73.mles <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme4_5_taxa_bs_73_m8_mles_easy.csv',header=F)
cat1.scheme1.5.taxa.m2a.bs.86.mles <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_bs_86_m2a_mles_difficult.csv',header=F)
cat1.scheme1.5.taxa.m8.bs.70.mles <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_bs_70_m8_mles_difficult.csv',header=F)
@

\begin{figure}[H]
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(cat1.scheme1.5.taxa.m2a.mles[c(15,19,39,59,67,69,81,86,90,92),4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme1.5.taxa.m2a.mles[c(15,19,39,59,67,69,81,86,90,92),7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M2a}
    \label{sfig:sim_M2a_diff_filter}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(cat1.scheme1.5.taxa.m8.mles[c(15,39,59,69,75,81,86,90,92),12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(cat1.scheme1.5.taxa.m8.mles[c(15,39,59,69,75,81,86,90,92),23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{M8}
    \label{sfig:sim_M8_diff_filter}
  \end{subfigure}
  \caption{MLE distributions of the $p_{\omega>1}$ and $\omega_{>1}$ parameters under M2a and M8.  Histograms are over simulated datasets for which the null hypothesis of no positive selection was rejected by a likelihood ratio test (10 of 100 under M2a and 9 of 100 under M8).  Data were simulated under \textit{irregular} conditions: 5 taxa, $100\%$ $\omega=1$.}
  \label{fig:hists-cat1-scheme1-filter}
\end{figure}

\clearpage

\begin{figure}[H]
  \centering
   \begin{subfigure}[t]{0.49\textwidth}\centering Simulated\end{subfigure}
   \begin{subfigure}[t]{0.49\textwidth}\centering Bootstrap\end{subfigure}
   \begin{subfigure}[t]{.01\textwidth}
     \vspace{15 mm}
     \rotatebox{90}{\textit{Regular}}
   \end{subfigure}
   \begin{subfigure}[t]{0.235\textwidth}
     \centering
     <<echo=F>>=
     par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0.8))
     hist(cat1.scheme4.5.taxa.m2a.mles[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
     axis(1,cex.axis=3,padj=1)
     par(mar=c(4.5,1.5,2.8,3))
     hist(cat1.scheme4.5.taxa.m2a.mles[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
     axis(1,cex.axis=3,padj=1)
     @
   \end{subfigure}
  \begin{subfigure}[t]{0.235\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mar=c(4.5,1.5,2.8,3))
    hist(cat1.scheme4.5.taxa.m2a.mles[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
  \end{subfigure}
   \begin{subfigure}[t]{0.235\textwidth}
     \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0.8))
    hist(cat1.scheme4.5.taxa.m2a.bs.4.mles[,8],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,3))
    hist(cat1.scheme4.5.taxa.m2a.bs.4.mles[,10],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
  \end{subfigure}
  \begin{subfigure}[t]{0.235\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mar=c(4.5,1.5,2.8,1))
    hist(cat1.scheme4.5.taxa.m2a.bs.4.mles[,9],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
  \end{subfigure}
  \par\bigskip
  \begin{subfigure}[t]{.01\textwidth}
    \vspace{12 mm}
    \rotatebox{90}{\textit{Irregular}}
  \end{subfigure}
  \begin{subfigure}[t]{0.235\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0.8))
    hist(cat1.scheme1.5.taxa.m2a.mles[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,3))
    hist(cat1.scheme1.5.taxa.m2a.mles[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
  \end{subfigure}
  \begin{subfigure}[t]{0.235\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mar=c(4.5,1.5,2.8,3))
    hist(cat1.scheme1.5.taxa.m2a.mles[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
  \end{subfigure}
  \begin{subfigure}[t]{0.235\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0.8))
    hist(cat1.scheme1.5.taxa.m2a.bs.86.mles[,8],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,3))
    hist(cat1.scheme1.5.taxa.m2a.bs.86.mles[,10],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
  \end{subfigure}
  \begin{subfigure}[t]{0.235\textwidth}
    \centering
    <<echo=FALSE,fig.align="center">>=
    par(mar=c(4.5,1.5,2.8,1))
    hist(cat1.scheme1.5.taxa.m2a.bs.86.mles[,9],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
  \end{subfigure}
  \caption{MLE distributions of the $p_{\omega<1}$, $\omega_{<1}$, $p_{\omega=1}$ and $\omega_1$ parameters under M2a.  Histograms are over 100 simulated and bootstrap datasets with the bootstrap datasets generated by sampling from one simulated dataset.  Data were simulated under \textit{regular} and \textit{irregular} conditions.\\ \\\textit{regular} simulation conditions: 5 taxa, $45\%$ $\omega=0$, $45\%$ $\omega=0.5$, and $10\%$ $\omega=5$\\ \textit{irregular} simulation conditions: 5 taxa, $100\%$ $\omega=1$}
  \label{fig:mle_p0_p1_w0_hists}
\end{figure}

\clearpage

<<mles_more_bs,echo=F>>=
## over more bootstraps (10,000) for a single simulation dataset
cat1.scheme1.5.taxa.m2a.bs.10.more.mles <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_5_taxa_bs_10_m2a_mles_more_bs_difficult.csv',header=F)
##cat1.scheme1.5.taxa.m2a.bs.10.more.mles <- cat1.scheme1.5.taxa.m2a.bs.10.more.mles[-which(cat1.scheme1.5.taxa.m2a.bs.10.more.mles[,11] > 15),]

cat1.scheme1.5.taxa.m2a.bs.10.more.mles[,11][cat1.scheme1.5.taxa.m2a.bs.10.more.mles[,11]>10] <- 10
cat1.scheme1.5.taxa.m8.bs.70.mles <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_bs_70_m8_mles_difficult.csv',header=F)
@

<<hists-cat1-scheme1-more-bs, fig.scap='MLE distributions over 10,000 bootstrap datasets under \\textit{difficult} conditions', fig.cap='Distributions of the $p_{\\omega>1}$ and $\\omega_{>1}$ parameters associated with positive selection estimated under models M2a and M8.  Histograms are over 10,000 bootstrap datasets drawn from a dataset simulated under \\textit{difficult} conditions (5 taxa, $100\\%$ $\\omega=1$).  Under M2a 1000 of the $\\omega_{>1}$ values estimated to be biologically unreasonable were capped at 10.', fig.subcap=c('M2a', 'M8'), out.width='.485\\linewidth', echo=FALSE>>=
par(mfrow=c(1,2))
hist(cat1.scheme1.5.taxa.m2a.bs.10.more.mles[,9],xlim=c(0,1),main='',xlab=expression(p[omega>1]),cex.axis=1.5,cex.lab=1.5,yaxt='n',ylab='',col='black',density=200)
hist(cat1.scheme1.5.taxa.m2a.bs.10.more.mles[,11],main='',xlab=expression(omega['>1']),cex.axis=1.5,cex.lab=1.5,yaxt='n',ylab='',col='black',density=200)
par(mfrow=c(1,2))
hist(1-cat1.scheme1.5.taxa.m8.bs.70.mles[,8],xlim=c(0,1),main='',xlab=expression(p[omega>1]),cex.axis=1.5,cex.lab=1.5,yaxt='n',ylab='',col='black',density=200)
hist(cat1.scheme1.5.taxa.m8.bs.70.mles[,11],main='',xlab=expression(omega['>1']),cex.axis=1.5,cex.lab=1.5,yaxt='n',ylab='',col='black',density=200)
@

\clearpage

<<before-after-smoothing,fig.scap='MLEs before and after smoothing', fig.cap='MLEs of the $p_{\\omega<1}$ and $p_{\\omega=1}$ parameters under model M2a before and after smoothing using a uniform kernel with different bandwidth parameters.  The parameters were estimated over 100 bootstrap samples under \\textit{irregular} simulation conditions (5 taxa, $100\\%$ sites $\\omega=1$).',echo=F,warning=F>>=
ps.no.smooth <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_5_taxa_m2a_bs_ds_p0_p1_nosmooth.csv',header=F,sep=' ')
ps.h.1.1 <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_5_taxa_m2a_bs_ds_p0_p1_h_.1_.1.csv',header=F,sep=' ')
ps.h.2.2 <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_5_taxa_m2a_bs_ds_p0_p1_h_.2_.2.csv',header=F,sep=' ')
ps.h.3.3 <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_5_taxa_m2a_bs_ds_p0_p1_h_.3_.3.csv',header=F,sep=' ')
ps.h.4.4 <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_5_taxa_m2a_bs_ds_p0_p1_h_.4_.4.csv',header=F,sep=' ')
ps.h.5.5 <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_5_taxa_m2a_bs_ds_p0_p1_h_.5_.5.csv',header=F,sep=' ')
ps<-data.frame(p0=c(ps.no.smooth[,1],ps.h.1.1[,1],ps.h.2.2[,1],ps.h.3.3[,1],ps.h.4.4[,1],ps.h.5.5[,1]),p1=c(ps.no.smooth[,2],ps.h.1.1[,2],ps.h.2.2[,2],ps.h.3.3[,2],ps.h.4.4[,2],ps.h.5.5[,2]),case=rep(c("Unsmoothed","h=0.1","h=0.2","h=0.3","h=0.4","h=0.5"),each=100))
ps$case <- factor(ps$case,levels = c("Unsmoothed","h=0.1","h=0.2","h=0.3","h=0.4","h=0.5"))
theme_set(theme_gray(base_size=18))
sm.plot <- ggplot(ps,aes(p0,p1)) + stat_bin2d(bins=15) + facet_wrap(~case,ncol=2)
sm.plot + theme(panel.grid.major=element_blank(),panel.grid.minor=element_blank(),panel.background=element_blank(),panel.margin=unit(2,"lines"),axis.line=element_line(colour="black")) +
##scale_fill_gradientn(colours=gray.colors(10,start=0.3,end=0.9,gamma = 2.2,alpha=NULL),guide="colorbar")
scale_fill_gradientn(colours=rainbow(5),guide="colorbar") + xlab(expression(p[omega<1])) + ylab(expression(p[omega==1]))
@

\clearpage

\begin{figure}
  \centering
  <<echo=F>>=
  cat1.scheme1.30.taxa.m2a.mles <- read.csv('~/scm/sba.git/doc/paper/data/cat1_scheme1_30_taxa_m2a_mles.csv',header=F)
  par(mfrow=c(1,2))
  hist(cat1.scheme1.30.taxa.m2a.mles[,4],xlim=c(0,1),main='',xlab=expression(p[omega>1]),cex.axis=1.5,cex.lab=1.5,yaxt='n',ylab='',col='black',density=200)
  hist(cat1.scheme1.30.taxa.m2a.mles[,7],main='',xlab=expression(omega['>1']),cex.axis=1.5,cex.lab=1.5,yaxt='n',ylab='',col='black',density=200)
  @
\caption{Distributions of the $p_{\omega>1}$ and $\omega_{>1}$ parameters associated with positive selection and estimated under model M2a.  Histograms are over 100 simulated datasets simulated under \textit{difficult} conditions ($100\%$ $\omega=1$) and using a 30-taxon tree.}
\label{fig:sim_M2a_diff_30_taxa}
\end{figure}

\clearpage

<<echo=F,warning=F>>=
c1s3.m2a.mles  <- read.csv("~/scm/sba.git/doc/paper/data/cat1_scheme3_5_taxa_m2a_mles.csv",header=F)
c1s3.m8.mles   <- read.csv("~/scm/sba.git/doc/paper/data/cat1_scheme3_5_taxa_m8_mles.csv",header=F)
c1s4.m2a.mles  <- read.csv("~/scm/sba.git/doc/paper/data/cat1_scheme4_5_taxa_m2a_mles.csv",header=F)
c1s4.m8.mles   <- read.csv("~/scm/sba.git/doc/paper/data/cat1_scheme4_5_taxa_m8_mles.csv",header=F)
c2s7.m2a.mles  <- read.csv("~/scm/sba.git/doc/paper/data/cat2_scheme7_5_taxa_m2a_mles.csv",header=F)
c2s7.m8.mles   <- read.csv("~/scm/sba.git/doc/paper/data/cat2_scheme7_5_taxa_m8_mles.csv",header=F)
c2s8.m2a.mles  <- read.csv("~/scm/sba.git/doc/paper/data/cat2_scheme8_5_taxa_m2a_mles.csv",header=F)
c2s8.m8.mles   <- read.csv("~/scm/sba.git/doc/paper/data/cat2_scheme8_5_taxa_m8_mles.csv",header=F)
@

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(c1s3.m2a.mles[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(c1s3.m2a.mles[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{study 3 - M2a}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(c1s3.m8.mles[,12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(c1s3.m8.mles[,23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{study 3 - M8}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(c1s4.m2a.mles[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(c1s4.m2a.mles[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{study 4 - M2a}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(c1s4.m8.mles[,12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(c1s4.m8.mles[,23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{study 4 - M8}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(c2s7.m2a.mles[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(c2s7.m2a.mles[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{study 7 - M2a}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(c2s7.m8.mles[,12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(c2s7.m8.mles[,23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{study 7 - M8}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(c2s8.m2a.mles[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(c2s8.m8.mles[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{study 8 - M2a}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(c2s8.m8.mles[,12],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(c2s8.m8.mles[,23],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{study 8 - M8}
  \end{subfigure}
  \caption{MLE distributions of the $p_{\omega>1}$ and $\omega_{>1}$ parameters under models M2a and M8 for two different simulation scenarios: without model misspecification (\textit{Correct Model}, studies 3 and 4) and with mild model misspecification (\textit{Mild Misspecification}, studies 7 and 8).  The data were simulated using a 5-taxon tree topology.  In studies 3 and 7, $50\%$ of the sites were simulated under neutral evolution ($\omega=1$) and $50\%$ of the sites under positive selection ($\omega=1.5$).  In studies 4 and 8, $45\%$ of the sites were simulated under purifying selection ($\omega=0$), $45\%$ under neutral evolution ($\omega=1$) and $10\%$ under positive selection ($\omega=5$).}
  \label{fig:s3478hists}
\end{figure}

\clearpage

\begin{figure}[H]
    \centering
  \begin{subfigure}[t]{.49\textwidth}
    \centering
    \hspace{10 mm} \small{$50\%$ $\omega=1$, $50\%$ $\omega=1.5$}
  \end{subfigure}
  \begin{subfigure}[t]{.49\textwidth}
    \centering
    \hspace{10 mm} \small{$45\%$ $\omega=0$, $45\%$ $\omega=1$, $10\%$ $\omega=5$}
  \end{subfigure}
  \\[-2.6ex]
  \begin{subfigure}[t]{.03\textwidth}
    \vspace{8 mm}
    \rotatebox{90}{\textit{\small{Mild Misspecification}} \hspace{4 mm} \textit{\small{Correct Model}}}
  \end{subfigure}
  \begin{subfigure}[t]{0.475\textwidth}
    \centering
    <<echo=F,warning=F>>=

    c1s3.ft.fp     <- scan("~/scm/sba.git/doc/paper/data/c1s3_opt_fp.csv",sep=',')
    c1s3.ft.tp     <- scan("~/scm/sba.git/doc/paper/data/c1s3_opt_tp.csv",sep=',')

    c1s3m2a.beb.fp <- scan("~/scm/sba.git/doc/paper/data/c1s3_m2a_beb_lrt_fp.csv",sep=',')
    c1s3m2a.neb.fp <- scan("~/scm/sba.git/doc/paper/data/c1s3_m2a_neb_lrt_fp.csv",sep=',')
    c1s3m2a.sba.fp <- scan("~/scm/sba.git/doc/paper/data/c1s3_m2a_sba_lrt_fp.csv",sep=',')
    c1s3m2a.beb.tp <- scan("~/scm/sba.git/doc/paper/data/c1s3_m2a_beb_lrt_tp.csv",sep=',')
    c1s3m2a.neb.tp <- scan("~/scm/sba.git/doc/paper/data/c1s3_m2a_neb_lrt_tp.csv",sep=',')
    c1s3m2a.sba.tp <- scan("~/scm/sba.git/doc/paper/data/c1s3_m2a_sba_lrt_tp.csv",sep=',')

    c1s3m8.beb.fp  <- scan("~/scm/sba.git/doc/paper/data/c1s3_m8_beb_lrt_fp.csv",sep=',')
    c1s3m8.neb.fp  <- scan("~/scm/sba.git/doc/paper/data/c1s3_m8_neb_lrt_fp.csv",sep=',')
    c1s3m8.sba.fp  <- scan("~/scm/sba.git/doc/paper/data/c1s3_m8_sba_lrt_fp.csv",sep=',')
    c1s3m8.beb.tp  <- scan("~/scm/sba.git/doc/paper/data/c1s3_m8_beb_lrt_tp.csv",sep=',')
    c1s3m8.neb.tp  <- scan("~/scm/sba.git/doc/paper/data/c1s3_m8_neb_lrt_tp.csv",sep=',')
    c1s3m8.sba.tp  <- scan("~/scm/sba.git/doc/paper/data/c1s3_m8_sba_lrt_tp.csv",sep=',')

    c2s7.ft.fp     <- scan("~/scm/sba.git/doc/paper/data/c2s7_opt_fp.csv",sep=',')
    c2s7.ft.tp     <- scan("~/scm/sba.git/doc/paper/data/c2s7_opt_tp.csv",sep=',')

    c2s7m2a.beb.fp <- scan("~/scm/sba.git/doc/paper/data/c2s7_m2a_beb_lrt_fp.csv",sep=',')
    c2s7m2a.neb.fp <- scan("~/scm/sba.git/doc/paper/data/c2s7_m2a_neb_lrt_fp.csv",sep=',')
    c2s7m2a.sba.fp <- scan("~/scm/sba.git/doc/paper/data/c2s7_m2a_sba_lrt_fp.csv",sep=',')
    c2s7m2a.beb.tp <- scan("~/scm/sba.git/doc/paper/data/c2s7_m2a_beb_lrt_tp.csv",sep=',')
    c2s7m2a.neb.tp <- scan("~/scm/sba.git/doc/paper/data/c2s7_m2a_neb_lrt_tp.csv",sep=',')
    c2s7m2a.sba.tp <- scan("~/scm/sba.git/doc/paper/data/c2s7_m2a_sba_lrt_tp.csv",sep=',')

    c2s7m8.beb.fp  <- scan("~/scm/sba.git/doc/paper/data/c2s7_m8_beb_lrt_fp.csv",sep=',')
    c2s7m8.neb.fp  <- scan("~/scm/sba.git/doc/paper/data/c2s7_m8_neb_lrt_fp.csv",sep=',')
    c2s7m8.sba.fp  <- scan("~/scm/sba.git/doc/paper/data/c2s7_m8_sba_lrt_fp.csv",sep=',')
    c2s7m8.beb.tp  <- scan("~/scm/sba.git/doc/paper/data/c2s7_m8_beb_lrt_tp.csv",sep=',')
    c2s7m8.neb.tp  <- scan("~/scm/sba.git/doc/paper/data/c2s7_m8_neb_lrt_tp.csv",sep=',')
    c2s7m8.sba.tp  <- scan("~/scm/sba.git/doc/paper/data/c2s7_m8_sba_lrt_tp.csv",sep=',')

    thin <- floor(seq(1,length(c1s3.ft.fp),length=(length(c1s3.ft.fp)-1)/100))

    fp <- c(c1s3.ft.fp[thin],c1s3m2a.neb.fp[thin],c1s3m2a.sba.fp[thin],c1s3m2a.beb.fp[thin])
    fp <- c(fp,c1s3.ft.fp[thin],c1s3m8.neb.fp[thin],c1s3m8.sba.fp[thin],c1s3m8.beb.fp[thin])
    fp <- c(fp,c2s7.ft.fp[thin],c2s7m2a.neb.fp[thin],c2s7m2a.sba.fp[thin],c2s7m2a.beb.fp[thin])
    fp <- c(fp,c2s7.ft.fp[thin],c2s7m8.neb.fp[thin],c2s7m8.sba.fp[thin],c2s7m8.beb.fp[thin])

    tp <- c(c1s3.ft.tp[thin],c1s3m2a.neb.tp[thin],c1s3m2a.sba.tp[thin],c1s3m2a.beb.tp[thin])
    tp <- c(tp,c1s3.ft.tp[thin],c1s3m8.neb.tp[thin],c1s3m8.sba.tp[thin],c1s3m8.beb.tp[thin])
    tp <- c(tp,c2s7.ft.tp[thin],c2s7m2a.neb.tp[thin],c2s7m2a.sba.tp[thin],c2s7m2a.beb.tp[thin])
    tp <- c(tp,c2s7.ft.tp[thin],c2s7m8.neb.tp[thin],c2s7m8.sba.tp[thin],c2s7m8.beb.tp[thin])

    roc.data <- data.frame(fp,tp,
                               Method=rep(c("OPT","NEB","SBA","BEB"),
                                              each=length(thin),times=4),
                               Study=rep(c('Study 3 - M2a',
                                               'Study 3 - M8',
                                               'Study 7 - M2a',
                                               'Study 7 - M8'),
                                             each=length(thin)*4),
                               Scenario=rep(c('CM','MM'),
                                                each=length(thin)*8))

    ##roc.data$Method <- as.factor(roc.data$Method)
    ##roc.data$Scenario <- as.factor(roc.data$Scenario)
    ##roc.plot <- ggplot(roc.data,aes(fp,tp)) + geom_point(aes(color=Method),size=4) + xlim(0,.25) + ylim(0,.25) + facet_wrap(~Scenario,ncol=2) + geom_line(aes(color=Method)) + geom_abline(slope=1,intercept=0)
    roc.plot <- ggplot(roc.data,aes(fp,tp)) +
    ##xlim(0,.3) + ylim(0,.3) +
        coord_cartesian(xlim=c(-0.005,.405), ylim=c(-0.005,.405)) +
        labs(x="False Positive Rate",y="True Positive Rate") +
        geom_line(aes(color=Method,linetype=Method),size=1.2) +
        geom_abline(slope=1,intercept=0,color='gray60') +
        ##geom_vline(xintercept=0) +
        ##geom_point(aes(color=Method,shape=Method),size=2) +
        ##scale_shape_manual(values=c(15,18,17,16)) +
        ##scale_color_manual(values=c("#F8766D","#C77CFF","#00BFC4","#7CAE00")) +
        scale_color_manual(values=c("gray30","grey40","grey50","black")) +
        scale_linetype_manual(values=c("dotted","dotdash","dashed","solid")) +
        scale_x_continuous(breaks=c(0.1,0.2,0.3)) +
        facet_wrap(~Study,ncol=2)

    ## roc.plot +
    ##         theme(panel.grid.major=element_blank(),
    ##                   panel.grid.minor=element_blank(),
    ##                   panel.background=element_blank(),
    ##                   panel.margin=unit(2,"lines"),
    ##                   legend.position="none",
    ##                   axis.line=element_line(colour="black"),
    ##                   text=element_text(size=20))

    roc.plot +
            theme(panel.margin=unit(0,"lines"),
                  panel.background=element_blank(),
                  strip.background=element_blank(),
                  panel.grid.major=element_line(color="gray95"),
                  legend.position="none",
                  axis.line=element_line(colour="black"),
                  text=element_text(size=20),
                  panel.border = element_rect(colour = "black", fill=NA, size=1))
@
    %\caption{Difficult with no Model Misspecification}
    \label{sfig:roc_hard_lrt}
  \end{subfigure}
  \begin{subfigure}[t]{0.475\textwidth}
    \centering
    <<echo=F,warning=F>>=

    c1s4.ft.fp     <- scan("~/scm/sba.git/doc/paper/data/c1s4_opt_fp.csv",sep=',')
    c1s4.ft.tp     <- scan("~/scm/sba.git/doc/paper/data/c1s4_opt_tp.csv",sep=',')

    c1s4m2a.beb.fp <- scan("~/scm/sba.git/doc/paper/data/c1s4_m2a_beb_lrt_fp.csv",sep=',')
    c1s4m2a.neb.fp <- scan("~/scm/sba.git/doc/paper/data/c1s4_m2a_neb_lrt_fp.csv",sep=',')
    c1s4m2a.sba.fp <- scan("~/scm/sba.git/doc/paper/data/c1s4_m2a_sba_lrt_fp.csv",sep=',')
    c1s4m2a.beb.tp <- scan("~/scm/sba.git/doc/paper/data/c1s4_m2a_beb_lrt_tp.csv",sep=',')
    c1s4m2a.neb.tp <- scan("~/scm/sba.git/doc/paper/data/c1s4_m2a_neb_lrt_tp.csv",sep=',')
    c1s4m2a.sba.tp <- scan("~/scm/sba.git/doc/paper/data/c1s4_m2a_sba_lrt_tp.csv",sep=',')

    c1s4m8.beb.fp <-  scan("~/scm/sba.git/doc/paper/data/c1s4_m8_beb_lrt_fp.csv",sep=',')
    c1s4m8.neb.fp <-  scan("~/scm/sba.git/doc/paper/data/c1s4_m8_neb_lrt_fp.csv",sep=',')
    c1s4m8.sba.fp <-  scan("~/scm/sba.git/doc/paper/data/c1s4_m8_sba_lrt_fp.csv",sep=',')
    c1s4m8.beb.tp <-  scan("~/scm/sba.git/doc/paper/data/c1s4_m8_beb_lrt_tp.csv",sep=',')
    c1s4m8.neb.tp <-  scan("~/scm/sba.git/doc/paper/data/c1s4_m8_neb_lrt_tp.csv",sep=',')
    c1s4m8.sba.tp <-  scan("~/scm/sba.git/doc/paper/data/c1s4_m8_sba_lrt_tp.csv",sep=',')

    c2s8.ft.fp  <-    scan("~/scm/sba.git/doc/paper/data/c2s8_opt_fp.csv",sep=',')
    c2s8.ft.tp  <-    scan("~/scm/sba.git/doc/paper/data/c2s8_opt_tp.csv",sep=',')

    c2s8m2a.beb.fp <- scan("~/scm/sba.git/doc/paper/data/c2s8_m2a_beb_lrt_fp.csv",sep=',')
    c2s8m2a.neb.fp <- scan("~/scm/sba.git/doc/paper/data/c2s8_m2a_neb_lrt_fp.csv",sep=',')
    c2s8m2a.sba.fp <- scan("~/scm/sba.git/doc/paper/data/c2s8_m2a_sba_lrt_fp.csv",sep=',')
    c2s8m2a.beb.tp <- scan("~/scm/sba.git/doc/paper/data/c2s8_m2a_beb_lrt_tp.csv",sep=',')
    c2s8m2a.neb.tp <- scan("~/scm/sba.git/doc/paper/data/c2s8_m2a_neb_lrt_tp.csv",sep=',')
    c2s8m2a.sba.tp <- scan("~/scm/sba.git/doc/paper/data/c2s8_m2a_sba_lrt_tp.csv",sep=',')

    c2s8m8.beb.fp <-  scan("~/scm/sba.git/doc/paper/data/c2s8_m8_beb_lrt_fp.csv",sep=',')
    c2s8m8.neb.fp <-  scan("~/scm/sba.git/doc/paper/data/c2s8_m8_neb_lrt_fp.csv",sep=',')
    c2s8m8.sba.fp <-  scan("~/scm/sba.git/doc/paper/data/c2s8_m8_sba_lrt_fp.csv",sep=',')
    c2s8m8.beb.tp <-  scan("~/scm/sba.git/doc/paper/data/c2s8_m8_beb_lrt_tp.csv",sep=',')
    c2s8m8.neb.tp <-  scan("~/scm/sba.git/doc/paper/data/c2s8_m8_neb_lrt_tp.csv",sep=',')
    c2s8m8.sba.tp <-  scan("~/scm/sba.git/doc/paper/data/c2s8_m8_sba_lrt_tp.csv",sep=',')

    thin <- floor(seq(1,length(c1s4.ft.fp),length=(length(c1s4.ft.fp)-1)/100))

    fp <- c(c1s4.ft.fp[thin],c1s4m2a.neb.fp[thin],c1s4m2a.sba.fp[thin],c1s4m2a.beb.fp[thin])
    fp <- c(fp,c1s4.ft.fp[thin],c1s4m8.neb.fp[thin],c1s4m8.sba.fp[thin],c1s4m8.beb.fp[thin])
    fp <- c(fp,c2s8.ft.fp[thin],c2s8m2a.neb.fp[thin],c2s8m2a.sba.fp[thin],c2s8m2a.beb.fp[thin])
    fp <- c(fp,c2s8.ft.fp[thin],c2s8m8.neb.fp[thin],c2s8m8.sba.fp[thin],c2s8m8.beb.fp[thin])

    tp <- c(c1s4.ft.tp[thin],c1s4m2a.neb.tp[thin],c1s4m2a.sba.tp[thin],c1s4m2a.beb.tp[thin])
    tp <- c(tp,c1s4.ft.tp[thin],c1s4m8.neb.tp[thin],c1s4m8.sba.tp[thin],c1s4m8.beb.tp[thin])
    tp <- c(tp,c2s8.ft.tp[thin],c2s8m2a.neb.tp[thin],c2s8m2a.sba.tp[thin],c2s8m2a.beb.tp[thin])
    tp <- c(tp,c2s8.ft.tp[thin],c2s8m8.neb.tp[thin],c2s8m8.sba.tp[thin],c2s8m8.beb.tp[thin])


    roc.data <- data.frame(fp,tp,
                               Method=rep(c("OPT","NEB","SBA","BEB"),
                                              each=length(thin),times=4),
                               Study=rep(c('Study 4 - M2a',
                                               'Study 4 - M8',
                                               'Study 8 - M2a',
                                               'Study 8 - M8'),
                                             each=length(thin)*4),
                               Scenario=rep(c('CM','MM'),
                                                each=length(thin)*8))

    ##roc.data$Method <- as.factor(roc.data$Method)
    ##roc.data$Scenario <- as.factor(roc.data$Scenario)
    ##roc.plot <- ggplot(roc.data,aes(fp,tp)) + geom_point(aes(color=Method)) + xlim(0,.25) + ylim(0,.25) + facet_wrap(~Scenario,ncol=2) + geom_line(aes(color=Method)) + geom_abline(slope=1,intercept=0)
    roc.plot <- ggplot(roc.data,aes(fp,tp)) +
        ##xlim(0,.06) + ylim(0,.75) +
        ##coord_cartesian(xlim=c(0,.06), ylim=c(0,.805)) +
        coord_cartesian(xlim=c(-0.005,.1),ylim=c(-0.005,.805)) +
        labs(x="False Positive Rate",y="True Positive Rate") +
        geom_line(aes(color=Method,linetype=Method),size=1.2) +
        geom_abline(slope=1,intercept=0,color='gray60') +
        ##geom_vline(xintercept=0) +
        ##geom_point(data=roc.data[thinned,],aes(color=Method)) +
        scale_shape_manual(values=c(15,18,17,16)) +
        ##scale_color_manual(values=c("#F8766D","#C77CFF","#00BFC4","#7CAE00")) +
        scale_color_manual(values=c("gray30","grey40","grey50","black")) +
        scale_linetype_manual(values=c("dotted","dotdash","dashed","solid")) +
        ##scale_x_continuous(breaks=seq(0,0.06,0.02)) +
        scale_x_continuous(breaks=c(0.02,0.04,0.06,0.08)) +
        guides(colour = guide_legend(override.aes = list(size=1.4))) +
        facet_wrap(~Study,ncol=2)

    ##roc.plot +
            ## theme(panel.grid.major=element_blank(),
            ##           panel.grid.minor=element_blank(),
            ##           panel.background=element_blank(),
            ##           panel.margin=unit(2,"lines"),
            ##           axis.line=element_line(colour="black"),
            ##           legend.title=element_blank(),
            ##           legend.key = element_rect(fill="transparent"),
            ##           legend.position=c(.9,.85),
            ##           legend.key.width = unit(4,"line"),
            ##           text=element_text(size=20))

    roc.plot +
        theme(panel.background=element_blank(),
              strip.background=element_blank(),
              panel.grid.major=element_line(colour="gray95"),
              panel.margin=unit(0,"lines"),
              axis.line=element_line(colour="black"),
              legend.title=element_blank(),
              legend.key=element_rect(fill="transparent"),
              legend.position=c(.84,.80),
              legend.key.width=unit(6,"line"),
              text=element_text(size=20),
              panel.border = element_rect(colour = "black", fill=NA, size=1))
@
    %\caption{Difficult with Model Misspecification}
    \label{sfig:roc_easy_lrt}
  \end{subfigure}
  \caption[ROC Curves after LRT.]{ROC curves for the detection of sites under positive selection for BEB, NEB, and SBA analyses of data generated under two different simulation scenarios: without model misspecification (\textit{Correct Model}, studies 3 and 4) and with mild model misspecification (\textit{Mild Misspecification}, studies 7 and 8).  Likelihood ratio tests were performed prior to site-wise analyses.  The data were simulated using a 5-taxon tree topology.  In studies 3 and 7, $50\%$ of the sites were simulated under neutral evolution ($\omega=1$) and $50\%$ of the sites under positive selection ($\omega=1.5$).  In studies 4 and 8, $45\%$ of the sites were simulated under purifying selection ($\omega=0$), $45\%$ under neutral evolution ($\omega=1$) and $10\%$ under positive selection ($\omega=5$).  Each plot includes a line for the lower bound (y=x) and an expected upper bound (OPT) when classification is made using the generating model parameters. The curves for studies 4 and 8 and study 7 under M8 are identical to those without pre-screening because the null hypotheses of likelihood ratio tests were rejected for all simulated datasets.  The curves for NEB do not always cover the whole range of false positive rates, because NEB sometimes estimates the $\omega$ distribution with all mass on $\omega > 1$.  In these cases, even with a posterior probability cut-off of 1, NEB still incorrectly classifies sites to be under positive selection.}
  \label{fig:roc_lrt}
\end{figure}

\clearpage

<<real_data_mles,echo=F>>=
bglobin.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/bglobin_m2a_mles.csv",header=F)
ccmF.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/ccmF_m2a_mles.csv",header=F)
CDH3.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/CDH3_m2a_mles.csv",header=F)
enam.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/enam_m2a_mles.csv",header=F)
HIVEnv.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/HIVEnv_m2a_mles.csv",header=F)
HIVpol.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/HIVpol_m2a_mles.csv",header=F)
HIVvif.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/HIVvif_m2a_mles.csv",header=F)
lysin.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/lysin_m2a_mles.csv",header=F)
mivN.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/mivN_m2a_mles.csv",header=F)
nuoL3.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/nuoL3_m2a_mles.csv",header=F)
perM.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/perM_m2a_mles.csv",header=F)
pgpA.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/pgpA_m2a_mles.csv",header=F)
RfaL.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/RfaL_m2a_mles.csv",header=F)
tax.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/tax_m2a_mles.csv",header=F)
#tax.m2a.params[,ncol(tax.m2a.params)][tax.m2a.params[,ncol(tax.m2a.params)] > 15] <- 15
TrbLVirB62.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/TrbL-VirB6_2_m2a_mles.csv",header=F)
TrbLVirB63.m2a.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/TrbL-VirB6_3_m2a_mles.csv",header=F)
bglobin.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/bglobin_m8_mles.csv",header=F)
ccmF.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/ccmF_m8_mles.csv",header=F)
#ccmF.m8.params[,ncol(ccmF.m8.params)][ccmF.m8.params[,ncol(ccmF.m8.params)] > 15] <- 15
CDH3.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/CDH3_m8_mles.csv",header=F)
enam.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/enam_m8_mles.csv",header=F)
HIVEnv.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/HIVEnv_m8_mles.csv",header=F)
HIVpol.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/HIVpol_m8_mles.csv",header=F)
HIVvif.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/HIVvif_m8_mles.csv",header=F)
lysin.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/lysin_m8_mles.csv",header=F)
mivN.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/mivN_m8_mles.csv",header=F)
nuoL3.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/nuoL3_m8_mles.csv",header=F)
perM.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/perM_m8_mles.csv",header=F)
pgpA.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/pgpA_m8_mles.csv",header=F)
RfaL.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/RfaL_m8_mles.csv",header=F)
tax.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/tax_m8_mles.csv",header=F)
#tax.m8.params[,ncol(tax.m8.params)][tax.m8.params[,ncol(tax.m8.params)] > 15] <- 15
TrbLVirB62.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/TrbL-VirB6_2_m8_mles.csv",header=F)
TrbLVirB63.m8.params <- read.csv("~/scm/sba.git/doc/real_data_results/mles/TrbL-VirB6_3_m8_mles.csv",header=F)
@


\begin{figure}
  \centering
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(bglobin.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(bglobin.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{$\beta$-globin}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(ccmF.m2a.params[,2],main=expression(p[omega==1]),xlim=c(0,1),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(ccmF.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{ccmF}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(CDH3.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(CDH3.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{CDH3}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(enam.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(enam.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{ENAM}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(HIVEnv.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(HIVEnv.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{env}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(HIVpol.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(HIVpol.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{pol}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(HIVvif.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(HIVvif.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{vif}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(lysin.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(lysin.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{lysin}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(mivN.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(mivN.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{mivN}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(nuoL3.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(nuoL3.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{nuoL3}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(perM.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(perM.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{perM}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(pgpA.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(pgpA.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{pgpA}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(RfaL.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(RfaL.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{RfaL}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(tax.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(tax.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{tax}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(TrbLVirB62.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(TrbLVirB62.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{TrbL-VirB6\_2}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(TrbLVirB63.m2a.params[,2],xlim=c(0,1),main=expression(p[omega<1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(TrbLVirB63.m2a.params[,5],main=expression(omega['<1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{TrbL-VirB6\_3}}
  \end{subfigure}
  \caption{Distributions of the $p_{\omega<1}$ and $\omega_{<1}$ parameters for the real data under model M2a.  Histograms are over 100 bootstrap datasets.}
  \label{fig:real_genes_m2a_p0_w0_mles}
\end{figure}

\clearpage

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(bglobin.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{$\beta$-globin}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(ccmF.m2a.params[,3],main=expression(p[omega==1]),xlim=c(0,1),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{ccmF}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(CDH3.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{CDH3}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(enam.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{ENAM}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(HIVEnv.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{env}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(HIVpol.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{pol}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(HIVvif.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{vif}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(lysin.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{lysin}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(mivN.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{mivN}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(nuoL3.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{nuoL3}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(perM.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{perM}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(pgpA.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{pgpA}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(RfaL.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{RfaL}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(tax.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{tax}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(TrbLVirB62.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{TrbL-VirB6\_2}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mar=c(4.5,1.5,2.8,0.4))
    hist(TrbLVirB63.m2a.params[,3],xlim=c(0,1),main=expression(p[omega==1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{TrbL-VirB6\_3}}
  \end{subfigure}
  \caption{Distributions of the $p_{\omega=1}$ parameters for the real data under model M2a.  Histograms are over 100 bootstrap datasets.}
  \label{fig:real_genes_m2a_p1_mles}
\end{figure}

\clearpage

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(bglobin.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(bglobin.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{$\beta$-globin}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(ccmF.m2a.params[,4],main=expression(p[omega>1]),xlim=c(0,1),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(ccmF.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{ccmF}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(CDH3.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(CDH3.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{CDH3}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(enam.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(enam.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{ENAM}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(HIVEnv.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(HIVEnv.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{env}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(HIVpol.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(HIVpol.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{pol}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(HIVvif.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(HIVvif.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{vif}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(lysin.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(lysin.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{lysin}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(mivN.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(mivN.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{mivN}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(nuoL3.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(nuoL3.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{nuoL3}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(perM.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(perM.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{perM}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(pgpA.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(pgpA.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{pgpA}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(RfaL.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(RfaL.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{RfaL}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(tax.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(tax.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{tax}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(TrbLVirB62.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(TrbLVirB62.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{TrbL-VirB6\_2}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(TrbLVirB63.m2a.params[,4],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(TrbLVirB63.m2a.params[,7],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{TrbL-VirB6\_3}}
  \end{subfigure}
  \caption{Distributions of the $p_{\omega>1}$ and $\omega_{>1}$ parameters for the real data under model M2a.  Histograms are over 100 bootstrap datasets.}
  \label{fig:real_genes_m2a_mles}
\end{figure}

\clearpage

\begin{figure}
  \centering
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(bglobin.m8.params[,ncol(bglobin.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(bglobin.m8.params[,ncol(bglobin.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{$\beta$-globin}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(ccmF.m8.params[,ncol(ccmF.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(ccmF.m8.params[,ncol(ccmF.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{ccmF}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(CDH3.m8.params[,ncol(CDH3.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(CDH3.m8.params[,ncol(CDH3.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{CDH3}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(enam.m8.params[,ncol(enam.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(enam.m8.params[,ncol(enam.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{ENAM}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(HIVEnv.m8.params[,ncol(HIVEnv.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(HIVEnv.m8.params[,ncol(HIVEnv.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{env}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(HIVpol.m8.params[,ncol(HIVpol.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(HIVpol.m8.params[,ncol(HIVpol.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{pol}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(HIVvif.m8.params[,ncol(HIVvif.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(HIVvif.m8.params[,ncol(HIVvif.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{vif}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(lysin.m8.params[,ncol(lysin.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(lysin.m8.params[,ncol(lysin.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{lysin}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(mivN.m8.params[,ncol(mivN.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(mivN.m8.params[,ncol(mivN.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{mivN}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(nuoL3.m8.params[,ncol(nuoL3.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(nuoL3.m8.params[,ncol(nuoL3.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{nuoL3}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(perM.m8.params[,ncol(perM.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(perM.m8.params[,ncol(perM.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{perM}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(pgpA.m8.params[,ncol(pgpA.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(pgpA.m8.params[,ncol(pgpA.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{pgpA}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(RfaL.m8.params[,ncol(RfaL.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(RfaL.m8.params[,ncol(RfaL.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{RfaL}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(tax.m8.params[,ncol(tax.m8.params)-11],xlim=c(0,1),breaks=seq(0,1,.1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(tax.m8.params[,ncol(tax.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{tax}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(TrbLVirB62.m8.params[,ncol(TrbLVirB62.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(TrbLVirB62.m8.params[,ncol(TrbLVirB62.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{TrbL-VirB6\_2}}
  \end{subfigure}
  \begin{subfigure}[t]{0.24\textwidth}
    \centering
    <<echo=F>>=
    par(mfrow=c(1,2),mar=c(4.5,1.5,2.8,0))
    hist(TrbLVirB63.m8.params[,ncol(TrbLVirB63.m8.params)-11],xlim=c(0,1),main=expression(p[omega>1]),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    par(mar=c(4.5,1.5,2.8,4))
    hist(TrbLVirB63.m8.params[,ncol(TrbLVirB63.m8.params)],main=expression(omega['>1']),cex.main=4,xlab='',axes=F,ylab='',col='black',density=200)
    axis(1,cex.axis=3,padj=1)
    @
    \caption{\textit{TrbL-VirB6\_3}}
  \end{subfigure}
  \caption{Distributions of the $p_{\omega>1}$ and $\omega_{>1}$ parameters for the real data under model M8.  Histograms are over 100 bootstrap datasets.}
  \label{fig:real_genes_m8_mles}
\end{figure}

\clearpage

\section{SBA Branch-Site Model A - Analysis of \textit{NR1D1}} \label{sec:nr1d1}
SBA for branch-site codon model A \citep{zhang2005evalimprovedbs} was implemented to demonstrate the feasibility of SBA implementations for new models.  The new implementation, which was completed within a few hours, can be found at \\
https://github.com/Jehops/codeml\_sba.  The nuclear receptor gene, \textit{NR1D1} \citep{baker2016functional}, was analyzed under NEB, BEB, and SBA methods.  The branch-site test of positive selection on the foreground branch leading to the human lineage was rejected at the 1\% level (LRT test statistic: 10.26612, p-value: 0.00135).  The MLEs of the $\omega$-distribution parameters are shown in table \ref{tab:NR1D1wmles}.  Because the estimated weights of the positive selection classes are very small, the estimates of $\omega>1$ were unreasonable.

Under both NEB and BEB, the same site had a posterior probably of positive selection larger than 0.99, whereas the posteriors were well below 0.5 for all other sites.  On the other hand, the mean posterior under SBA for the same site was 0.879.  Plots of the maximum likelihood estimates (MLES) of the $\omega$-distribution parameters are shown in figure \ref{fig:bsAwbs}.

\clearpage

\begin{table}[H]
  \caption{Estimates of the $\omega$-distribution parameters for the \textit{NR1D1} gene under branch-site model A.}
  \centering
  \begin{tabular}[h!]{*{5}l}
    \toprule
    Site Class          & 0       & 1       & 2a        & 2b \\
    \cmidrule(lr){1-1} \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
    weight              & 0.95058 & 0.04751 & 0.00183   & 0.00009 \\
    background $\omega$ & 0.03702 & 1.00000 & 0.03702   & 1.00000 \\
    foreground $\omega$ & 0.03702 & 1.00000 & 999.00000 & 999.00000 \\
    \bottomrule
  \end{tabular}
  \label{tab:NR1D1wmles}
\end{table}

\clearpage

\begin{figure}[H]
  \centering
      <<echo=F>>=
    nr2c1.pp <- read.csv("~/scm/sba.git/doc/paper/data/nr2c1_modelA_sba_foreground_branches_ps.csv", header=F)
    site.mean.pp <- as.vector(apply(nr2c1.pp,2,mean))
    nr2c1.params <- read.csv("~/scm/sba.git/doc/paper/data/nr2c1_modelA_params.csv", header=F)
    nr2c1.smoothed.params <- read.csv("~/scm/sba.git/doc/paper/data/nr2c1_modelA_smoothed_params.csv", header=F)

    p0 <- nr2c1.params[,dim(nr2c1.params)[2]-3]
    p1 <- nr2c1.params[,dim(nr2c1.params)[2]-2]
    p2 <- (1-p0-p1)*p0/(p0+p1) + (1-p0-p1)*p1/(p0+p1)

    w0 <- nr2c1.params[,dim(nr2c1.params)[2]-1]
    w2 <- nr2c1.params[,dim(nr2c1.params)[2]]

    p0.s <- nr2c1.smoothed.params[,dim(nr2c1.smoothed.params)[2]-3]
    p1.s <- nr2c1.smoothed.params[,dim(nr2c1.smoothed.params)[2]-2]
    p2.s <- (1-p0.s-p1.s)*p0.s/(p0.s+p1.s) + (1-p0.s-p1.s)*p1.s/(p0.s+p1.s)
    par(mfrow=c(3,3),mar=c(4,1,2,1))
    hist(p0,main='',breaks=5,cex.lab=1.5,yaxt='n',ylab='',xlab=expression(p[omega<1]))
    hist(p1,main='',breaks=5,cex.lab=1.5,yaxt='n',ylab='',xlab=expression(p[omega==1]))
    hist(p2,main='',breaks=5,cex.lab=1.5,yaxt='n',ylab='',xlab=expression(p[omega>1]))
    hist(p0.s,main='',breaks=5,cex.lab=1.5,yaxt='n',ylab='',xlab=expression(paste(p[omega<1],' (smoothed)')))
    hist(p1.s,main='',breaks=5,cex.lab=1.5,yaxt='n',ylab='',xlab=expression(paste(p[omega==1],' (smoothed)')))
    hist(p2.s,main='',breaks=7,cex.lab=1.5,yaxt='n',ylab='',xlab=expression(paste(p[omega>1],' (smoothed)')))
    hist(w0,main='',cex.lab=1.5,yaxt='n',ylab='',xlab=expression(omega['<1']))
    frame()  ## empty plot
    hist(w2,main='',cex.lab=1.5,yaxt='n',ylab='',xlab=expression(omega['>1']))
        @
        \caption{Branch-site model A $\omega$ distribution parameter estimates over bootstrap samples.  A bandwidth parameter of 0.4 was used to smooth the $p$ estimates.}
        \label{fig:bsAwbs}
\end{figure}

% \begin{figure}
%   \centering
%   <<echo=FALSE,dependson=c('lysin'),out.width="0.99\\textwidth",fig.align="center">>=
%   ##plot(range(0,lysin.m2a.omegas[,3]), c(0,1), xlab=expression(omega), ylab="Cumulative Distribution Function", type="n", bty="l")
%   ##for(i in 1:dim(lysin.m2a.omegas)[1]) lines(stepfun(lysin.m2a.omegas[i,],c(0,lysin.m2a.cum.ps[i,]),right=T))
%   plot(stepfun(lysin.m2a.omegas,c(0,lysin.m2a.ps.cumsum)/100),xlab=expression(omega),ylab="Bootstrap Cumulative Probability",main="")
%   @
%   \caption[Need to write this.]{Need to write this... .}
%   \label{fig:lysin_m2a_mle_cdf}
% \end{figure}

\clearpage

\bibliographystyle{apalike}
\bibliography{/home/jrm/scm/references.git/refs}

\end{document}
